---
title: "Tema 7 - Aplicaciones Lineales"
author: "Juan Gabriel Gomila & María Santos"
output:
  slidy_presentation: default
  ioslides_presentation:
    widescreen: yes
    css: Mery_style.css
    logo: Images/matriz_mov.gif
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

# Definiciones básicas

una aplicacion lineal sera una transformacion, una funcion que nos llevara objetos de un espacio vectorial a otro presrvando algunas propiedades

**Aplicación entre 2 conjuntos.** (Es una coorrespondencia entre cada elemento del primero y un solo elemento del segundo). Sean $A$ y $B$ dos conjuntos dados. Una aplicación de $A$ en $B$ es una correspondencia que a cada elemento $x\in A$ le asocia un, y solo un, elemento $y\in B$. de modo que para que algo sea una aplicacion hacemos corresponder a cada elemento del espacio de partida (En este caso $A$) uno y solo un elemnto de la derecha, podria ser que dos elemntos de la izquierda terminarian en el mismo de $B$, tambien podria ser que alguno de $B$ se quedara sin flecha pero **NO** podria ser que alguno de $A$ se quedara sin apuntar alguno de $B$, estamos obligado para que sea una aplicacion a que este conjunto de flechas asocie todos y cada uno de los elemntos de $A$ a uno y solo un elemnto de $B$, asi que en $A$ nadien se pude quedar sin flecha ni tampo de $A$ uno solo pueden salirle dos flechas

<div class = "center">
![<l class = "phototext">Ejemplo de aplicación entre dos conjuntos</l>](Images/aplin1.png)
</div>

Esto se sule pensar como una relacion de modo que a cada elemento de $A$ se va relacionando con algun elemento de $B$, solo pueden tener una relacion y nadien se puede quedar solo en $A$, aqui tendriamos una aplicacion que la podemos establecer como se nos de la gana, ejemplo: todos los de la izquierda pueden ir al mismo de la derecha, los de la derecha pueden ir una parte arriba otra abajo, podria ser que alguno de la derecha se quedara sin flecha, Dependiendo de como lo construyamos, la aplicacion lineal sera de un tipo o de otro, aqui solo estamos definiendo lo que es aplicacion, Pero tenemos tres tipos de aplicaciones:

**Aplicación exhaustiva.** Sea ($f$ de $A$ en $B$ (estos son dos conjuntos)) $f:A\longrightarrow B$ una aplicación. Se dice que $f$ es exhaustiva si, y solo si,($f$ de todo$(A)$ ) $f(A) = B$. Es decir, si todos los elementos de $B$ tienen una **anti-imagen** o **antecendente**:

Metamaticamente seria que para todo elemento $b$ de $B$ existeun elemento $a$ de $A$ tal que cuando aplicamos $f$ al elemento $a$ de $A$ nos da el elemento $b$ de $B$

$$\forall b\in B,\ \exists a\in A:\ f(a) = b$$


Una aplicacion es exhaustiva si a todo elemento de $B$ le llega una flecha, Nadien de $B$ se puede quedar solo, sin su pareja de $A$, fiejese que hay un elemento de $B$ que tiene dos parejas de $A$, asi para que sea exhaustiva todo elemento $b$ del conjunto $B$ tienen  que tener que pareja (tiene que tener anti-imagen), le tiene que llegar una flecha

<div class = "center">
![<l class = "phototext">Ejemplo de aplicación exhaustiva</l>](Images/aplin2.png)
</div>

Mas general es aquella a la que todo elemento del espacio de llegada le llega almenos una flecha, nadien del espacio del de llegada se puede quedar sin flecha

**Aplicación inyectiva.** Sea $f:A\longrightarrow B$ una aplicación. Se dice que $f$ es inyectiva si distintos elementos de $A$ tienen distinta imagen en $B$. ya sabemos que a todo elemento de $A$ le tiene que salir una flecha (en el caso de exhaustiva a todo elemento de $B$ le tiene que llegar una flecha). Para que sea **inyectiva** dos elemntos diferentes de $A$ tienen que tener distinta imagen en $B$

Esto significa que para todo $x,y$ elementos diferentes del conjunto $A$ (que es el de partida), $f(x)$ tienen que ser diferente del $f(y)$, No puede haber dos elementos del conjunto $A$ que tengan la misma pareja en el conjunto $B$, si que podria pasar que en el conjunto $B$ alguien se quedara sin pareja, pero lo que no puede pasar es que en el conjunto $A$ haya dos elementos diferentes que vayan a parar al mismo elemnto de $B$ osea que dos elemnto de $A$ difertnes tengan la misma pareja en $B$

$$x,y\in A,\ x\ne y\Rightarrow f(x)\ne f(y)$$

Esto es equivalente a decir que si dos elementos tienen la misma imagen para $f$ (es decir si extieran una $x$ y una $y$ tal que $f(x)$ fuera igual a $f(y)$ entonces $x=y$, en efecto si estos dos elemntos son tales que sus imagenes son iguales ($f(x)\ \text{y }f(y)$) esto significa que la pareja es la misma), entonces son el mismo, pues si dos elementos tienen la misma pareja, lo unico que puede pasar es que tanto uno como el otro sean el mismo $x=y$

$$f(x) = f(y)\Rightarrow x = y$$
En esta imagen tenemos que dos elementos de $A$ diferentes tienen distintas imagenes en $B$, ningun otro elemnto de $A$ puede volver a parar a uno de $B$ que ya tenga flecha, y en $B$ hay uno que se quedo solo pero eso no importa, lo que importa es que no hay dos elementos de $B$ que tengan dos anti-imagenes o lo que es lo mismo es que cualquier elemento de $A$ tiene una anti-imagen que no le llega a ninguna dos flechas, si $x$ es difertne de $y$ entonces $f(x)$ es diferente de $f(y)$

<div class = "center">
![<l class = "phototext">Ejemplo de aplicación inyectiva</l>](Images/aplin3.png)
</div>

y juntando los conceptos el de inyectivo y el de exahustivo se puede definir un concepto mayor

De la definición anterior, se deduce que en caso de tener una aplicación inyectiva, como máximo cada elemento de $B$ tendrá una anti-imagen (como maximo a cada elemnto de $B$ le llegara una flecha o ninguna). En otras palabras, la anti-imagen de un elemento de $B$ es o bien un elemento de $A$ o bien el conjunto vacío, $\emptyset$. Si nos dan un elemento de $B$
su anti-imagen o es un elemento o es el conjunto vacio (no le llega niguna flecha).

Si juntaramos los dos conceptos, inyectiva (que elementos difertnes van a parar a elementos difertnes) y exhaustiva (que todo elemento de $B$ le llega flecha) y si unimos las dos cosas
(a todo elemnto de $A$ le sale flecha, no pueden ir dos al mismo sitio y tienen imagenes todos ellos diferentes), pues  el concepto de: **Aplicacion exhaustiva**

En esta imagen tenemos una aplicacion inyectiva a la izquierda (imagenes de elemntos diferentes son diferentes) pero no seria exhaustiva porque a un elemnto de $B$ no le llega niguna flecha, ala derecha seria exhaustiva porque a todo elemento de $B$ le llega flecha ero no seria inyectiva porque a varios elemnto de $B$ le llegan dos flechas, si juntamos los dos conceptos e intentamos que una funcion sea a la vez inyectiva y exhaustiva sale el concepto de **Biyectiva**

<div class = "center">
![<l class = "phototext">Aplicación inyectiva a la izquierda, exhaustiva a la derecha </l>](Images/aplin4.png)

</div>

**Aplicación biyectiva.** Sea (una funcion $f$ definida entre un conjunto $A$ y $B$) $f:A\longrightarrow B$ una aplicación. Se dice que $f$ es biyectiva si es inyectiva y exhaustiva a la vez. Ello equivale a

Todo elemento $b$ de $B$ tiene una unica ($\exists!$) anti-imagen $a$ de $A$ tal que $f(a)$ es igual a $b$

$$\forall b\in B,\ \exists! a\in A:\ f(a) = b$$

Esto significa que a todo $A$ le sale flecha por ser aplicacion, a todo $B$ le llega flecha por ser exhaustiva y a ningun elemento de $B$ le pueden llegar dos flechas para que sea inyectiva, esto se traduce en que todo elemento de $A$ y todo elemento de $B$ estan relacionados unos con otros medinate una y solo una flecha

<div class = "center">
![<l class = "phototext">Ejemplo de aplicación biyectiva</l>](Images/aplin1.png)
</div>

En otras palabras una aplicacion biyectiva es una correspondnecia uniboca entre elemntos de $A$ y elementos de $B$, para que sea mas facil de recordar es como un matrimonio cada elemento de $A$ esta casado con uno y solo un elemento de $B$ (nadien tiene dos flechas de llegada, ni salida, nadien se queda solo). Si el conjunto $A$ es finito obliga a que el numero de elemntos que tengamos en $A$ sea igual al numero de elementos que tenemos en $B$.

Esto es lo que podemos utilizar para poder traducir un espacio vectorial a otro, las matrices cuadradas 2x2 eran lo mismo que el espacio vectorial $\mathbb{R}^4$, si lo pensamos todas las matrices 2x2 tienen 4 elementos, todos los vectores de $\mathbb{R}^4$ tiene 4 coordenadas , no sera dificil establecer una biyeccion (establecer una aplicacion que sea biyectiva) entre $\mathbb{R}^4$ y las matrices cuadradas de orden 2 pero eso sera con el concepto de aplicacion lineal

# Aplicaciones lineales

Ahora que ya sabemos que una aplicacion es una correspondendcia entre dos conjuntos, en matematicas se establecen conjuntos de todo tipo estas aplicaciones son tan importantes que se le llama morfismo: que es una aplicacion que es capaz de conservar la estructuradel objetoque se esta estudiando en ese momento, en este caso sera con los espacios vectoriales

## Breve introducción

Una vez estudiada la estructura de espacio vectorial, a continuación estudiaremos las funciones o aplicaciones que conservan esta estructura.

Por lo general, este tipo de funciones recibe el nombre de **morfismos**, pero en el caso de espacios vectoriales, se utiliza más el término **aplicación lineal**.

Resulta interesante que tipo de funcion o tipo de aplicacion es capaz de conservar esta estructura entre dos espacios vectorial diferentes, que sumar una matriz o sumar un vector en principio son elementos de diferentes espacios vectoriales se esencialmente lo mismo, es decir que el resultado es equivalente en el sentido que la matriz suma o el vector (no iguales) pero equivalentes, esa es la idea que busquemos transformaciones,aplicaciones,funciones que conserven la estructura como sumar elementos de un lado se lo mismo que sumar en otro lado que multiplicar por un escalar en uno de los espacios se lo mismo que multiplicar en el otro lado asi que estas aplicaciones que sean capaes de conservar la estructura el objetos estudiado en nuestro caso los espacios vectoriales, reciben en general el nombre de **morfimos** existen muchos tipos de morfismos
algunos conservan la caracterizacion como la continuidad, diferenciabilidad, topologia, entonces buscamos transformaciones de un espacio a otro que sean capases de mantener las propiedades que si algo era cierto en el espacio de partida cuando nosotros apliquemos $F$ a ese objeto sigue siendo cierto o equivalente en el espacio de llegada y con los espacios vectoriales los morfimos se llaman **Aplicaciones Lineales**

## La aplicación identidad

**Aplicación identidad.** Es la aplicación que dado un vector de un espacio vectorial $E$ (Este es su simbolo $I$) que transforma cada vector de $E$ en sí mismo: Simplemente nos dan un vector $x$ de este espacio vectorial la funcion $I$ de $x$ es igual a $x$ simplemente cualquier vector se transforma en si mismo la aplicacion identidad aplicada a un vector da ese propio vector 

$$\begin{matrix}I:&E\longrightarrow E\\
&x\mapsto x
\end{matrix}$$

Ahora hay que preguntarse si se comporta bien la aplicacion lineal respecto de la suma y el producto por un escalar de elemntos del espacio vectorial $E$

En primer lugar, estudiaremos si hay alguna relación entre la imagen de una suma de vectores, $I(x+y)$, y las imágenes de cada uno de los sumandos, $I(x),I(y)$

Por definición de la aplicación identidad: (Recuarda que $I$ de un vector es el propio vector)

$$I(x+y) = x+y$$

Esto es asi porque la aplicacion de identidad transforma cada vector en si mismo esto significa que si hacemos $I(\text{de cualquier cosa})=\text{a esa misma cosa}$

Por otro lado, vemos como aplicando la identidad $I$ por separado a cada vector $x$ e $y$ nos da lo mismo

$$\left.\begin{matrix}I(x) = x\\I(y) = y\end{matrix}\right\}\Rightarrow I(x)+I(y) = x+y$$

Con lo cual se cumple, que si juntamos las dos diferentes maneras de sacar $x+y$ de arriba y juntas las dos expreciones nos queda que $I(x+y)$

$$I(x+y) = x+y= I(x)+I(y)$$

Por tanto la imagen de la aplicacion identidad aplicada al vector $I(x+y)$ es lo mismo que la imagen de $X$ por la identidad ($I(x)$) mas la imagen de $y$ por la identidad ($I(y)$) en otras palabras la imgen de la suma de vectores es la suma de las imagenes del los sumandos en si se comporta bien con la suma que es la imagen de la suma es la suma de imagenes

**Proposición.** Dada la aplicación identidad, la imagen de la suma es la suma de imágenes de los vectores por tanto es lo mismo sumar en el espacio de partida como en el espacio de llegada que aunque el espacio es el mismo la transformacion podria ser que rompiera esa suma bien definida. Eneste caso va bien porque sumado en el espacio de partida es igual a sumar en de llegada, habra aplicaciones definidas de un espacio $E$ en si misma que no sera a aplicacio de Identidad que no se van a comprtar tan bien, no van a tener esta propiedad que la suma de imagenes es la imagen de la suma, la idetidad lo cumple de momento por eso lo hace tan interesante estudiarle y de momento podemos afirmar que para la aplicacion identidad la imagen de la suma es la suma de imagenes

Ahora hay que preguntarnos si para la identidad se comporta bien el producto por un escalar

En segundo lugar, estudiaremos si existe alguna relación entre la imagen de un escalar por un vector $I(\lambda x)$ y la imagen del vector $I(x)$.

Por definición de aplicación identidad:(ya sabemos que culquier cosa apicada a la identidad es esa misma cosa)

$$I(\lambda x) = \lambda x$$

Por lo tanto, existe la siguiente relación 

$$I(\lambda x) = \lambda x = \lambda I(x)$$

**Proposición.** Dada la aplicación identidad, la imagen del producto de un escalar por un vector es el escalar por la imagen de dicho vector.

Al final es igual a $\lambda I(x)$ porque en si $I(x)=x$ por lo tanto se puede escribir asi con la $I(x)$ y en otras palabras la imagen de $I(\lambda x)=\lambda I(x)$, si la aplicacion es la identidad, la imagen del producto de un escalar por un vector es el escalar por la imagen de dicho vector asi que la identidad cumple que $I(x+y)$ es $I(x)+I(y)$ e $I(\lambda x)=\lambda I(x)$, estas dos propiedades lo que hacen es transmitir conservar las dos operaciones uqe teniamos en cada uno de los espacios vectoriales (la suma y el producto por un escalar), cuando en general se cumplan estas dos propiedades la aplicacion que tengamos resulta muy interesante y se definira como aplicacion lineal,por ahora hay que saber que la identidad es capas de conservar la suma y el producto por un escalar (Hay ejemplos donde esto no se cumple)

## Aplicacion Constante

Es aquella que envía cualquier elemento del espacio vectorial de partida al mismo elemento del espacio vectorial de llegada.

**Por ejemplo:** si tomamos como $f$ una función definida sobre los números reales hasta los números reales que a cada elemento $x$ le hace corresponder un número fijo $K$.
$$f:\mathbb{R}\rightarrow\mathbb{R}$$
$$x\longrightarrow K$$

Esto sería la aplicación constante igual a $K$, fijese que; 
$$f(x)=K\hspace{5mm}\forall\ x\in\mathbb{R}$$
Vamos a ver, qué pasa con alguna de estas aplicaciones.

Tomemos la aplicación definida $f:\mathbb{R}\rightarrow\mathbb{R}$ y a cada elemento $x$ le enviamos el número $2$,$x\longrightarrow 2$, por ejemplo: $f(0)=2,\ f(-5)=2\  f(\pi)=2$ en otras palabras $f(x)=2\hspace{5mm}\forall\ x\in\mathbb{R}$.

Simplemente todos y cada uno de los números reales le enviamos al número 2.

Como hemos visto antes con la aplicación identidad vamos a estudiar si existe alguna relación entre la suma de vectores y las imágenes de cada uno de los sumandos, es decir nos preguntamos si o no sería cierto que,$f(x+y)=f(x)+f(y)$, pues por definición de aplicación constante $f(x+y)=2$ ($f$ de un elemento siempre es 2), por su lado $f(x)=2$ y $f(y)=2$, Entonces todo esto es $2$ y por tanto $f(x)+f(y)$ en este caso va a ser $2+2=4$.

Asi que $f(x+y)=2$ pero $f(x)+f(y)$ es 4. Por tanto lo que podemos garantizar es que no se cumple que estas dos cosas sean iguales: $f(x+y)$ no es $f(x)+f(y)$, así que en este caso la imagen de la suma no es la suma de imágenes entonces la aplicación parece que no es lineal para la suma.

En segundo lugar también nos interesaba estudiar la relación que existe entre la imagen de el escalar por un vector y la imagen del propio vector, es decir nos preguntamos si sería cierto que por una vez estas ¿$f(\lambda x)\cdot\lambda f(x)$? para cualquier $\lambda$ y cualquier elemento $x$ del espacio vectorial.

Pues como antes. $f(\lambda x)=2$ (Porque $F$ de cualquier cosa es 2) mientras que $\lambda f(x)=\lambda\cdot2=2\lambda$ así que en general y para cualquier valor del $\lambda$ tampoco va a ser cierto que $f(\lambda x)=\lambda f(x)$, así que no ocurre que la imagen del producto de un escalar por un vector, sea el escalar por la imagen del vector, así que esta aplicación constante parece que es menos interesante que la aplicación identidad, de la identidad sí que se cumplían estas igualdades.

En este caso parece que es falso., Así que si como ejercicio: que si nos dan una aplicación constante $f(x)=k$, para valores de $k$, que $k$ tendríamos que elegir para que se cumpliera que $f(x+y)=f(x)+f(y)$ (es decir que sea $f$ una aplicación lineal para la suma) y también que $f(\lambda x)=\lambda f(x)$

Qué valor de $k$ es aquel que acepta la linealidad de estas dos y a ver si existe algún valor que sea más interesante que el resto y que cumple estas dos propiedades utilizando la aplicación constante.

## Definición de aplicación lineal

La aplicacion identidad conserva la suma y el producto por uun escalar, cuando es lineal

**Aplicación lineal.** Sean $E$ y $F$ dos $\mathbb{K}$-espacios vectoriales (Dos espacios vectoriales sobre el mismo cuerpo). Dada una aplicación $f$ definida del espacio $E$ al espacio $F$ de modo que a cada vector $x$ del espacio $E$ se le transforma en el vector $f(x)$ del espacio $F$ tal que 


$$\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)
\end{matrix}$$

diremos que esta aplicacion $f$ es lineal, si se verifica que:

Para cualquier vector $x$,$y$ del espacio $E$ de partida se dice que es lineal  si la imagen de la suma de vectores es la suma de las imagenes de esos vectores

$$\forall \vec{x},\vec{y}\in E,\quad f(\vec{x}+\vec{y})= f(\vec{x})+f(\vec{y})$$
y ademas para cualquier vector $x$ de $E$ y un escalar $\lambda$ del cuerpo es la imagen del producto de un escalar por un vector es el escalar por la imagen del vector 

$$\forall \vec{x}\in E,\ \forall\lambda\in \mathbb{K},\quad f(\lambda\vec{x})= \lambda f(\vec{x})$$

Si una aplcacion $f$ definida entre dos espacios vectoriales $E$ y $F$ cualesquiera conserva la suma de imagenes  y conserva el producto por un escalar entonces se dice que la aplicacion $F$ es Lineal una aplicacion cumpla estas dos caracteristicas .

Las dos condiciones anteriores son equivalentes a una tercera: (Cuando tenemos que demostra que algo es lineal podemos hacerlo mas rapido aunque es mas compleja)

Para dos escalares $\lambda$ y $\mu$ del cuerpo y cualquier vector $x$ e $y$ del espacio vectorial $E$, la imagen de $\lambda\vec{x}+\mu\vec{y}$ es igual a $\vec{x})+\mu f(\vec{y}$, esta condicion unifica las dos anteriores y es equivalente de forma trivial

$$\forall \lambda,\mu\in\mathbb{K},\ \vec{x},\vec{y}\in E,\quad f(\lambda\vec{x}+\mu\vec{y}) = \lambda f(\vec{x})+\mu f(\vec{y})$$

**Ejemplo 1**

La identidad es ejemplo de aplicacion lineal pero en este caso

Nos piden si la siguiente aplicación, conocida como **Primera proyección**, es lineal o no:

Esta definida de $\mathbb{K}^2$ en $\mathbb{K}$ y la primera proyeccion es la aplicacion que dado cualquier punto o vector de $\mathbb{K}^2$ nos devuelve solo la primera coordenada, Por ejemplo: $f(0,0)=0$, $f(0,7)=0$, $f(0,99)=0$, $f(-1,3)=-1$,$f(28,7)=28$, La primera proyeccion es una aplicacion (Transformacion) definida del espacio vectorial $\mathbb{K}^2$ al espacio vecctorial $\mathbb{K}$ (Ahora no son el mismo espacio vectorial) y la aplicacion queda definida como $f(x,y)=x$ que es solo la primera coordenada

$$\begin{matrix}f:&\mathbb{K}^2\longrightarrow \mathbb{K}\\
&(x,y)\mapsto x
\end{matrix}$$

Vamos a ver si es lineal para la suma o el producto

- **Lineal para la suma**: (Nesecitamos que: $f$ de un vector mas otro vector tiene que ser la imagen de un vector mas la imagen de otro vector), en este caso los elemntos de $\mathbb{K}^2$ son pares de coordenadas (Tuplas de dos elementos), asi que queremos comprobar que $f((x_1,y_1)+(x_2,y_2))$ es igual a $f((x_1,y_1)+f(x_2,y_2))$ tomamos dos elementos del espacio de partida, ($(x_1,y_1)$, este seria el primer elemneto del espacio de partida), el espacio $\mathbb{K}^2$ tiene por elementos pares $(x,y)$ entonces un elemento sera de la forma $(x_1,y_1)$.

Entonces lo que podemos hacer de momento es sumar estos dos vectores $f((x_1,y_1)+(x_2,y_2))$ porque viven en el espacio de partida (estamos sumando en $\mathbb{K}^2$) asi que seria igual a $f(x_1+x_2,y_1+y_2)$, recordemos que en este caso es quedarnos solo con la primera coordenada entonces seria $x_1+x_2$ pues es la primera componente del vector al que estamos aplicando $f$ asi que por un lado, $f$ de la suma seria $x_1+x_2$ (Sumar las primeras componentes de los dos vectores)

$$f((x_1,y_1)+(x_2,y_2)) = f(x_1+x_2,y_1+y_2) = x_1+x_2$$

Por su parte, $f(x_1,y_1)$ es la primera coordenada $x_1$ y $f(x_2,y_2)$ es la segunda coordenada $x_2$ entoces $f(x_1,y_1)+f(x_2,y_2)$ sera $x_1+x_2$, Fijaros que de nuevo podemos empalmar que $f$ de la suma $f((x_1,y_1)+(x_2,y_2))$ es $x_1+x_2$ pero a su vez esta suma es la suma de $f(x_1,y_1)+f(x_2,y_2)$

$$\left.\begin{matrix}f(x_1,y_1) = x_1\\f(x_2,y_2) = x_2\end{matrix}\right\}\Rightarrow f(x_1,y_1)+f(x_2,y_2) = x_1+x_2$$ 

Por lo tanto, La primera condicion que sea lineal para la suma es cierta para esta primera proyeccion

$$f(x_1,y_1)+f(x_2,y_2) = f((x_1,y_1)+(x_2,y_2))$$

- **Lineal para el producto por escalar: ** Solo hay que demostrar que $f$ de $\lambda$ por el vector es $\lambda$ por $f$ del vector, Quien seria: $f(\lambda(x,y))$ podemos multiplicar $\lambda$ por ese vector que seria $f(\lambda x,\lambda y)$ pero $f$ de un vector en el caso de la primera proyeccion es solo la primera componente porque entonces seria $\lambda x$ y esto es lo mismo que la $\lambda$ y multiplicar por $f(x,y)$ porque $f(x,y)$ es igual a solo la $x$ (La primera componente), todo junto seria:

$$f(\lambda(x,y)) = f(\lambda x,\lambda y) = \lambda x = \lambda f(x,y)$$

Asi se demuestraque es lineal para el producto por un escalar, Como recomendacion cada vez que coloquemos un igual en una demostracion de este estilo ensima del igual le colocamos una flecha y digamos porque es ese igual, por ejemplo: $f(\lambda(x,y)) = f(\lambda x,\lambda y)$ este igual es cierto porque en el espacio vectorial $\mathbb{K}^2$ $\lambda\cdot\text{vector}$ es igual a la igualacion de arriba es por propia definicion de como se multiplica un escalar por un vector ya que $\mathbb{K}^2$ es un espacio vectorial, esta defincion: $f(\lambda x,\lambda y) = \lambda x$ es porque $f$ se a definido asi de la primera proyeccion es tal cual asi porque $f$ aplicado a $(\lambda x, \lambda y)$ , para: $\lambda x = \lambda f(x,y)$ de nuevo es porque la primera proyeccion esta definida asi pero sobre el elemento $(x,y)$. Esto seria solo de pensar de que cada vez que pongamos un igual **$=$** porque lo estamos ponieno segun la propiedad o definicion que estamos utilizando para tener bien en claro las cosas.

Podrimaos comprobar pero para la segunda proyeccion en que en lugar de la $x$ se pasa la $y$

$$\begin{matrix}f:&\mathbb{K}^2\longrightarrow \mathbb{K}\\
&(x,y)\mapsto y
\end{matrix}$$

**Proposición.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales y $f:E\longrightarrow F$ una aplicación. Entonces son equivalentes decir estas tres cosas:

- $f$ es lineal, (Que sea lineal significa que cumple aquellas dos propiedades $f(x+y)=f(x)+f(y)$ y $f(\lambda x)=\lambda f(x)$, La segunda era la otra forma equivalente con la $y$)
- $f(\alpha\cdot x+\beta\cdot y) = \alpha\cdot f(x)+\beta\cdot f(y)$ para todos $\alpha,\beta\in\mathbb{K}$ y $x,y\in E$, (Sale de la arriba que acabamos de ver)
- ($f$ es equivalente a cualquier combinacion lineal de vectores)$f\left(\sum_{i = 1}^n\alpha_i \cdot x_i\right) = \sum_{i = 1}^n\alpha_i \cdot f(x_i)$ para todos los $\alpha_i\in\mathbb{K}$ y $x_i\in E$, Es decir la imagen de una combinacion lineal es la combinacion lineal de las imagenes 

**Ejercicio 1** Demostrar formalmente esta Proposición.

## Ejemplos de Aplicacion Lineal

Por ejemplo ya hemos comentado que la aplicación identidad que se define de un espacio vectorial en sí mismo y que a cualquier elemento X se le otorga el mismo elemento, es una aplicación lineal 

$$\begin{matrix}I:&\mathbb{E}\longrightarrow \mathbb{E}\\
&x\mapsto x
\end{matrix}$$

Del mismo modo hay un ejercicio que era para ver si alguna de las aplicaciones constantes resultaba ser lineal.

Pues $f$ definida de un espacio vectorial en otro ($F$ cualquiera) definida por cualquier elemento de $x$ se envía al número $0$, $f(x)=0$.Para todo $x$ del espacio vectorial también es lineal 

$$\begin{matrix}f:&\mathbb{E}\longrightarrow \mathbb{F}\\
&x\mapsto 0
\end{matrix}$$

Más en general si fijamos un número alpha cualquiera $\alpha\in\mathbb{K}$ resulta que la aplicación como $f_\alpha$ que envía un elemento del espacio vectorial $E$ al elemento $\alpha x$ también resulta ser lineal.

$$\begin{matrix}f_\alpha:&\mathbb{E}\longrightarrow \mathbb{E}\\
&x\mapsto \alpha x
\end{matrix}$$

Se puede demostrar entre las muchas formas una seria que $f(ax+by)=af(x)+bf(y)$ y comprobar que se cumple la igualdad.

Más específico, si tomamos una aplicación $f$ definido en un espacio vectorial concreto $\mathbb{R}^2$ y la definimos hacia $\mathbb{R}^3$ y un vector $(x,y)$ del plano lo enviamos a $\mathbb{R}^3$ de forma $x+y$ en la primera coordenada $2x-y$ en segunda coordenada $y-x$ en la tercera coordenada pues resulta que también sería lineal.

$$\begin{matrix}f:&\mathbb{R}^2\longrightarrow \mathbb{R}^3\\
&(x,y)\mapsto (x+y,2x-y,y-x)
\end{matrix}$$

Si la queréis demostrar, pues tendríamos que demostrar que $f(a(x_1y_1)+b(x_2y_2))=af(x_1y_1)+bf(x_2y_2)$. Si tomamos aquí la parte de la izquierda del igual $f(a(x_1y_1)+b(x_2y_2))$ sería hacer la $f$ como $f(ax_1+bx_2,ay_1+by_2))$ que si aplicamos la definición de la $f$ pues sería en este caso  por primera coordenada con la segunda coordenada $(ax_1+bx_2+ay_1+by_2,2(ax_1+bx_2)-(ay_1+by_2),ay_1+by_2-(ax_1+bx_2))$, Si ahora operamos y separando y juntando como nos interesa, si sacamos la $a$ y $b$ factor común nos quedaría: $a(x_1+y_1,2x_1-y_1,y_1-x_1)+b(x_2+y_2,2x_2-y_2,y_2-x_2)$, esto es ni más ni menos que: $af(x_1,y_1)+bf(x_2,y_2)$, así es como podriamos demostrar que esta aplicación $f$ es lineal.

En general una aplicación $f$ definida entre dos espacios vectoriales $\mathbb{R}^n$ y $\mathbb{R}^m$ que se defina como:

$$\begin{matrix}f:&\mathbb{R}^n\longrightarrow \mathbb{R}^m\\
&(x_1,x_2,...,x_n)\mapsto \sum_{i=1}^{n}a_{1i}x_i,\cdots,\sum_{i=1}^{n}a_{mi}x_i
\end{matrix}$$

Es decir haciendo una combinación lineal de las entradas de las componentes $(x_1,x_2,...,x_n)$ de los vectores del espacio de salida **(Serian los Sumatorios)** que que simplemente se hizo una combinación lineal de cualquiera de las coordenadas de partida ($a_{1i}x_i$) en cualquiera de las de llegada ($a_{mi}x_i$), pues haciendo combinaciones lineales de vectores, salen en general aplicaciones lineales, donde los $a_{ij}\in\mathbb{K}$ que coloquemos sean nuemros de cuerpo. Esa es la gracia de álgebra lineal, que sean cierto las ecuaciones que escribamos ($a_{1i}x_i$,$a_{mi}x_i$) sean lineales (mientras sean polinomios de grado 1 sin término independiente).

Si pusiéramos algún polinomio al cuadrado o al cubo o algún término independiente resultarían en cosas que no serian aplicaciones lineales.

Por su lado no significa que únicamente podemos trabajar a este nivel si $E$ y $F$ son espacios y subespacio vectorial respectivamente, el uno del otro en un $\mathbb{K}$ espacio vectorial. $E\subseteq F\ \mathbb{K}-e.v$. Pues resulta que las aplicaciones Immersion por un lado o inclusión:

la inclusión de $i_F$ que se define desde el subespacio pequeño ($F$) hasta el grande ($E$) que a cada elemento $x$ de $F$ le hace corresponder el mismo $x$ de $E$ simplemente es como incluir el subespacio de $F$ en $E$ por eso se llama la inclusión.

$$\begin{matrix}i_F:&F\longrightarrow E\\
&x\mapsto x
\end{matrix}$$

La aplicación (proyección), la que se llama $\pi_F$ que es lo contrario, que a cada elemento del espacio vectorial $E$ le hacemos corresponder $1$ del cociente módulo $F$, se llama la proyección sobre $F$ y es que a cada elemento del espacio vectorial $E$ lo enviamos a la clase de equivalencia $[x]$ módulo $F$ es decir a su representante dentro de la relación de equivalencia módulo $F$.

$$\begin{matrix}\pi_F:&E\longrightarrow E/F\\
&x\mapsto [x]_F
\end{matrix}$$

Estas también son aplicaciones lineales definidas sobre subespacios o el cociente en este caso.

También si es un $\mathbb{K}$ espacio vectorial, resulta que si tomamos una base $x_1,x_2,...,x_n$ que es una base del espacio vectorial $E$.

Sabemos que cualquier elemento $x\in\mathbb{E}-e.v$ se podría escribir de forma única como una combinación lineal: $x=\alpha_1x_1+\alpha_2x_2+...+\alpha_nx_n$ con los $\alpha_i$ del cuerpo en cuestión.

La aplicación definida de el espacio vectorial $E$ hasta $\mathbb{K}^n$ que envía cada vector $x$ al vector de coordenadas de componentes del elemento en la base del espacio vectorial elegida que en este caso sería $(\alpha_1,\alpha_2,...,\alpha_n)$ es la aplicación que envía a cada elemento del espacio vectorial a las coordenadas de ese vector en una base como elemento de $\mathbb{K}^n$. También es una aplicación linea:

$$\begin{matrix}l\cdot E\longrightarrow \mathbb{K}^n\\
x\mapsto (\alpha_1,\alpha_2,...,\alpha_n)
\end{matrix}$$

Por último acerca de espacios y subespacios.

Si $F$ y $G$ son subespacios vectoriales complementarios de $E$, es decir la suma directa $E=E\oplus F$ da el espacio vectorial. Ya sabemos que en este caso cualquier vector $x\in E$ tiene la particularidad que se escribe de forma única como suma de una parte que pertenece a $F$ llamémosle $y_x+z_x$:

$$x\in E\longrightarrow x=y_x+z_x,\ y_x\in F,\ z_x\in G$$

Esto sabemos que siempre es así, que en la suma directa de un de dos subespacios para conformar a uno más grande cualquier vector del espacio grande se puede escribir de forma única como suma de un vector de uno de los subespacios más otro vector del otro subespacio.

Pues resulta que existen dos proyecciones más.

Existe la proyección sobre $F$ que toma un vector de $E$ y lo manda a $F$ a partir de que a $x$ mandarlo a la componente $y_x$ de esa suma directa: $P_F: \begin{matrix}E\longrightarrow F\\x\mapsto y_x\end{matrix}$

Y existe también la proyección $P$ sobre $G$ que un vector de $E$ cualquiera lo proyecta sobre el subespacio vectorial $G$ a partir de enviarlo a la componente de la suma directa que pertenece a G:$P_G: \begin{matrix}E\longrightarrow G\\x\mapsto z_x\end{matrix}$

Entonces éstas también son dos aplicaciones lineales. Vamos a demostrarlo, para ello simplemente pues partimos de $x=y_x+z_x\in E$ ya que esto es posible si $x$ es un elemento de $E$ ya que es $E\oplus F$ y tomemos otro vector $u\in E$ que tendrá componentes o se podrá expresar como $u=y_u+z_u$. Donde evidentemente el $y_x,y_u\in F$ y $z_x,z_u\in G$ son simplemente las descomposiciones de $x$ y de $u$ en la suma directa. Y tomemos $\alpha,\beta\in\mathbb{K}$ entonces si yenemos todo eso anterior, podríamos escribir: 
$\alpha x+\beta u=\alpha(y_x+z_x)+\beta(y_u+z_u)$ y podríamos redistribuir para colocarlo como nos interesara, podríamos poner $(\alpha y_x +\beta y_u)$ que sería la parte del espacio vectorial $F$ más $(\alpha z_x+\beta z_u)$ que sería la parte que viviría en el espacio vectorial $G$: $(\alpha y_x +\beta y_u)+(\alpha z_x+\beta z_u)$ además esta última expresión es única por tanto la única forma de escribir $\alpha x+\beta u$ como suma directa de elementos de $F$ y $G$ es precisamente así: $(\alpha y_x +\beta y_u)+(\alpha z_x+\beta z_u)$ esta sería la única expresión posible ya que la suma directa tiene expresiones únicas y cada vector se puede escribir de una única forma como un vector por $x$ más otro vector por la parte del Espacio $G$.

Dado que es única ahora podríamos aplicar la proyección sobre $F$ o sobre $G$, aqui la vamos hacer la $P_f$ sobre $\alpha x+\beta u=(\alpha y_x +\beta y_u)+(\alpha z_x+\beta z_u)$ evidentemente aplicar esto sería en el caso de $F$ quedarnos sólo con la parte de la expresión que pertenece a $F$ es decir la primera parte $(\alpha y_x +\beta y_u)$ la proyección sería $P_f(\alpha x+\beta u)=\alpha y_x+\beta y_u$ y fijaros que esto en efecto es $\alpha P_f(x)+\beta P_f(u)$ así que en efecto es una aplicación lineal por tanto $P_f$ como $P_G$ son aplicaciones lineales en el contexto del subespacio vectorial suma

En el caso que si tomamos algo como por ejemplo la aplicación de:

$$\begin{matrix}l: \mathbb{R}^2\longrightarrow \mathbb{R}^3\\
(x,y)\mapsto (x+y,x+y-1,y)
\end{matrix}$$

Esta no va a ser lineal y no lo va a ser por culpa de este menos $-1$ que tenémos aquí. No tendremos entre nosotros una aplicación lineal.

Esto se puede demostrar por la definición. Pero también es inmediato demostrarlo a partir de la proposición que nos afirma que la imagen del Zeros y la aplicación es lineal. La imagen del vector cero debería ser cero $l(\vec{0})=\vec{0}$ es una condición necesaria aunque no es suficiente, si es lineal esto tiene que pasar y si no pasa esto no es lineal.

¿Entonces quién es $F$ del vector cero? aplicando este ejemplo nos queda,$l(0,0)=(0+0,0+0-1,0)=(0,-1,0)\not=\vec{0}$

Por tanto esta aplicación no es lineal, aqui se a demostrado a través de un contraejemplo de una propiedad que debería de cumplirse en el caso en que fuera lineal y como no se cumple pues puedo afirmar que no es lineal porque si lo fuera se cumpliría.


Nos preguntamos ¿El que sea lineal debe ser cierto que en lugar de para dos sea cierto para $n$? Si, era precisamente la última proposición anterior. Y si una aplicación es lineal por ejemplo si tomamos el vector cero del espacio de partida a qué número debe ir o a qué elemento debe ir del espacio de llegada o si tomamos un elemento y su opuesto que ya sabéis que en el espacio de partida $x+(-x)=0$ sus imágenes $F$ de $x$ y $F$ de $(-x)$ también sumadas debe dar cero o no.

Estas preguntas que salen de forma natural cuando uno estudia los espacios vectoriales resultan muy interesantes porque si tuvieras que elegir un candidato del espacio vectorial de llegada $F$ para enviar el cero del espacio de partida $E$, ¿Cuál elegirías? pues probablemente elegirías el cero.

## Imagen del vector nulo

**Propiedad.** Dada una aplicación lineal (Definida de $E$ en $F$  talque a $x$ le mandamos un vector $f(x)$)$$\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)
\end{matrix}$$

La imagen del vector nulo $\vec{0}_E$ de $E$ es el vector nulo $\vec{0}_F$ de $F$

$$f(\vec{0}_E) = \vec{0}_F$$

o lo que es lo mismo $F$ de $0$ es $0$, $F$ del $0$ de $E$ (El $F$ del espacio de partida es el $0$ del espacio de llegada ($0_F$)), aqui se puso de forma explicita ya que se puso el subindice a cual espacio pertenence pero en general no se pondra asi

**Demostración**

El vector nulo $\vec{0}_E$ es el neutro de la suma de $E$, por tanto (Para cualquier vector $x$ que pertence a $E$ esta sumatoria nos da $x$ como $0$ es el neutro de la suma, el $0$ cumado con cualquier vector del espacio vectorial $E$ da ese propio vector)
$$\forall\vec{x}\in E\Rightarrow \vec{x}+\vec{0}_E= \vec{x}$$

Como $\vec{x}+\vec{0}_E = \vec{x}$ podemos aplicar $f$ a la izquierda y a la derecha del igual, entonces $f(\vec{x}+\vec{0}_E) = f(\vec{x})$ como estas cosas son iguales al aplicar $f$ tambien van a ser iguales

Como $f$ es lineal, esto significa que $f(\vec{x}+\vec{0}_E) = f(\vec{x})+f(\vec{0}_E)$, es una de las dos propiedades que cumple por ser lineal

Entonces, por lo anterior se obtiene que $f(\vec{x})+f(\vec{0}_E) = f(\vec{x}+\vec{0}_E)$ y al mismo tiempo $f(\vec{x}+\vec{0}_E) = f(\vec{x})$, juntandolo todo nos quedaria:

$$f(\vec{x})+f(\vec{0}_E) = f(\vec{x})$$

Si eliminamos estas $f(\vec{x})$ que son iguales a la izquierda o derecha del igual y los que nos quedaria es que $f(\vec{0}_E) = 0$

Donde, en efecto, $f(\vec{0}_E)$ es el elemento neutro de la suma de $F$ ($F$ del neutro del espacio vectorial $E$ es el neutro del espacio vectorial $F$)

$$f(\vec{0}_E) = \vec{0}_F$$

De momento esta aplicacion lineal concerva que la imagen del neutro sea el neutro, de ahi que las aplicaciones lineales conservarian la estructura del espacio vectorial

Comom se nos ah presentado los espacios vectoriales $E$ y $F$ puede que no tengan nada que ver que uno sea polinomios otro matrices, vectores o funciones continuas, simplemente el que sea el $0$ del espacio de partida ira a parar si la aplicacion es lineal al $0$ del espacio de llegada, ejemplo podria ir a parar a (vector $0$,matriz $0$,funcion $0$)

## Imagen del vector opuesto

**Propiedad.** Dada una aplicación lineal: 

$$\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)
\end{matrix}$$

La imagen del vector opuesto es el opuesto de la imagen del original 

$$f(-\vec{x}) = -f(\vec{x})$$

Este menos que tenemos aqui $-\vec{x}$ es del espacio vectorial $E$ y el menos que tenemos $-f(\vec{x})$ es del espacio vectorial $F$ que son el espacio vectorial de donde vivien

**Demostración**

La suma de un vector y su opuesto siempre es el elemento neutro, asi que para cualquier $x$ que tengamos en $E$ si sumamos $x$ mas el opuesto de $x$ siempre nos va a ser el cero del espacio de partida
$$\forall\vec{x}\in E\Rightarrow \vec{x}+(-\vec{x}) = \vec{0}_E$$

Como $\vec{x} +(-\vec{x}) = \vec{0}_E$ esto es cierto podemos volver a aplicar $f$ a la izquierda y a la derecha del igual, entonces: $f(\vec{x}+(-\vec{x})) = f(\vec{0}_E)$

Como $f$ es lineal ($f$ de una suma es la suma de $f$): $f(\vec{x}+(-\vec{x})) = f(\vec{x})+f(-\vec{x})$

Por lo anterior si lo juntamos nos quedaria que tenemos $f(\vec{x})+f(-\vec{x}) = f(\vec{0}_E)$

Pero de la propiedad anterior (la que se ah demostrado antes), se sabe que $f(\vec{0}_E) = \vec{0}_F$ y, por tanto la operacion de arriba se puede escribir como, $f(\vec{x})+f(-\vec{x}) = \vec{0}_F$. Donde, por la propiedad (que debe cumplir un elemento para ser el opuesto de otro es que su suma debe dar $0$) en otras palabras del elemento $f(-\vec{x})$ debe ser el opuesto de $f(\vec{x})$, es el unico elemento tal que sumado con otro da el neutro, en otras palabras si se pasa a restar $f(\vec{x})$ al otro lado del igual 

$$f(-\vec{x}) = -f(\vec{x})$$

Estas son dos propiedades que se conservan en los espacios vectoriales que son que el $0$ de un espacio va a parar al espacio del otro y el opuesto de un espacio va a parar al opuesto de otro preserva estas propiedades o la estructura de un espacio vectorial, los $0$ van a parar con $0$, los opuestos van a aparar a opuestos

## Más sobre aplicaciones lineales

**Proposición.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales (Construidos sobre el mismo cuerpo) y $f:E\longrightarrow F$ una aplicación lineal. Entonces

- Si $H$ es un subespacio vectorial de $E$ (del subespacio vectorial de partida), entonces $f(H)$ es subespacio vectorial de $F$. Es decir la imagen del espacio vectorial es todo un espacio vectorial este resultado nos dice que si dentro de un espacio vectorial buscamos un subespacio son con aplicarle $F$ a ese subespacio construiremos todo un conjunto en el espacio de llegada que por defecto no tendria por que ser un subespacio vectorial pero lo es, la imagen de un subespacio vectorial si la aplicacion es lineal es un subespacio vectorial y lo que extraemos de aqui es que las aplicaciones linealesconservan bien la estructura, no quiere decir que la imagen de un opuesto sea el opuesto de la imagen o $f(0)=0$ si no que si tomamos todo  un subespacio y le aplicamos $F$ lo que nos sale es un subespacio vectorial del espacio de llegada
- Si $K$ es un subespacio vectorial de $F$ (del espacio de llegada), entonces (La antimagen) $f^{-1}(K)$ es un subespacio vectorial de $E$ o sea no es que ademas transforme espacios en espacios, es que la antimagen de un espacio tambien es un subespacio vectorial, tanto tirando elementos para adlenante con $F$ o para atras desde $F$ hasta $E$, lo que obtenemos en el caso de transformar subespacios, son subespacios vectoriales, asi que existe una forma de relacionar los subespacios vecotoriales de partida con los de llegada, la imagen de un subespacio es un sobespacio, la antimagen de un subespacio vecctorial tambien es un subespacio vectorial 
- Si $G$ es otro $\mathbb{K}$-espacio vectorial y tubieramos dos aplicaciones lineales $f:E\longrightarrow F$, $g:F\longrightarrow G$ son aplicaciones lineales, entonces la aplicación (es la composicion que se lee al revez, $f$ compuesta con $g$) $g\circ f:E\longrightarrow G$ es lineal, que basicamente seria tomar un elemento de $E$ aplicarle $F$ y a $F$ de ese elemento aplicarle $G$, este resultado nos dice que podemos ir empalmando aplicaciones lineales, las unas con las otras e ir transformando subespacio de $E$ a subespacio de $G$ pasando por $F$ entre medias

**Ejercicio 2**

Demostrar formalmente esta Proposición. (Para demostrar que algo es un subespacio vectorial se tienen que demostrar que el $0$ pertenence, que la suma de elementos pertence y que el producto por escalar tambien pertenence) por ejemplo en este caso si tenemos dos elemenos del suespacio $f(H)$ la suma de ellos tambien pertence a $f(H)$ y el producto por un escalar por un elemento de $f(H)$ tambien pertenence al $f(H)$.

**¡Ojo!** Denotaremos por (Esta $L$ gotica de $E$ a $F$ el conjunto de todas las aplicaciones lineales definidas entre $E$ y $F$ que sean lineales por ejemplo la identidad podria ser una en el caso que el espacio de partida y de llegada fuera el mismo, otra la funcion que envia todo a $0$, en general cualquier funcion que sea que pase entre dos espacios $E$ y $F$ que sea lineal, forma parte de el conjunto de todas las aplicaiciones) 

$\mathcal{L}(E,F) = \{f:E\longrightarrow F\ |\ f\ \text{ es lineal}\}$

Fijese que los elementos que viven en $\mathcal{L}$ son fuciones lineales pues resulta que $\mathcal{L}(E,F)$ el conjunto de todas las aplicaciones lineales entre $E$ y $F$ con las operaciones de suma de funciones  y con el producto de un escalar por la funcion es un $\mathbb{K}$-espacio vectorial, asi que todas las funciones lineales las juntamos todas 

**Ejercicio 3**

Demostrar formalmente que $\mathcal{L}(E,F)$ (El conjunto de todas las aplicaciones lineales entre $E$ y $F$) junto con las operaciones

$$(f+g)(x) = f(x)+g(x)\quad f,g\in\mathcal{L}(E,F)$$, se pronuncia como $f+g$ de $x$ es $f(x)+g(x)$ para cualquier elemento $f$ y $g$ del conjunto de aplicaciones lineales

$$(\alpha f)(x) = \alpha\cdot f(x)\quad f\in\mathcal{L}(E,F),\ \alpha\in\mathbb{K}$$, se dice  $(\alpha f)$ aplicado a $x$ es $\alpha\cdot f(x)$ para cualquier $f$ del conjunto de aplicaciones lineales y $\alpha$ cualquier escalar del cuerpo

es un $\mathbb{K}$-espacio vectorial

Esto verifica todas las propiedades de un espacio vectorial

Bastará con demostrar que la suma de aplicaciones lineales es lineal y que el producto de un escalar por una aplicación lineal es también lineal.

## Ejercicio 2

Teníamos $f$ aplicación lineal y $H$ un subespacio vectorial de $E$ Y también teníamos $\mathbb{K}$ es un subespacio vectorial de $F$. Se supone que $f$ esta definida de un espacio vectorial $E$ a un espacio vectorial $F$: $f:E\longrightarrow F$

Básicamente lo que nos decía la proposición era que: 

**1°** la imagen de un subespacio vectorial $f(H)$ es subespacio vectorial de $F$. Porque vive dentro del espacio vectorial de partida $E$ y su imagen es de $F$

**2°** la antimagen de un subespacio vectorial $f^{-1}(\mathbb{K})$ es súbespacio vectorial de $E$.

**3°** Si $G$ es otro espacio vectorial de modo que tenemos $f:E\longrightarrow F$, $g:F\longrightarrow G$ y evidentemente $f$ y $g$ son espacios vectoriales y lineales entonces la composición de $f$ compuesta con $g$: $g\circ f$ que iría definida desde $E$ hasta $G$ directamente sin pasar por $F$: $g:F\longrightarrow G\longrightarrow g\circ f:E\longrightarrow G$, también es lineal.

Vamos a demostrar estos tres apartados:

**1°:** Veamos que $f(H)$ es subespacio vectorial de $F$,(que la imagen de un espacio vectorial es un subespacio vectorial).

Si $H$ es subespacio vectorial de $E$ podemos considerar $y_1,y_2$ dos elementos de $f(H)$ dos elementos que vivan de momento en ese conjunto, queremos ver que ese conjunto es más que un conjunto, que es su subespacio vectorial.

Pues tomemos dos elementos de $f(H)$ y consideremos $\alpha,\beta\in\mathbb{K}$ sobre el cual está construido el espacio vectorial, en nuestro caso por definición $f(H)$ es el conjunto de elementos que son imagen de un elemento de $H$ entonces: $\exists x_1,x_2\in H$ tal que $f(x_1)=y_1$ y $f(x_2)=y_2$, simplemente existen dos elementos de $H$ tales que cuando le aplicamos $F$ nos da el $y_i$ respectivo.

Entonces en este caso tendríamos que $\alpha y_1+\beta y_2$, se podría escribir como $\alpha f(x_1)+\beta f(x_2)$ pero como $\alpha$ es lineal podemos meter los escalares dentro y juntar las sumas de modo que nos quedaría: $f(\alpha x_1\beta x_2)$ esto es porque $F$ es una aplicación lineal, pero esto que nos quedo aquí pertenece a $f(H)$ porque este elemento $\alpha x_1\beta x_2$ que tenemos es de $H$ y además una combinación lineal de elementos de $H$ es de $H$ por la propia definición de subespacio vectorial. Así que en efecto la imagen pertenecerá a $f(H)$ o lo que es lo mismo, tenemos demostrado que $f(H)$ es un subespacio vectorial porque tomando combinaciones lineales de elementos de $f(H)$ como estos que tenemos aquí $\alpha y_1+\beta y_2$ esto es una combinación lineal de elementos de $f(H)$ resulta que pertenece a $f(H)$ que es cerrado por la combinación lineal.

**2°: ** Vamos a demostrar que $f^{-1}(\mathbb{K})$ la antimagen de un espacio vectorial de $F$ es un subespacio vectorial de $E$ y vamos a hacer lo mismo. Vamos a tomar dos elementos que sean de $f^{-1}(\mathbb{K})$ y vamos a comprobar que su combinación lineal también lo es.

Tomemos dos elementos, en este caso como estamos en el espacio de $E$ estos elementos vamos a llamarle $x_1,x_2$ que son dos elementos de la antimagen $f^{-1}(\mathbb{K})$ para saber que significa esto tomamos $\alpha,\beta\in\mathbb{K}$ y por definición de $f^{-1}(\mathbb{K})$ lo que vamos a tener es $f(x_1),f(x_2)\in\mathbb{K}$, si $x_1,x_2$ son la antimagen de $\mathbb{K}$ es porque cuando les aplicamos la $F$ pertenecen a esa imagen $\mathbb{K}$ y si pertenecen a $\mathbb{K}$ como $\mathbb{K}$ es un súbespacio vectorial, una combinación lineal suya también será de $\mathbb{K}$, por ejemplo: $\alpha f(x_1) +\beta f(x_2) \in\mathbb{K}$. Pero en este caso si $F$ es una aplicación lineal esta expresión que tenemos $\alpha f(x_1) +\beta f(x_2) \in\mathbb{K}$ técnicamente es lo mismo que $f(\alpha x_1+\beta x_2)$ y esto acabamos de decir que pertenece a $\mathbb{K}$. Pero si todo esto es de $\mathbb{K}$ es que esa combinación lineal $\alpha x_1+\beta x_2$ es un elemento de la antiimagen $f^{-1}(\mathbb{K})$ o sea que en cualquier caso la combinación lineal de elementos de la antimagen $x_1,x_2\in f^{-1}(\mathbb{K})$ pertenece en efecto a la propia antimagen $\alpha x_1+\beta x_2\in f^{-1}(\mathbb{K})$ así que es la definición de que sea un subespacio vectorial.

**3°:** Partiremos del espacio de partida, es decir queremos comprobar que la composición de $g\circ f$ que nos lleva del primer espacio $E$ al tercero $G$ pasando por el segundo evidentemente: $g\circ f:E\longrightarrow G$ queremos demostrar que esto es lineal, tomamos dos elementos del espacio de partida $x_1,x_2\in E$ y vamos a tomar dos escalares $\alpha,\beta\in\mathbb{K}$ sobre el cual están montados los tres espacios vectoriales $E$,$F$ y $G$, Si aplicamos la composición $g\circ f(\alpha x_1 + \beta x_2)$, entonces esto resultaría en aplicar $g$ al resultado $g(f(\alpha x_1 + \beta x_2))$ asi que habría que aplicar primero $f$ a la combinación lineal y luego aplicar $g$ a ese resultado, pero como $f$ es lineal, seria lo mismo que hacer $g(\alpha f(x_1) + \beta f(x_2))$ y si ahora aplicamos que $g$ es lineal, pues nos quedaría la combinación $\alpha g(f(x_1))+\beta g(f(x_2))$ Y si reescribimos la composición nos queda $\alpha g\circ f(x_1) + \beta g\circ f(x_2)$. Por tanto la composición $g\circ f(\alpha x_1 + \beta x_2)$ aplicada a una combinación lineal es una combinación lineal de las composiciones $\alpha g\circ f(x_1) + \beta g\circ f(x_2)$ o lo que es lo mismo, la composición de aplicaciones lineales resulta en una aplicación lineal.

Estas tres propiedades junto con las dos que ya hemos visto la imagen del cero es el cero y la imagen del opuesto es el opuesto de la imagen son las propiedades básicas que nos permiten caracterizar todo lo que puede ocurrir gracias a una aplicación en espacios y subespacios.

# Núcleo e imagen de una aplicación lineal

## Núcleo

**Núcleo.** Sea la aplicación lineal $f$

$$\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)
\end{matrix}$$
Sea la aplicacion lineal $f$ definida entre los conjuntos $E$ y $F$ De modo que a cada elemento de $x$ lo mandamos a un cierto $f(x)$

Se denomina núcleo de la aplicacion lineal $f$ y se denota como (Es por el ingles de **Kernel**) $\ker(f)$ o $\text{Nuc}(f)$ es el conjunto de todos los elementos de $E$ que es el espacio de partida tales que su imagen coincide con el cero de $F$ que es el espacio de llegada, es decir:

El $\ker(f)$ son todos los vectores $x$ del espacio de partida $E$ tales que $f(\vec{x}) = \vec{0}_F$

$$\ker(f) = \{\vec{x}\in E:\ f(\vec{x}) = \vec{0}_F\}$$
Nosotros ya sabemos que el $0$ va a parar al $0$ esto es porque lo hemos demostrado, simepre que la aplicacion sea lineal $f(0)=0$, la pregunta seria si existen mas vectores que al aplicar $f$ van a parar al $0$, para eso existe el **Nucleo** que el nucleo de una aplicacion lineal $f$ seran todos los vectores del espacio de partida tales que cuando les apliquemos la funcion $f$, la aplicacion lineal, nos de el vector $0$ del espacio de llegada

**Teorema.** Sea la aplicación lineal 

$$\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)
\end{matrix}$$

Entonces el nuceo de una aplicacion lineal $\ker(f)$ es un subespacio vectorial de $E$, es decir si tomamos todos los vectores tal que si les aplicamos $f$ y van a parar al $0$ esto es un subespacio vectorial, Para demostrarlo es facil:

Para que sea subespacio el $0$ tiene que pertenencer, $f(0)=0$, si por lo tento el $0$ pertenence al nucleo, si dos elementos son del nucleo, la suma tiene que ser del nucleo y si dos elementos son del nucleo quiere decir que $f(x)$ y $f(y)$ da $0$ pero quedara $f(x)+y$ como $f$ es lineal $f(x)+y$ sera $f(x)+f(y)$ que sera $0+0=0$ por tanto $f(x)+y = 0$ y $f\lambda\cdot x$ si $x$ es del nucleo pues $f\lambda\cdot x$ como $f$ es lineal es $\lambda\cdot f x$ pero como $x$ es del nucleo $fx=0$ asi que nos quedaria $\lambda\cdot 0 =0$ asi que se cumple las tres propiedades para que el **nucleo sea un subespacio vectorial de $E$**, se puede escribir mas detallado paso por paso las tres propiedades que debe de cumplir el nucleo para que sea un subespacio vectorial

## Imagen

**Imagen.** Sea la aplicación lineal 

$$\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)
\end{matrix}$$

Se denomina imagen de $f$ (imagen de la aplicacion lieneal) y se denota por $\text{Im}(f)$ al conjunto de elementos del espacio vectorial de llegada $F$ que tienen una anti-imagen atraves de la aplicacion lineal $f$, es decir:

La imagen de $f$ son los elementos del espacio de llegada, los $\vec{y}\in F$, tales que existe un elemento $x$ del espacio de partida $E$ que al aplicar $f(x)$ nos da el vector $y$ tal que una forma rapida de decir la definicion es que la imagen son todos los elementos del espacio de llegada a los que les llega almenos una flecha, son los elementos del espacio de llegada $F$ con sus elementos $y$

$$\text{Im}(f) = \{\vec{y}\in F:\ \exists\vec{x}\in E:\ f(\vec{x}) = \vec{y}\}$$

**Teorema.** Dada la aplicación lineal 

$$\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)
\end{matrix}$$

Entonces $\text{Im}(f)$ es un subespacio vectorial pero en este caso de $F$ (del espacio de llegada) porque la imagen tiene como elementos que viven dentro de $F$.

Para demostrarlo, ¿El $0$ pertenence a la imagen? Si, porque resulta que al $0$ le llega flecha del $0$ precisamente del espacio de partida,asi que el cero vive en la imagen porque le llega flecha, si a uno o dos elementos les llega flecha, se puede demostrar gracias a que $F$ es lineal, usando la definicion de la imagen que le llega flecha para todo elemento $y$ de la imagen existe un $x$ del espacio de partida $E$ talque $f(x)=y$ y ademas utilizamos que $F$ es una aplicacion lineal

**Teorema.** Si $E$ es un espacio vectrorial de dimensión finita $n$ y se tiene la aplicación lineal $f:E\longrightarrow F$. Entonces, $\text{Im}(f)$ tambien es un espacio vecorial de dimensión finita menor o igual que $n$, en otras palabras, la dimencion de la $\text{Im}(f)$ es menor o igual que la dimencion del espacio de patida $E$

$$\dim(\text{Im}(f))\le n$$

Intuitivamente una aplicacion sea lineal o no al final es una serie de flechas (de correspondencia) entre elementos de partida y de llegada, recordar que para que sea una aplicacion una cosa que tenia que cumplir era que todo elemento de partida tenia que salirle una flecha pero le tenia que salir una, entonces si solo nos sale una flecha lo que no puede pasar es que lleguen mas flechas de las que salen por tanto si nosotros salimos de un espacio vectorial que tiene dimencion $n$ seria inverosimil que en el esapcio de llegada tubieramos dimencion $n+5$ habriamos crecido en dimencion y por lo tanto  haviramos crecido en numero de flechas, asi que es normal que se cumpla este teorema porque al final como cada elemento del espacio de salida solo puede salir una flecha, la dimencion del espacio de llegada (la imagen) tiene que ser menor o igual que $n$ (es la dimencion delespacio de salida), esto significa que si tenemos un espacio vectorial de dimencion $3$ la dimencion de la imagen de $f$ donde $f$ sea cualquiera aplicacion lineal que sea definida entre $E$ y $F$ tendra como mucho dimencion $3$ (No podra crecer de dimencion), tambien pasa otra cosa ya que como la imagen vive dentro del espacio vectorial $F$ la dimencion no podra ser superior a la de $F$, algo que se puede decir mas generico es que la dimencion de la imagen de $f$ es menor o igual que el minimo entre la dimecion del espacio de paritida y el de llega no se puede salir del espacio de llegada, tampoco nos podemos salir de la dimencion del espacio de salida por que significa que han llegado mas flechas de las que podrian haber salido del espacio vectorial de salida

# Clasificación de una aplicación lineal

Sea $f: E\longrightarrow F$ una aplicación lineal

**Monomorfismo.** Si $f$ es inyectiva (que sea inyectiva es que dos elementos diferentes del espacio de salida van a parar a dos elementos diferentes del espacio de llegada, en otras palabras nadien del espacio de llegada puede recibir dos flechas)

**Epimorfismo.** Si $f$ es exhaustiva (que todo elemento del espacio de llegada teniendo que recibir almenos una flecha, nadien se podia quedar sin flecha, sin correspondencia en el espacio de llegada)

**Isomorfismo.** Si $f$ es biyectiva o sea que sea Inyectiva y exhaustiva a la vez, es decir todos los elementos del espacio de salida tienen una correspondnecia unica con todos los elementos del espacio de llegada, nadien se queda sin que le salga flecha, nadien se queda sin que le llegue flecha y nadien recibe dos flechas, asi entre todos los de salida y todos los de llegada

**Endomorfismo.** Si $f$ va de un espacio al mismo. Es decir, si $f$ es tal que $$f: E\longrightarrow E$$, Simplemente es una aplicación definida de un espacio en sí mismo por ejemplo la identidad. aquella que era $f(x)=x$ que estaba definida en un espacio vectorial $E$ en sí mismo. entonces esto es una aplicación lineal definida del propio espacio vectorial $E$ que tengamos en si mismo.

**Automorfismo.** Si $f$ es un endomorfismo biyectivo, No es más que un endomorfismo, una aplicación lineal definida de un espacio en sí mismo que sea biyectivo, un endomorfismo biyectivo se le llama un automorfismo.

La idea de estas palabras es que lo vamos a poder relacionar precisamente con el núcleo y con la imagen.

De momento definamos:

**Espacios vectoriales isomorfos.** Si existe un isomorfismo $f:E\longrightarrow F$. Lo denotamos por $E\cong F$, si existe una aplicacion biyectiva entre $E$ y $F$ se dice que estos son espacios vectoriales isomorfos

**Proposición.** Sea $f:E\longrightarrow F$ una aplicación lineal, entonces 

- $f$ es monomorfismo (es inyectiva) si, y solo si, (El nucleo de $f$) $\ker(f) = \{\vec{0}_E\}$, Ya se dijo que en el núcleo cabían todos los vectores que iban a parar al cero del espacio de llegada y que por ejemplo el cero de espacio de partida siempre viví ahí porque una propiedad de que $f$ fuera lineal es que $f$ de $f$ sea cero. En otras palabras si el único vector que pertenece  al núcleo es el vector cero y nadie más, En ese caso $f$ será un monomorfismo o en otras palabras $f$ será inyectiva 
- $f$ es epimorfismo si, y solo si, $F = \text{Im}(f)$, El espacio de llegada de $f$ coincide con toda la imagen, esto es más sencillo de interpretar porque al final que sea Epimorfismo significa que es exhaustivo que sea exhaustivo significa que a todo elemento del espacio de llegada le llega Flecha, todo elemento del espacio de llegada es $f$ y que le llegue flecha era al concepto de pertenecer a la imagen así que $F$ será un Epimorfismo si sólo si la imagen de la aplicación lineal coincide con el espacio de llegada 

**Ejercicio 4**

Demostrar formalmente esta Proposición.

**Teorema.** Sean $E$ y $F$ espacios vectoriales de dimensión finita sobre $\mathbb{K}$ y $f:E\longrightarrow F$ una aplicación lineal, entonces son equivalentes:

- $f$ es un isomorfismo, es decir $f$ es biyectivo
- $\dim(E) = \dim(F)$ y $ker(f)=\{0\}$, son como un matrimonio, todo elemento del espacio de salida tiene un unico elemento en el espacio de llegada se traduce en que las dimenciones tienen que ser iguales, no se puede tener mas de una que de la otra, tiene que haber el mismo numero de elementos en cada lado, en otras palaras la dimencion de $E$ tiene que ser igual a la dimencion de $F$
- $\ker(f) = \{0_E\}$ y $Im(f) = F$, En otras palabras se cumple la condición de monomorfismo y la condición de epimorfismo

**Proposición.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales isomorfos. Sea $f:E\longrightarrow F$ un isomorfismo (que exista entre esos dos), entonces existe la aplicación inversa de $f$, llamemosla $f^{-1}:F\longrightarrow E$ y es también lineal. ppr tanto si $E$ y $F$ son isomorfos y $f$ es un isomorfismo definido entre $E$ y $F$, lainversa qe es la aplicacion que en lugar de mandar los elementos de $E$ se los mandamos a $F$ tambien es una aplicacion lineal e isomorfismo por propia construccion

**Ejercicio 5**

Demostrar formalmente esta Proposición.

# Teorema del Rango

En el caso de espacios vectoriales de dimensión finita, los subespacios núcleo e imagen de una aplicación lineal $f:E\longrightarrow F$ están siempre relacionados por el siguiente teorema conocido como el `Teorema del Rango`

**Teorema del Rango.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales de dimensión finita y $f: E \longrightarrow F$ una aplicación lineal. Entonces, $\ker{(f)}$ e $\text{Im}(f)$ tambien son espacios vectoriales de dimensión finita y se relacionan a travez de la dimencion del espacio vectorial de partida $E$ igual a la dimencion del nucleo de $F$ mas la dimencion de la imagen de $F$

$$\dim(E) = \dim(\ker(f))+\dim(\text{Im}(f))$$.

Basicmaente lo que nos dice es que la dimencion del espacio de partida es la suma de la dimencion del nucleo mas a dimencion de la imagen, si el nucleo es muy grande significa que la imagen tendra que ser pequeña para compensar la dimencion de $E$ o si la imagen es muy grande es porque el nucleo es pequeño en cuanto a dimeniones, hay que recordr que la dimencion coincide co n el numero de  del que forma parte una base del espacio vectorial por tanto el numero total de vectores que conforman una base del espacio vectorial $E$ es igual al numero de vectores que forman una base del nucleo mas el numero de vectores que forman una base de la imagen de la aplicacion lineal

**Ejercicio 6**

Demostrar formalmente este Teorema.

Pero porque se llama teorema del rango si en la exprecion de arriba no hay rangos ni matrices para poder hablar de rangos, pues se llama

**Rango de una aplicación lineal.** Sea $f: E\longrightarrow F$ una aplicación lineal con $\dim(E)=n$. Se denomina rango de $f$ a la dimensión del subespacio vectorial imagen de $f$, es decir si $f$ es una aplicacion lineal definida entre $E$ y $F$ espacios vectoriales y la dimencion de $E$ es igual a $n$ (es finita) se llama rango de $f$ coincide con la dimencion de la imagen de $f$

$$\text{rg}(f) = \dim(\text{Im}(f))$$

Esto significa que volvemos a tener este cocepto del rango de una aplicacion lieal que es la dimencion del espacio vecotiral imagen de $f$ esto signica que los conceptos de rango estan relacionados, y ademas este rango de una aplicacion lineal si queremos coincide con el nuemro de vectores linealmente independientes es decir que forman una base de la imagen de $f$

**Observación.** Fijaos que ahora el `Teorema del Rango` queda como: Dimencion del espacio vectorial $E$ es igual a la dimencion del nucleo de $f$ mas el rango de $f$ y asi es como se le entinede porque se le denomina el teorema del rango, esta es la forma mas normal que se escribe porque hace referncia al rango de la aplicacion lineal, esta es una forma directa de sacar el rango despejandolo de esta formula

$$\dim(E) = \dim(\ker(f))+\text{rg}(f)$$

## Consecuencias del Teorema del Rango

El `Teorema del Rango` nos permite ver una caracterización sencilla de cuándo dos espacios vectoriales finitos son isomorfos: (utilizando la formula anterior se puede deucior los sigueintes resutados)

**1°- Proposición.** Dos $\mathbb{K}$-espacios vectoriales de dimensión finita $E,F$ son isomorfos (Es decir existe una biyeccion, una aplicacion lineal biyectiva entre ellos) si, y solo si, $\dim(E)=\dim(F)$, un detalle si son isomorfos la aplicacion es inyectiva y exhaustiva, viendo la formula del teorema del rango, si es inyectiva el nucelo es $0$ por tanto $dim(\ker(f))=0$ y la dimencion de $f$ coincide con todo si es exahustiva el rango de $f$ o la imagen es igual a todo $f$ asi que nos quedaria $dim(E)=dim(f)$

**Ejercicio 7**
Demostrar formalmente esta Proposición.

Una versión similar al resultado anterior, pero referido a cuándo una aplicación $f$ concreta es un isomorfismo es la siguiente:

**2°- Proposición.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales de dimensión finita y sea $f:E\longrightarrow F$ una aplicación lineal. Entonces $F$ es un isomorfismo si, y solo si, $\dim(E) = \dim(F)$ y $\ker{(f)} = \{0\}$, al principio puede parecer que es lo mismo que la anterior pero no es asi. Son isomorfos si y solo si la $dim(E)=dim(F)$, Aqui se añade que ademas el nucleo de $f$ sea igual a $0$ esto queire decir que aqui presuponemos de la existencia de una aplicacion lineal, si tenemos una aplicacion lineal podemos decir que $E$ es isomorfo a $F$ atraves de esa aplicacion lineal si la dimenciones son iguales y el nucleo de la aplicacion lieneal es igual a $0$, sin embargo en la propocicion anteriro si no tenemos ninguna aplicacion lineal simplemente podemos determinar que $E$ y $F$ son son isomorfos si las dimenciones son iguales, **Esta proposicion utiliza la exisitencia de una aplicacion lineal**

**Ejercicio 8**
Demostrar formalmente esta Proposición.

**3°- Proposición.** (Podemos hablar de las bases deun espacio vectorial ya que todo espacio vectorial tenia una base) Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales y sea $f:E\longrightarrow F$ una aplicación lineal, $E$ puede tener una base y $F$ puede tener otra base

- si $f$ es un monomorfismo (Si es inyectiva) y $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n$ son un conjunto de vectores linealmente independientes del espacio vectorial $E$ (No exijimos que sea base simplemente que sean linealmente independientes) entonces, la imagen de ellos:

$$f(\vec{u}_1),f(\vec{u}_2),\dots,f(\vec{u}_n)$$

son tambien vectores linealmente independientes pertenecientes a $F$

(La imagen de vectores linealmente independientes del espacio vectorial $E$ da como resultado vectores linealmente independientes pertenecientes al espacio vectorial $F$)

- si $f$ es un epimorfismo (si es exhaustiva) y $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n$ son un conjunto generador de $E$, entonces $f(\vec{u}_1),f(\vec{u}_2),\dots,f(\vec{u}_n)$ generan el espacio vectorial $F$

(La imagen de los vectores del conjunto generador de $E$ es un generador de $F$)

**Ejercicio 9**
Demostrar formalmente esta Proposición.

**Observación.** Notad que si se cumple que $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n$ generan el espacio vectorial $E$ y $f:E\longrightarrow F$ es una aplicación lineal cualquiera (No necesariamente inyectiva o exahustiva, ni nada ), entonces lo que se puede afirmar es  $f(\vec{u}_1),f(\vec{u}_2),\dots,f(\vec{u}_n)$ son un sistema generador de la $\text{Im}(f)$ que es donde van a parar todos esos vectores, la imagen de un sistema generador seria un sistema generador de la imagen en este caso. En el caso de ser $f$ exhaustiva, $\text{Im}(f) = F$ y, por tanto la propocicion **2°**, en este caso  $f(\vec{u}_1),f(\vec{u}_2),\dots,f(\vec{u}_n)$ generarían (Toda la imagen o lo que es lo mismo) todo $F$

**4°- Proposición.** Sea $f:E\longrightarrow F$ una aplicación lineal donde $E,F$ son dos $\mathbb{K}$-espacios vectoriales con $E$ de dimensión finita $n$. Sea $G$ un subespacio vectorial de $E$. Entonces se verifica que el teorema del rango para el subespacio en el sentido de que la dimencion del subespacio vectorial $G$ es igual a la suma de la dimencion de $G$ interseccion el nucleo de $f$ mas la dimencion de $f$ de $G$ que seria la imagen por asi decir del subespacio $G$.

$$\dim(G) = \dim(G\cap \ker(f))+\dim(f(G))$$

fijaros que lo unico que se ah hecho es restringir el teorema del rango a un subespacio vectorial $G$ del espacio vectorial de partida $E$ de modo que la dimencion del subespacio $G$ seria la dimencion del torzo de nucleo que cae en $G$ (es donde esta la interseccion) mas la dimencion de la imagen de $G$ osea $dim(f(G))$ recuerda que esto es un subespacio vectoria de $F$ y tambien que si $G$ y el nucleo son subespacios su interseccion tambien es subespacio entonces la formula esta bien definida

Con todo esto se ah relacionado espacios, subespacios, nucleos, imagenes, detectar funciones inyectivas,exhaustivas, isomorfismos que es lo que nos interesa buscar , isomorfismos en esapcios vectoriales

**Ejercicio 10**
Demostrar formalmente esta Proposición.

## Ejemplo de Aplicaciones Lineales II

Se van a ver ejemplos de monomorfismos, epimorfismos, isomorfismos, cómo calcular núcleos imágenes y demás con ejemplos concretos de aplicaciones lineales.

cuando se explico los números complejos se dibujaba un plano cartesiano de donde estaban los números complejos con módulos y argumentos, donde en el eje horizontal está la parte real $\mathbb{R}_e$ del número complejo arriba en el eje vertical está la parte imaginaria $\mathbb{I}_m$ del complejo. Pero esto nos recordará a $\mathbb{R}^2$ que es el plano con dos coordenadas $x,y$. Pues resulta que en efecto $\mathbb{R}^2$ el conjunto de coordenadas reales de orden $2$ es isomorfo a los números complejos, son dos $\mathbb{R}-e.v$ isomorfos. Y un isomorfismo explícito entre $\mathbb{R}^2$ y los complejos: $f:\mathbb{R}^2\longrightarrow\mathbb{C}$ biene dado por la pareja de números reales $(x,y)$ se les hace corresponder el número complejo con parte real $x$ y parte imaginaria $y$, ese sería un posible isomorfismo.

$$\begin{matrix}f:&\mathbb{R}^2\longrightarrow\mathbb{C}\\
&(x,y)\mapsto x+iy
\end{matrix}$$

Se puede comprobar como ejercicio que en efecto esta aplicación es lineal y que además es biyectiva que establece el primero de los isomorfismo.

**Otro ejemplo: **

En este caso vamos a tomar $f:\mathbb{R}^3\longrightarrow\mathbb{R}^2$ definida como al conjunto de puntos $(x,y,z)$ de $\mathbb{R}^3$ les hacemos corresponder $(x+y-2z,x-y+z)$

$$\begin{matrix}f:&\mathbb{R}^3\longrightarrow\mathbb{R}^2\\
&(x,y,z)\mapsto (x+y-2z,x-y+z)
\end{matrix}$$

Esto es evidentemente una aplicación y es lineal, de esta forma con polinomios de grado 1 y lo podriamos demostrar que es lineal, ¿Será inyectiva o exhaustiva o biyectiva? Habrá que calcular el núcleo y la imagen.

Para el núcleo hay que buscar las coordenadas $(x,y,z)$ de un vector, tales que cuando aplicamos $f(x,y,z)$ nos de el vector $(0,0)$ del espacio de llegada, entonces esa sería la condición para que $(x,y,z)$ perteneciera al núcleo. Esto es lo mismo que igualar las dos componentes:
$$
\left\{
x + y - 2z  = 0 \atop
x - y + z  = 0
\right.
$$

Si resolvemos este sistema de ecuaciones lineal homogéneo. Si sumamos estas dos ecuaciones nos desaparece la $y$ y nos quedaría $2x-z=0$ se traduce en $z=2x$ y si sustituimos esto en la primera ecuación o en la segunda, En este caso en la segunda nos quedaría $x-y+x2=0$, si despejamos nos queda $y=3x$. Por tanto el núcleo $\ker(f)$ estaría formado por los vectores donde la $x$ sería una variable libre, la $y$ sería $3x$ y la $Z$ sería $2x$ con $x$ evidentemente un número real: $\ker(f)\{x,3x,2x\},\ x\in\mathbb{R}$ y esto con el tema de espacios vectoriales, es el espacio vectorial generado por el vector $\langle1,3,2\rangle$ sacandole $x$ factor común, lo cual nos permite decir que $dim(\ker(f))=1$, Por teorema del rango nos dice que la dimensión del espacio de partida que es $3$ en este caso, sería igual a la dimensión del núcleo más la dimensión de la imagen, por tanto de aquí sacaríamos que $dim(\text{img}(f))=2$ para que sume $3$ pero como la imagen de $f$,$img(f)$, es un subespacio vectorial del espacio de llegada que es $\mathbb{R}^2$ tiene dimensión $2$ pero la $img(f)$ también tiene dimensión $2$, entonces se puede decir de forma explícita que esta aplicación lineal tiene por imagen de $f$ igual a todo el espacio vectorial de llegada: $img(f)=\mathbb{R}^2$, por lo tanto no sería un monomorfismo porque la dimensión de la imagen no es cero, pero sí sería un epimorfismo porque la imagen es todo el espacio vectorial de llegada, **Aquí tenemos explícitamente cómo calcular un núcleo e imagen.**

**Otro Ejemplo: **

$$
\begin{matrix}f:&\mathbb{R}^3\longrightarrow\mathbb{R}^4\\
&(x,y,z)\mapsto (x+y+z,x+z,x+y,x)
\end{matrix}
$$
 
Esto sería una aplicación es lineal, **¿Cómo serían los vectores del núcleo?** un vector sería del núcleo si $g(x,y,z)$ fuera igual al vector cero del espacio de llegada es decir: $g(x,y,z)=(0,0,0,0)$ y esto se traduce en que las cuatro componentes deberian ser cero:

$$
\left\{
\begin{array}{rcl}
     x+y+z & = & 0
  \\ x+z & = & 0
  \\ x+y & = & 0
  \\ x & = & 0
\end{array}
\right.
$$

Vemos que de forma trivial $x=0$ a partir de la última, si sustitimos en la tercera nos queda $y=0$, si sustituímos en la segunda nos queda $z=0$ y en otras palabras tendríamos que el único vector que pertenece al núcleo de $G$.$\ker(G)=\{0,0,0\}$ o lo que es lo mismo sería un núcleo trivial y por tanto $G$ sería un monomorfismo.

¿Cómo sabemos que no es Epimorfismo?:

Aplicando el teorema del rango, la dimensión del espacio de partida $dim(E)=3$ entonces la dimensión del espacio de partida debería ser igual a la dimensión del núcleo que es $0$ más la dimensión de la imagen de $f$. $3=dim(E) = 0 +dim(img(f))$ De aquí deducimos directamente que $dim(img(f))=3$, en este caso la imagen es un subespacio vectorial del espacio de llegada, pero el espacio de llegada es $\mathbb{R}^4$ por tanto si la imagen tiene dimensión $3$ nunca podrá ser igual a un espacio de dimensión $4$. En este caso si que es inyectiva pero no puede ser exhaustiva porque la imagen tiene dimensión $3$, sin embargo la aplicación tiene como punto de llegada un espacio vectorial de dimensión $4$.

**Otro Ejemplo: **

Si $E$ es un $\mathbb{K}$ espacio vectorial cualquera y $F,G$ son subespacios vectoriales de $E$ tales que la intersección es nula, $F\cap G = \{0\}$, el único vector que pertenece a la vez al subespacio vectorial $F$ y $G$ es el vector cero.

Hemos visto ya anteriormente que se puede construir la suma directa de $F$ y $G$ como $\mathbb{K}$ espacios vectoriales y también como subespacios vectoriales de modo que podríamos decir que $E=F\oplus G$ segun el tema de espacios vectoriales, pues resulta que pensado $F\oplus G$ como un $\mathbb{R}$ espacio vectorial donde tenemos componentes $(x,y)$ que pertenecen a uno o a otro y pensado ese mismo $F\oplus G$ que lo ponemos igual como la suma directa de un elemento de $F$ y otro de $G$. $x\oplus y$, si establecemos esta aplicación $\phi$: 

$$
\begin{matrix}\phi:&F\oplus G\longrightarrow F\oplus G\\
&(x,y)\mapsto x\oplus y
\end{matrix}
$$
Esto resulta entre un isomorfismo y aquí cobra sentido que dos conceptos diferentes suma directa $F\oplus G$ y producto $F\oplus G$ se llamen exactamente igual porque son isomorfos, esto ocurre mucho en matemáticas, cuando tenemos algo que se parece tanto que en este caso es isomorfo, le damos el mismo símbolo y hay un momento que uno no sabe si está trabajando con un producto cartesiano $(x+y)$ de dos espacios vectoriales o con la suma directa $x\oplus y$ de los mismos, siempre y cuando la intersección sea el vector cero.

Vamos a ver que esto es una aplicación lineal, para ver que es lineal $\phi$ hay que demostrar que $\phi$ de la suma es la suma de $\phi$ y lo mismo con el producto por un escalar simplemente $\phi(\alpha(x,y)+\beta(x',y'))$ que sería con respecto al espacio vectorial de partida una combinación lineal, pues sobre ese espacio vectorial podemos ejecutar la combinación lineal y nos quedaría $\phi(\alpha x+\beta x',\alpha y+\beta y')$ simplemente se ah sumado dentro de ese espacio vectorial. Ahora  se aplica el hecho de que $\phi$ de $xy$ es $x+y$ para establecer que $\phi$ aplicado a toda esta combinación lineal sería: $(\alpha x +\beta x')+(\alpha y +\beta y')$ reordenamos como nos interese $\alpha (x+y) + \beta (x'+y')$ y utilizando de nuevo la definición de $\phi$ remplazondo los valores que son $(x+y)$ y $(x'+y')$ es $\alpha\phi(x,y)+\beta\phi(x',y')$ de donde juntando el inicio $\phi(\alpha(x,y)+\beta(x',y'))$ y el final $\alpha\phi(x,y)+\beta\phi(x',y')$ obtenemos que en efecto $\phi$ cumple la condición de ser lineal.

Si fuese exhaustiva puede ser trivial porque $\phi$ es exhaustiva ya que cualquier elemento que escriba de la suma directa $F\oplus G$ siempre es de la forma $x+y$ con $x\in F,y\in G$, pero precisamente si tomamos esa $x$ y esa $y$ y le aplicamos $\phi$ vamos a obtener el elemento $x+y$ por tanto cualquier elemento de la suma directa, siempre tomando la parte única que es de $F$ y la parte única que es de $G$ y aplicándole $\phi$ obtendremos ese elemento de la suma directa

si fuese inyectiva también se puede demostrar fácilmente porque si comprobamos que $\phi(x,y)=0$ vamos a demostrar que si el único vector que va para el cero es el propio $(0,0)$ hemos terminado. Entonces tomemos un elemento $\phi(x,y)$ cualquiera y supongamos que es cero $\phi(x,y)=0$ en otras palabras $(x,y)$ pertenece al núcleo de $\phi$.
$\phi(x,y)=0$ significa que $x+y=0$ pero esto significa que $x=-y$ pero $x\in F$ y $y\in G$ por tanto si un elemento de $F$ es igual a otro de $G$ es que los dos elementos son de la intersección $F\cap G$ y la intersección es $\{0\}$ por hipotesis, así que no nos quedaría más remedio que $x=y=0$ fueran exactamente el vector cero y en otras palabras el único elemento que pertenecería al núcleo de $\phi$ sería el vector cero, $\ker(\phi)=\{0,0\}$ y por tanto demostraría que en efecto $\phi$ es una aplicación inyectiva y por tanto es inyectiva es exhaustiva y es lineal.

Concluimos con estos tres puntos que $\phi$ es un isomorfismo que nos permite relacionarlo todo y hablar de la suma directa $F\oplus G$ como el producto cartesiano de $F,G$ o bien como suma única de una descomposición de los vectores de cómo un trocito en $F$ más el otro trocito en $G$.

## Ejercicio encuentra una base del nucleo y la imagen de $f$

Dada una aplicación lineal $f$ definida entre $\mathbb{R}^3$ y $\mathbb{R}^2$ que dado un vector $(x,y,z)$ lo enviamos a un vector de $\mathbb{R}^2$ con coordenadas $(x+y-2z,x-y+z)$

$$
\begin{matrix}f:&\mathbb{R}^3\longrightarrow \mathbb{R}^2\\
&(x,y,z)\mapsto (x+y-2z,x-y+z)
\end{matrix}
$$

Evidentemente se trata de una aplicación que es lineal y lo que queremos encontrar: 

- **¿Quien es el núcleo de $f$?**(¿Quien sería la base del núcleo?)
- **¿Quien sería la imagen?**(una base de la imagen de $f$).

Para encontrar el núcleo sabemos que $(x,y,z)\in\ker(f)$ si y sólo si $f(x,y,z)$ en este caso es el vector $(0,0)$ como $f(x,y,z)$ sería lo mismo que decir $(x+y-2z,x-y+z)=(0,0)$ y lo vemos mejor si lo expresamos como: 

$$
\left\{
\begin{array}{rcl}
     x+y-2z & = & 0
  \\ x-y+z & = & 0
\end{array}
\right.
$$
llegados a este punto podemos resolver este sistema de dos ecuaciones y tres incógnitas por reducción por ejemplo eliminando la $\not y$ nos queda $2x-z=0$ y que $z=-2x$ Y si sustituimos en cualquiera de las dos ecuaciones nos quedaría (En la segunda ecuacion) $x-y+2x=0$ y despejando nos quedaría $y=3x$ por tanto: $y=3x,z=2x,x=x$ ya que $x$ nos queda libre y si lo sustituyemos a la primera nos quedará $x=x$ o lo que es lo mismo $0=0$ y por tanto llegarémos a la conclusión que los vectores del núcleo de $f$ se pueden escribir como: $\ker(f)=\{x,3x,2x\,\ x\in\mathbb{R} \}$ y esto es lo mismo que decir, que el núcleo viene generado por el vector $\langle 1,3,2 \rangle$ por lo tanto $dim(\ker(f))=1$ y esta sería la dimensión del núcleo.

Aplicando la fórmula de las dimensiones o el teorema del rango, la dimensión del espacio vectorial de partida de dimensión de $\mathbb{R}^3$ sería igual a la dimensión del núcleo de $f$ más la dimensión de la imagen de $f$: $dim(\mathbb{R}^3)=dim(\ker(f))+dim(img(f))$, ya sabémos que el espacio de $\mathbb{R}^3$ tiene dimensión $3$, el núcleo tiene dimensión $1$ por tanto $3=1+dim(img(f))$ despejando la dimensión de la imagen de f nos quedará que $dim(img(f))=2$ pero si nos fijámos la imagen de $f$ es un subespacio vectorial del espacio de llegada de $\mathbb{R}^2$ pero es que $\mathbb{R}^3$ ya tiene dimensión $2$ por tanto si la imagen tiene que tener dimensión $2$ dentro de un espacio de dimensión $2$ lo que podemos afirmar es que $img(f)=2$, así lo podríamos finalizar.

Pero como nos pedía de forma explícita una base de la imagen y no siempre va a pasar que sea el espacio total de llegada vamos a desarrollar cómo se **calcularía una base de la imagen**:

cualquier elemento de la imagen sera $f(x,y,z)$ igual a la expresión que tenemos arriba que era $(x+y-2z,x-y+z)$, esto es así, cualquier elemento de la imagen tendrá esta forma y esto lo podemos expresar como suma de tres vectores, el vector que se lleva a cada una de las distintas letras las $x$,$y$,$z$; $(x,x)+(y,-y)+(-2z,z)$ este sería una forma de expresar cada una de las coordenadas por separado, pero si se saca la $x,y,z$ factor común de una serie de vectores nos quedaría $x(1,1)+y(1,-1)+z(-2,1)$, podríamos decir que la imagen de $f$ está generado por estos tres vectores: $img(f)=\langle (1,1),(1,-1),(-2,1)\rangle$ es decir estos tres vectores son un sistema generador de la imagen de $f$ pero claro la imagen de $f$ vive dentro de $\mathbb{R}^2$ por tanto como mucho el número de vectores linealmente independientes va a ser $2$ y aquí tenemos $3$ entonces esto no es una base, hay que extraer de aquí como mucho dos vectores linealmente independientes, fijese que por ejemplo estos vectores $(1,1)$ $(1,-1)$ son linealmente independientes, una forma de verlo fácilmente es que si elaboramos el determinante del primer vector en columna y el segundo en la segunda columna:

$$
\left\|
\begin{array}{rcl}
     1 & 1
  \\ 1 & -1
\end{array}
\right| =-2\not=0
$$

Este determinante fácilmente nos queda $-2$ diferente de cero entonces estos dos vectores son linealmente independientes, dos vectores linealmente independientes dentro de un espacio de dimensión dos, son una base suya y por tanto podéis decir que: $img(f)=\langle(1,1)(1,-1)\rangle$ que son dos vectores linealmente independientes dentro de un espacio de dimensión $2$ por tanto son una base de la imagen.

Recuerda que ya habíamos demostrado a través del teorema del rango que la dimensión de la imagen iba a ser $2$, por tanto aquí lo único que ha sido es ratificar la búsqueda de una base de la imagen que como la imagen es el total, pero que pasa si otro dia tenemos que buscar la base de un espacio vectorial que no sea $\mathbb{R}^2$ o $\mathbb{R}^3$ o que no sea fácilmente toda la canónica sino que tendremos que extraer alguna serie de vectores para que conformen una base de la imagen pues sería exactamente así: expresamos la combinación lineal, separándo las $x$,$y$ y $z$ de la expresión analítica de la aplicación lineal esos vectores que salen que multiplica a cada una de las coordenadas sería un sistema generador y de ahí cribas el que es linealmente dependiente de los otros lo eliminamos para quedaros con los independientes y esos que sean independientes serán precisamente una base del subespacio vectorial imagen de $f$.

## Ejercicios 4 y 5

Estamos con $f$ una aplicación lineal definida entre un espacio vectorial $E$ de partida y un espacio vectorial $F$ de llegada entonces queremos demostrar muchas cosas:

**1°** es que $f$ es inyectiva o es un monomorfismo si sólo si el núcleo de $f$ se reduce al vector cero del espacio de partida $E$, $\ker(f)=\{\vec{0}_E\}$ (el núcleo siempre es un subespacio vectorial) entonces $f$ es inyectiva o un monomorfismo si el núcleo es únicamente el espacio vectorial trivial 

Para demostrarlo primero vamos a suponer que $f$ es un monomorfismo y quieremos saber si es verdad $\ker(f)=\{\vec{0}_E\}$ del espacio de partida.

Tomemos $x\in\ker(f)$ uno cualquiera, quieremos ver que este va a ser $0$. Si pertenece al núcleo podemos decir que un elemento del núcleo cumple que $f(x)=0$ y por tanto por ser $f$ lineal como $f(0)=0$ tendríamos pues básicamente que $f(x)=f(0)$, Simplemente se tendrían dos vectores tales que $f(x)=0=f(0)$, esto nos llevaría a que $f(x)=f(0)$ pero la aplicación es inyectiva por hipótesis esto es un monomorfismo entonces recuerda que en un monomorfismo si dos imágenes son iguales es porque los elementos son iguales, así que $x=0$ y como $x$ era arbitrario cualquier elemento del núcleo tiene que ser necesariamente cero.

Recíprocamente si queremos demostrar lo contrario, supongamos que $\ker(f)=0$ esto es lo que sabemos y queremos demostrar que $f$ es un monomorfismo entonces supongamos que $f(x)=f(x')$ y veamos que $x=x'$ para demostrar que es inyectiva o que es un monomorfismo si $f(x)=f(x')$ esto significa que $f(x)-f(x')=0$ pero como $f$ es lineal esto nos quedaría que $f(x-x')=0$ y esto quedría decir que $x-x'\in\ker(f)$ pero el nucleo de $f$ por hipótesis sólo contiene el vector cero: $\ker(f)=0$ así que esto nos dice que $x-x'=0$ y en otras palabras $x=x'$ de donde nos dice sin ningún problema que $f$ es inyectiva o lo que es lo mismo un monomorfismo.

**2°** $f$ es exhaustiva o epimorfismos si y sólo si la imagen de $f$ coincide con el espacio vectorial de llegada: $img(f)=F$

Que sea un Epimorfismo significa que todo elemento del espacio de llegada tiene anti-imagen pero claro todo elemento del espacio llegada es la imagen de $f$ y lo que estamos diciendo es que $img(f)=F$ por tanto todos los elementos de $f$ tienen una anti-imagen así que esa seria la demostración.

**3°** Del de arriba sale que $f$ es un isomorfismo si el núcleo de $f$ es cero y si la imagen de $f$ es todo $F$ pero a la vez que la dimensión del espacio de partida $E$ es igual a la dimensión del espacio de llegada $F$: $dim(E)=dim(F)$

Si $f$ es un isomorfismo quieremos ver que las dimensiones coinciden $dim(F)=dim(E)$
aqui significa que $E$ y $F$ son isomorfos y significa que son igual de grandes entonces el hecho que la dimensión coincida es básico.

Además si es isomorfismo es inyectiva y es exhaustivo entonces al ser inyectiva de regalo tenemos que $\ker(f)=0$ y $img(f)=F$ por tanto no hay casi nada que demostrar, simplemente se cumple que la imagen es todo $F$ y que las dimensiones coincidan.

Recíprocamente si tenemos por un lado que si $dim(E)=dim(F)$ podemos aplicar el teorema
del rango porque ahora podríamos decir que $dim(F)=dim(E)$ pero la $dim(E)$ utilizando el teorema del rango nos quedaría que se puede expresar como $dim(\ker(f))+dim(img(f)$.

Además si el rango o el kernel es cero, por hipótesis si la dimensión del kernel es cero nos queda que $dim(F)=dim(E)=dim(img(f))$ pero como la $img(f)$ es un subespacio vectorial de la misma $dim(F)$ no podriamos hacer más que $img(f)=F$ en ese caso si esto se cumple como en el apartado anterior $f$ sería exhaustiva y al tener la dimensión del núcleo igual a cero tendríamos que $F$ sería inyectiva y juntando las dos cosas tendríamos que en efecto $F$ es un isomorfismo y así es como demostraría esta parte de la proposición que se cumple que el isomorfismo es equivalente a hablar de igualdad de dimensiones o que el núcleo es cero y la imagen es el total.

En particular esto nos deja con un resultado interesante y es que si $E$ y $F$ son dos espacios vectoriales isomorfos, si $E$ es isomorfo a $F$ y $f$ es un isomorfismo entre $f:E\longrightarrow F$ entonces existe una aplicación lineal inversa $f^{-1}:F\longrightarrow E$ que también es un isomorfismo y es lineal

Para demostrarlo simplemente si $f$ es un isomorfismo esto se traduce en que $f$ es biyectiva esto significa que existe la inversa $\exists f^{-1}:F\longrightarrow E$, para ver que esta inversa también va a ser lineal podríamos tomar dos elementos $y_1,y_2\in F$. Y como $F$ sabemos que es biyectiva, sabemos que existiría sin ningún problema $\exists x_1,x_2\in E$ tales que $f(x_1)=y_1$ y $f(x_2)=y_2$, como se cumple esto, también se cumpliría que $x_1=f^{-1}(y_1)$ y $x_2=f^{-1}(y_2)$, pero si ahora hacemos $f^{-1}(y_1+y_2)$ y estos es lo mismo que hacer $f^{-1}(f(x_1)+f(x_2))$ porque $y_1$ y $y_2$ son respectivamente las imágenes de $x_1,x_2$ así es como los construimos, como son las imágenes, ahora podemos aplicar que $f$ es lineal ($f^{-1}$ es lo que estamos demostrando) esto nos quedaría $f^{-1}(f(x_1+x_2))$ y ahora así que nos quedaría la anti-imagen de la imagen de $f$ ya que $f^{-1}$ compuesto con $f$ es la identidad y la identidad es $x_1,x_2$ en este caso y esto sería lo mismo que escribir $f^{-1}(y_1)+f^{-1}(y_2)$ asi tenemos demostrado que la anti-imagen $f^{-1}(y_1+y_2)$ de la suma es la suma de las anti-imágenes $f^{-1}(y_1)+f^{-1}(y_2)$ y análogamente sin ningún problema podriamos demostrar que la anti-imagen de $f^{-1}(\alpha y)=\alpha f^{-1}(y)$ y con la primera propiedad y esta segunda $f^{-1}(\alpha y)=\alpha f^{-1}(y)$ que tenemos como ejercicio demostraría trivialmente que en efecto $f$ es lineal por tanto la anti-imagen de un isomorfismo sería un isomorfismo, sería una aplicación que también sería lineal y biyectiva.

## Ejercicios del 6 al 10

Empecemos por **el teorema de el rango**.

En el contexto de aplicaciones lineales teníamos $f$ una aplicación lineal definida dentro de un espacio vectorial $E$ y otro espacio vectorial $F$ de llegada: $f:E\longrightarrow F$ y suponíamos que la dimensión de $E$ era finita entonces el teorema nos afirmaba que el núcleo de la imagen también son espacios vectoriales de dimensión finita y que $dim(E)=dim(\ker(f))+dim(img(f))$. Para demostrarlo ya sabemos que el núcleo de $f$ es un súbespacio vectorial de $E$ que es el espacio de partida y por tanto si el espacio de partida es de dimensión finita, un subespacio suyo también es de dimensión finita.

Supongamos que $dim(\ker(f))=r$ y $dim(E)=n$ entonces del núcleo podríamos considerar una base suya, pues sería una serie de vectores $u_1,u_2,...,u_r$ estos vectores se podrían ir completando hasta formar una base de $E$ de modo que necesitaría unos vectores $u_1,u_2,...,u_r,u_{r+1},...,u_n$ y estos vectores si tomamos la imagen de los primeros $k$ vectores, Si tomamos uno de estos $u_1,u_2,...,u_r$, tomamos $f(u_i)=0$ para la $i=1,...,r$ precisamente esa es la definición de que sea un vector del núcleo 

lo que vamos a ver en esta demostración es que si hacemos la imagen de los ultimos vectores $u_{r+1},...,u_n$,hacemos $f(u_i)$ para $i=r+1,...,n$, este conjunto de vectores es una base de la image, si consegimos ver que esto es una base de la $img(f)$ habremos conseguido demostrar la fórmula de las dimensiones porque la imagen tendra dimensión $n-r$.

Para esto vamos a demostrar primero que son linealmente independientes:

Vamos a tomar una combinación lineal de esos $n-r$ vectores: 
$$\sum_{i=r+1}^{n}\alpha_i f(u_i)=0$$

Como es una combinación lineal y la aplicación $f$ es lineal, podríamos meter $\alpha_i$ dentro el sumatorio y escribirlo como: 

$$f\left(\sum_{i=r+1}^{n}\alpha_i u_i\right)=0$$

Esto significaría que la combinación lineal del Interior 

$$\sum_{i=r+1}^{n}\alpha_i u_i\in\ker(f)$$


Por tanto por ser un elemento del kernel debería ser combinación lineal de la base del núcleo así que el sumatorio debería poder escribirse como:

$$\sum_{i=r+1}^{n}\alpha_i u_i = \sum_{i=1}^{r}\alpha_i u_i$$

Del segundo sumatorio tenemos $\alpha_i u_i$ que son los $r$ primeros vectores que forman
una base del núcleo pero esto si lo escribimos bien nos quedaría:

$$\sum_{i=1}^{r}\alpha_i u_i - \sum_{i=r+1}^{n}\alpha_i u_i=0$$

Tenemos una combinación lineal de toda la base del espacio vectorial $E$ pero como es una base de $E$ estos vectores $u_i,...,u_n$ todos son linealmente independientes y por tanto todos los escalares deberían ser $0$, $\alpha_i=0\forall i=1,...,n$ esto significaría que los $n-r$ que habíamos elegido son ceros para $\alpha_i=0\ i=r+1,...,n$ o lo que es lo mismo que son linealmente independientes porque la combinación lineal que hemos elegido justo al inicio $\sum_{i=r+1}^{n}\alpha_i f(u_i)$ tiene por únicos coeficientes ceros 

Falta ver que generan todo el espacio imagen, para esto si tomamos un vector $y\in img(f)$ si pertenece a la imagen sabemos que $\exists\ x\in E$ tal que $y=f(x)$ es la definición de que sea un elemento de la imagen y este $x$ será una combinación lineal de la base de $E$ entonses sera este sumatorio por toda la base que es $u_i$:

$$\sum_{i=1}^{n}\alpha_i u_i $$
Entonces $y=f(x)$ pero esto sería $f$ de toda esta combinación lineal: 

$$ f\left(\sum_{i=1}^{n}\alpha_i u_i\right)$$

y ahora esto lo podemos escribir como la $f$ es lineal sacamos el sumatorio y los coeficientes fuera de la función $f$:

$$\sum_{i=1}^{n}\alpha_i f(u_i)$$

Pero para que los primeros $r$ vectores desde el $1$ hasta el $r$ son vectores del núcleo y por eso $f(u_i)=0$ asi de todo este sumatorio sólo nos quedaría: 

$$\sum_{i=r+1}^{n}\alpha_i f(u_i)$$
Con esto hemos tomado un vector cualquiera de la imagen $y\in img(f)$ y lo hemos expresado como esta combinación lineal de arriba asi con la notación que hemos considerado $f$ desde $u_{r+1}$ hasta $f$ de $u_n$ forman una base de la imagen, la dimensión de la imagen es $n-r$ y por tanto si sumamos dimensión de $E$ es la suma de la dimensión del kernel más la dimensión de la imagen entonces primera proposición totalmente demostrada 

**Segunda demostracion** 

Si $E$ es isomorfo a $F$ si y sólo si en dimensiones coinciden $dim(E)=dim(F)$ eso es el teorema, aquí en principio podemos demostrarlo como aplicación directa del teorema del rango porque como en el caso de que son isomorfos existe una aplicación lineal $f:E\longrightarrow F$ un isomorfismo que es biyectivo, al ser biyectivo verifica que $\ker(f)=\{0\}$ por tanto aplicando el teorema del rango que acabamos de demostrar, la dimensión del espacio vectorial $E$ sería la del núcleo $0$ más la dimensión de la imagen: $dim(E)=0+dim(img(f))$ pero al ser biyectiva, la imagen es todo el espacio de llegada $F$: $img(f)=F$ asi nos quedaria $dim(E)=0+dim(img(f))=dim(F)$, la dimensión de imagen $f$ es lo mismo que la dimensión de $F$ porque toda la imagen es todo el espacio vectorial y así habría demostrado al menos la hida que es la parte fácil.

Recíprocamente supongamos que en un espacio vectorial de dimensión finita coincide que $dim(E)=dim(F)$ y consideremos $u_1,u_2,...,u_n$ una base del espacio vectorial de partida $E$ y $v_1,v_2,...,v_n$ una base de $F$, entonces si tomamos $x\in E$ que se escribirá de forma única como una combinación lineal: $\sum_{i=1}^{n}\alpha_i u_i$ y en este caso podríamos definir la aplicación lineal $f:E\longrightarrow F$ que envía a cada elemento $x$ a $f(x)$ definido como $f$ de la combinación lineal $f\left(\sum_{i=1}^{n}\alpha_i u_i\right)$ que vendría dada por otro sumatorio com los mismos coeficientes $\alpha_i$ pero en este caso los multiplicamos por $v_i$ porque es la base de $F$:

$$
\begin{matrix}f:&E\longrightarrow F\\
&x\mapsto f(x)=f\left(\sum_{i=1}^{n}\alpha_i u_i\right)=\left(\sum_{i=1}^{n}\alpha_i v_i\right)
\end{matrix}
$$

entonces la $f$ podriamos demostrar que es lineal porque si tomamos $f(x+y)$ donde $x$ y $y$ son dos combinaciones lineales cualesquiera serían $f\left(\sum_{i=1}^{n}\alpha_i u_i +\sum_{i=1}^{n}\beta_i v_i\right)$ esto se podría reescribir sacado los vectores factor común: $f\left(\sum_{i=1}^{n}(\alpha_i + \beta_i) u_i\right)$ por la aplicación directa de cómo se ha definido la $f$, sacaríamos los coeficientes delante de los vectores $v_i$ de la base $\sum_{i=1}^{n}(\alpha_i + \beta_i) v_i$ pero como la suma la podemos separar aplicando la distributiva nos quedaría $\sum_{i=1}^{n}\alpha_iv_i + \sum_{i=1}^{n}\beta_iv_i$, en efecto esto es por la forma que tiene el vector $x,y$ seria $f(x)+f(y)$ sería lineal para la suma y análogos lo dejamos para el producto por un escalar

Si es exhaustiva lo único que tendríamos que ver es que cualquier elemento del espacio de llegada $y\in F$ como los $v_i$ son una base, pues se podrá escribir como cierta combinación lineal $y=\sum_{i=1}^{n}\alpha_i v_i$ pero si en este caso tomamos un vector $x$ que tenga las mismas coordenadas pero con la base $u_i$: $x=\sum_{i=1}^{n}\alpha_i u_i$ esto será un vector de $E$ que trivialmente por la forma en la que se ah definido $f$, $f(x)=y$ así que cualquier elemento le podremos localizar una anti-imagen.

Si es inyectiva, para la $x$ que es un elemento cualquiera del espacio de partida: $x=\sum_{i=1}^{n}\alpha_i u_i$ y es un elemento que pertenece al núcleo $\in\ker(f)$ o lo que es lo mismo es un elemento tal que $f(x)=0$, Esto significaría que como $f(x)$ es la misma combinación lineal pero multiplicada por $v_i$ esta combinación lineal sería cero: $x=\sum_{i=1}^{n}\alpha_i v_i=0$, si es $0$ recuerdando que los $v_i$ son una base del espacio vectorial y por eso son linealmente independientes, por tanto la única forma que una combinación lineal sea igual a cero es que todos los $\alpha_i=0\forall i=1,...,n$ pero si todos son ceros el vector $x$ que nos habíamos inventado es exactamente $x=0$ no tiene ninguna componente y por tanto $\ker(f)=\{0\}$ o lo que es lo mismo $f$ es inyectiva.

De este modo se verifica que la aplicación $f$ que hemos definido es claramente un isomorfismo o lo que es lo mismo $E$ y $F$ son dos espacios vectoriales isomorfos.

**Otra propocicion**

Si $f:E\longrightarrow F$ entonces $f$ es isomorfismo si y sólo si $dim(E)=dim(F)$ y $\ker(f)=0$ pero esta no la vamos a demostrar porque ya fue una de las demostraciones que hicimos anteriormente donde utilizábamos que el núcleo fuera cero para incorporar la fórmula del teorema del rango y así cancelar esa parte.

**Ejercicio 9: **

$f:E\longrightarrow F$ y teníamos vectores del espacio vectorial tales que nos permitían decir que: 

- **1°: ** $f$ es un monomorfismo y $u_1,u_2,...,u_n$ son vectores linealmente independientes $E$ entonces resulta que sus imágenes $f(u_1),...,f(u_n)$ son vectores linealmente independientes de $F$.

Lo único que tenemos que hacer para demostrar que son linealmente independientes pues hay que tomar una combinación lineal de ellos vale igualarla a $0$ y ver que todos los coeficientes son cero: $\sum_{i=0}^{n}\alpha_i f(u_i)=0$, pero como $f$ es lineal nos quedaría que $f(\sum_{i=0}^{n}\alpha_i u_i)=0$ y además por ser inyectiva el núcleo es el trivial asi que el único vector que pertenece al núcleo en un monomorfismos es el $0$, si $f$ de algo es cero, ese algo que tenemos aquí es el vector cero: $\sum_{i=0}^{n}\alpha_i u_i=0$, llegados a este punto los $u_i$ son linealmente independientes asi tenemos una combinación lineal de una serie de vectores linealmente independientes igual a cero, resulta que todos los $\alpha_i=0$,$i=1,..,n$.

- **2°: ** Si $f$ era un Epimorfismo por tanto es exhaustiva y tomábamos en este caso vectores $\langle u_1,u_2,...,u_n\rangle$ que fueran un sistema generador de $E$ es decir que ese conjunto generar al espacio vectorial $E$, entonces $\langle f(u_1),f(u_2),...,f(u_n)\rangle$ era en efecto un conjunto generador de $F$.

Aqui hay que ver que generan todo el espacio, tomemos un vector $y\in F$, por ser exhaustiva $\exists\ x\in E$ e tal que $f(x)=y$, pero como los $u_i$ son un sistema generador del espacio vectorial $E$, $x$ será una combinación lineal: $\sum_{i=0}^{n}\alpha_i u_i$ y por tanto juntando lo todo $y=f(x)=f(\sum_{i=0}^{n}\alpha_i u_i)$ como $f$ es lineal, la combinación lineal sale fuera nos queda: $y=\sum_{i=0}^{n}\alpha_i f(u_i)$ asi cualquier vector $y$ del espacio vectorial de llegada se puede escribir como combinación lineal de los $f(u_i)$ por tanto estos generan todo el espacio vectorial de llegada $F$.

**Ultima Propocicion**

Decía que si $f:E\longrightarrow F$ donde $E$y$F$ son espacios vectoriales de dimensión finita y $G$ es un subespacio vectorial de $E$ entonces resulta que la dimensión de ese subespacio se puede describir como $dim(G)=dim(G\cap\ker(f))+dim(f(G))$, es decir es la parte de $G$ que cae dentro del núcleo más esa dimencion, sería como el teorema del rango adaptado a subespacioa

¿Qué pasa si tenemos un subespacio vectorial dentro del original?,¿Qué relación guarda el teorema del rango con este subespacio vectorial?.

Si $f$ es la aplicación lineal, nos basta con considerar $f$ restringida a $G$ es una aplicación lineal definida desde $G$ como espacio de partida y hasta $F$ como espacio de llegada, es restringir $F$ para que sólo se calcule sobre los elementos que pertenecen al subespacio vectorial $G$:

$$f_{|G}:G\longrightarrow F$$

Claramente esta restricción sigue siendo lineal y el núcleo de esta aplicación lineal, $\ker(f_{|G})=G\cap \ker(f)$ del mismo modo, la imagen de la restricción: $img(f_{|G})=f(G)$, con estos dos puntos, solo tenemos que aplicar el teorema del rango a esta restricción (a esta aplicación lineal) para tener demostrado el resultado. 

Asi tenemos las demostraciones utilizando el teorema del rango como punto de partida.

## Primer Teorema de Isomorfía

Lo primero que se extra del teorema del rango, este nos da un resultado muy impactante y tiene mucha utilidad

**Primer Teorema de Isomorfía.** Sea ($f$ una aplicacion lineal definida entre dos espacios vectoriales, $E$ de partida y $F$ de llegada) $f:E\longrightarrow F$ una aplicación lineal. Entonces: 

(El espacio vectorial $E$ cociente por el nucleo de $f$ es isomorofo a la imagen de $f$)

$$E/\ker(f)\cong\text{Im}(f)$$

Es decir que si tomamos el espacio vectorial $E/\ker(f)$ esto es isomorof a $img(f)$

Más concretamente, la aplicación que a cada clase de la relacion de equivalencia módulo $\ker(f)$ le hace corresponder la imagen por $f$ de cualquiera de sus representantes, (es decir enviamos la clase $x$ a $f(x)$) $[x]\mapsto f(x)$, es un isomorfismo de espacios vectoriales.

Recordemos que el conjunto del espacio vecotorial cociente de $E$ por la relacion de pertenencia al $\ker(f)$ lo que hacia era agrupar, juntar, compactar elementos del espacio vecotrial $E$ tal que su diferencia perteneciera al nucleo, recordando que dos elementos se relacionan o pertenecen a la misma clase del conjunto cociente si y solo si la diferencia de ellos pertenece al nucleo esta seria la definicion del **espacio cociente** $E/\ker(f)$

Aqui lo que afirmamos es que la aplicacion que a cada clase de equivalencia modulo $f$, que a todas las clases le hace corresponder la imagen de $f$ de cualquiera de los representantes de la misma, enviamos una clase de $x$ por esta relacion de equivalencia a $f(x)$ independientemente del representante es un ismorofismos entre espacios vectoriales

**Ejercicio 11**
Demostrar formalmente este Teorema.

**Observación.** El `Teorema` anterior es cierto para espacios vectoriales de cualquier dimensión, no necesariamente finita. 

Nótese que el caso de dimensión finita es fácil de demostrar ya que la isomorfía viene dada porque tienen la misma dimensión: (Recordar lo siguente: La dimencion del cociente de $E$ por el nucleo de $f$ era la dimencion del espacio $E$ menos la dimencion del nucleo de $f$ (Esta formula ya la habiamos visto con los espacios vectoriales) y esto es aplicacion directa del teorema del rango donde en este caso se despejo la dimencion de la imagen de $f$ asi que es trivial de demostrar para espacios de dimencion finita y para espacios de dimencion infinita es mas complicado)

$$\dim(E/\ker(f)) = \dim(E)-\dim(\ker(f)) = \dim(\text{Im}(f))$$

## Ejercicio 11

Vamos a demostrar el primer teorema de isomorfia que nos decía que $f:E\longrightarrow F$ entonces $E/\ker(f)\cong \text{Im}(f)$, la única restricción es que $f$ sea lineal, más concretamente la aplicación que a cada clase módulo núcleo de $F$ le hace corresponder la $\text{Im}(f)$ de cualquiera de sus representantes es un isomorfismo entre espacios vectoriales, es decir $\Phi$ definida entre $E$ módulo núcleo de $F$ y la $\text{Im}(f)$ que a cada clase de equivalencia $[x]$ la enviamos a $f(x)$ a $F$ de alguno de sus representantes:

$$
\begin{matrix}\phi:&E/\ker(f)\longrightarrow \text{Im}(f)\\
&[x]\mapsto f(x)
\end{matrix}
$$

Esta cosa que tenemos aquí es un isomorfismo, lineal, inyectiva, exhaustiva y además está bien definida porque no depende de los representantes 

Vamos a tener que demostrar que esta bien definida, que es lineal, que es inyectiva y que es exhaustiva, cuatro cosas para ver que está $\phi$ es un isomorfismo de espacios vectoriales.

Si **está bien definida** (que no depende del representante elegido):

En nuestro caso lo que queremos ver es que si yo tenemos clase de $x$ y clase de $y$, $[x]=[y]$ dos clases iguales donde $x$ e $y$ son representantes lo que queremos ver es que $\phi([x])=\phi([y])$ recordar que en una clase de equivalencia puede haber todos los elementos que queramos, podemos escoger a cualquiera como representante, la idea es que, si dos elementos pertenecen a la misma clase sus imágenes tienen que ser iguales para que la aplicación esté bien definida.

Si $[x]=[y]$ entonces $x$,$y$ se relacionan a través de la relación núcleo de $F$, $x\sim \ker(y)$ que esto es equivalente a decir $x-y\in\ker(f)$ pero que pertenezcan al núcleo de $f$ significa que $f(x-y)=0$ porque con elementos es del núcleo si da cero, y como $f$ es lineal significaría que $f(x)-f(y)=0$ y esto significaría que evidentemente $f(x)=f(y)$ y por tanto de aquí sacaríamos sin ningún problema que $\phi$ que como la hemos definido es $\phi[x]$ pero $f(x)=f(y)$ es igual a $\phi[y]$ por tanto $\phi$ no depende del representante de la clase elegido y está bien definida.

El hecho de que **sea lineal** es trivial simplemente si tomamos $\phi$ de una suma de clases $\phi([x]+[y])$ evidentemente la suma de clases es lineal entonces esto sería $\phi([x+y])$ pero por definición de $\phi$ seria $f(x+y)$ pero ahora es $f$ quien es lineal entonces esto sería $f(x)+f(y)$ y por definición de $\phi$ esto es $\phi([x])+\phi([y])$ por tanto $\phi$ es lineal para las clases.

En tercer lugar hay que demostrar que **es inyectiva** es decir que el núcleo es cero o lo
que es lo mismo, que si tomamos un elemento $[x]\in E/\ker(f)$ entonces $\phi([x])=0$ para demostrar esto, si $\phi([x])=f(x)$ y $f(x)=0$ esto es lo que significa es $x \in\ker(f)$ esto lo que quiere decir $[x]=[0]$, seria $[0]$ la clase del representante $0$ que es la clase del núcleo entonces la única forma en que la $\phi([x])=0$ es que la $[x]$ sea la del núcleo, así que es inyectiva puesto que el único elemento que pertenece al núcleo es la clase del elemento neutro $[0]$.

Por último lado que **sea exhaustiva**, para ser exhaustiva si todo elemento $y\in\text{Im}(y)$ es de la forma $y=f(x)$ para algún $x\in E$ sabiendo esto para la $\phi$ resulta que la $[x]$ es una anti-imagen de $y$, esto significa que $\phi([x])=y\in\text{Im}(f)$ y como se ha definido así, obtenemos que la aplicación es exhaustiva y juntando los 4 resultados son isomorfos el espacio vectorial $E$ cociente por el núcleo de $f$ aplicación lineal y la imagen de $f$.

## Ejemplo de aplicacion del primer teorema de isomorfia

Vamos a considerar la siguente aplicación lineal 

$$
\begin{matrix}f:&\mathbb{R}^2\longrightarrow\mathbb{R}\\
&(x,y)\mapsto f(x,y)=x-y
\end{matrix}
$$
Como dos ejercicios se puede **demostrar que es lineal** y que **el núcleo (Solo hay que igualar a cero)** de esta aplicación lineal es el espacio vectorial formado por los puntos $\ker(f)=\{(x,y)|x-y|\}$ y esto es lo mismo que el espacio vectorial generado $\langle (1,1)\rangle$ bajo este contexto el conjunto cociente de $\mathbb{R}^2/\ker(f)$ estará formado por todas las rectas que sean paralelas al vector $v=(1,1)$ es decir todas las rectas paralelas a la diagonal principal que es la primera diagonal de $\mathbb{R}^2$, el vector $(1,1)$ sería en un plano carteciano el que sale del centro apuntando en diagonal entonces se le dibuuja una recta en la misma direccion a lo largo del plano cuyo vector director es exactamente el $(1,1)$, básicamente las diferentes clases de equivalencia serían todas aquellas rectas paralelas a éste $(1,1)$ todas estas rectas tendrian vector director $(1,1)$ esto sería todas las clases de equivalencia o todo el conjunto cociente $\mathbb{R}^2/\ker(f)$

<div class = "center">
![<l class = "phototext">Clases de Equivalencias</l>](Images/ClasesEquivalencias.jpg)
</div>

Por otro lado $f$ trivialmente es exhaustiva, como se tiene que cumplir el teorema del rango $dim(\mathbb{R}^2)=dim(\ker(f))+dim(\text{Im}(f))$, fijaros que $dim(\mathbb{R}^2)=2$, el $dim(\ker(f))=1$ porque básicamente está generado por un único vector, esto nos lleva a que la imagen tiene que tener $dim(\text{Im}(f))=1$ pero la imagen de $f$ es un subespacio vectorial del espacio de llegada $\mathbb{R}$ pero $\mathbb{R}$ ya tiene dimensión $1$ entonces un espacio vectorial de dimensión $1$ dentro de un espacio de dimensión $1$ es todo el espacio, así que simplemente $\text{Im}(f)=\mathbb{R}$ esto significaría que $f$ es exhaustiva, no es inyectiva porque el núcleo no tiene dimensión 0, aplicando entonces el isomorfismo que se ah demostrado en el primer teorema de isomorfia que en este caso seria:

$$
\begin{matrix}\phi:&\mathbb{R}^2/\ker(f)\longrightarrow\text{Im}(f)\\
&[x]\mapsto f(x)
\end{matrix}
$$

Esto sería un isomorfismo y los elementos de $\mathbb{R}^2/\ker(f)$ poniendolo correctamente las clases no tienen un elemento sino que tienen dos coordenadas sería la $[(x,y)]$ y lo enviáramos a $f(x,y)=x-y$ y tal cual lo hemos colocado sería igual a $f([(x-y,0)])$ 

$$
\begin{matrix}\phi:&\mathbb{R}^2/\ker(f)\longrightarrow\text{Im}(f)\\
&[(x,y)]\mapsto f(x,y)=x-y=f([(x-y,0)])
\end{matrix}
$$

simplemente calcular que $f$ de esta clase $f([(x-y,0)])$ es exactamente a $f$ de $f(x,y)$. fijaros que a cada clase de equivalencia $[(x,y)]$ posibles, corresponden a una recta paralela al $(1,1,)$ una recta de la forma $y=x+k$, estas son las rectas paralelas a la recta que se ah pintado anteriormente y que básicamente cortaría al eje de las $X$ en un punto concreto.

Por ejemplo, si esto fuera la recta:

<div class = "center">
![<l class = "phototext">Clases de Equivalencias</l>](Images/rectaCortaEje.jpg)
</div>

Así que el isomorfismo al final lo que hace es corresponder a cada una de las rectas de $\mathbb{R}^2/\ker(f)$ su intersección con el eje de las $X$, simplemente de cada una de las rectas que serían las clases de equivalencia $[(x,y)]$ se comprime todo y se le asigna ese punto que interseccta el eje de las $X$ como es ve arriba,

A partir de ahí si tuviéramos otra recta, la clase de equivalencia de esta recta sería la recta en sí misma pero $\phi$ de esa clase de equivalencia quedaría
enviada de nuevo al punto de corte con el eje de las $X$: 

<div class = "center">
![<l class = "phototext">Clases de Equivalencias</l>](Images/rectaEjemplo2.jpg)
</div>

Así que simplemente es una forma de identificar (de asociar, de relacionar) de forma biunívoca (de forma exactamente única) cada recta del conjunto cociente $\mathbb{R}^2/\ker(f)$ con todos y cada uno de los valores de la recta real.

## Descomposición canónica de una aplicación lineal

**Proposición.** Sea $f:E\longrightarrow F$ una aplicación lineal. Entonces $f$ se puede poner como composición de tres aplicaciones lineales: una exhaustiva, una biyectiva y una inyectiva según el siguiente diagrama:

Aqui tenemos la aplicacion lineal $f$ que nos lleva del espacio vectorial $E$ al espacio vectorial $F$, por un lado de cualquier elemento que tengamos del espacio vectorial $E$ podemos utilizar la funcion $\pi$ que nos manda a su clase de equivalencia $E/\ker(f)$, es decir dado cualquier elemento $x$ del espacio vecotrial $E$ podemos calcular su clase utilizando la relacion de equivalencia $E/\ker(f)$ entonces esta aplicacion proyecta cualqueir vector del espacio vectorial $E$ directamnte lo proyecta al espacio vectorial $E/\ker(f)$ esta aplicacion es exautiva porque cualquier clase de equivalencia siempre tiene almenos un punto del espacio vectorial que acaba en esa clase.

La **segunda** seria la aplicacion $\phi$ que nos lleva de $E/\ker(f)$ a la $\text{Im}(f)$ es una aplicacion biyectiva y es asi por aplicacion directa del primer teorema de isomorfia (existe una biyeccion): $E/\ker(f)\cong\text{Im}(f)$.

Por ultimo la $\text{Im}(f)$ la podemos enviar (la podemos incluir) dentro de $F$ ya que la $\text{Im}(f)$ es un subespacio vectorial de $F$ y la aplicacion lineal $\iota$ que es la aplicaicion inclucion es como si fuera la identidad pero la diferencia es que en lugar de esta definido dentro del mismo espacio vectorial esta definido sobre diferentes espacios e incluye todos los vectores de la $\text{Im}(f)$ dentro de $F$ entonces como es parecida a la aplicacion identidad tiene la particularidad que puntos diferentes de $\text{Im}(f)$ bienen a parar a diferentes puntos de $F$, asi que la inclusion es inyectiva

$$\begin{matrix}
E & \xrightarrow{f} & F\\
\downarrow_{\pi} & & \uparrow_{\iota}\\
E/\ker(f) & \xrightarrow{\varphi} & \text{Im}(f) 
\end{matrix}$$

El recorrido de la explicacion pasa por debajo, primero calcular la proyeccion $E/\ker(f)$ luego aplicar el isomorfismo del primer teorema de isomorfia $\text{Im}(f)$ y depues incluir ese elemento de la imagen en el espacio vectorial $F$, Es el mimso reccorrido que hubieramos hecho para aplicar la aplicacion lineal $F$ directamente 

donde $\pi$ denota la proyección, $\iota$ la inclusión y $\varphi$ el isomorfismo dado por el `Primer Teorema de Isomorfía`.

En definitiva podemos decir que $f$ es directamente la compocicion de la proyeccion, el isomorfimo del teorema de isomorfia y la inclucion en este orden

$$f = \iota\circ\varphi\circ\pi$$

Toda aplicacion lineal siempre descompone en compocicion de tres aplicaciones lineales una primera exhaustiva, una segunda biyectiva y una tercera inyecctiva en este orden, siguiendo este diagrama

**Ejercicio 12**
Demostrar formalmente esta Proposición.

## Segundo Teorma de Isomorfía

Del `Primer Teorema de Isomorfía` podemos deducir un segundo:

**Segundo Teorema de Isomorfía.** Sea $E$ un $\mathbb{K}$-espacio vectorial y $F,G$ subespacios vectoriales de $E$. Entonces se cumple que:

El conjunto cociente del subespacio, suma de $F+G$ modulo/cociente $F$ es isomorfo al espacio vectorial $G$ modulo/cociente interseccion $F$ con $G$

$$(F+G)/F\cong G/(F\cap G)$$

Recuperando el primer enunciado de isomorfia y lo aplicamos directamente a la relacion de pertenencia $F$ es decir como si $F$ fuera el nucleo de una aplicacion lineal se puede obtener este resultado que tenemos aqui

**Ejercicio 13**
Demostrar formalmente este Teorma.

## Ejercicio 12 y 13

Vamos a demostrar dos consecuencias directas del primer teorema de isomorfia.

La primera es la descomposición canónica de una aplicación lineal, nos dice que si tenemos entre $E\longrightarrow F$ una aplicación lineal $f$ cualquiera entonces $F$ se puede descomponer en composición de tres aplicaciones lineales, una que es exhaustiva, una que es biyectiva (un isomorfismo) y una que es inyectiva de modo que, la exhaustiva es la primera es la que nos lleva de $E$  a $E/\ker(f)$ lo que típicamente llamamos la proyección $\pi$ esta es exhaustiva, el isomorfismo es $\phi$ precisamente el que nos da el primer teorema de isomorfia que define un isomorfismo entre $E/\ker(f)$ y la $\text{Im}(f)$ (Del primer teorema de isomorfismo) Y finalmente es la inclusión que manda a cada elemento de la $\text{Im}(f)$ a el conjunto global $F$ ya que la imagen de $f$ es un subespacio de $F$.

Entonces la proyección $\pi$ es exhaustiva, $\phi$ es un isomorfismo (es Biyectiva) y $\iota$ la inclusión es inyectiva.

$$\begin{matrix}
E & \xrightarrow{f} & F\\
\downarrow_{\pi} & & \uparrow_{\iota}\\
E/\ker(f) & \xrightarrow{\varphi} & \text{Im}(f) 
\end{matrix}$$

Resulta que $f$ es composicion de aplicar $f=\iota\cdot\phi\cdot\pi$

Vamos a demostrarlo, Precisamente tenemos que para cualquier vector $x\in E$, podemos aplicar $\iota\cdot\phi\cdot\pi(x)$ fijese que la proyección $\pi$ es la que envía cada elemento de $E$ a su clase de equivalencia entonces esto sería $\iota\cdot\phi\cdot\pi([x])$ aplicado a la clase de $[x]$ porque la aplicación $\pi$ proyecta el espacio vectorial $E$ sobre el conjunto cociente.

En segundo lugar la $\phi$ definida en el primer teorema de isomorfia envía $\phi$ de la $[x]$ a $f(x)$ tal cual $\iota(f(x))$ Y la propia inclusión lo único que hace es dejar el elemento igual porque la inclusión es $f(x)$ igual al propio $x$ sólo que lo enviamos a un espacio más grandes así que nos queda: $\iota(f(x))=f(x)$

Esta composición de estas tres funciones directamente nos da $f(x)$ Y del primer teorema de isomorfia se deduce fácilmente que esto es un isomorfismo. El hecho de que sea exhaustiva (la proyección $\pi$) a cada elemento $[x]\in E/\ker(f)$ es tal que la proyección $\pi$ aplicado a la $x$ nos da $\pi(x)=[x]$ así que cualquier clase sobre el conjunto $E/\ker(f)$ procedería de la proyección del propio elemento de la clase $[x]$ y además estaría bien definida porque si una misma clase tiene dos representantes, la propia proyección los mandaría al mismo sitio, y es exhaustiva.

El hecho de que la inclusión es inyectiva, lo único que tenemos que demostrar es que dos elementos diferentes van a parar a imágenes diferentes pero el $\text{Im}(f)$ un subespacio vectorial de $F$ y la propia aplicación inclusión está definida como $\iota(x)=x$ entonces esta aplicación es inyectiva.

Falta el segundo teorema de isomorfia que nos dice que si $F,G$ son subespacios vectoriales de otro mas grande que es $E$ sobre un mismo cuerpo $\mathbb{K}$ entonces se cumple el isomorfismo de que $(F+G)/F\cong G/(F\cap G)$ para la demostración lo único que hay que hacer es definir una aplicación $f$ que por la forma que tiene aquí el segundo teorema de isomorfia se ve que puede definirse entre $G$ como espacio de partida y en este caso $(F+G)/F$ como espacio de llegada Y viene definida por los elementos $y$ que pertenecen a $G$ los mandamos a la clase de equivalencia $[y]$ y esta nos la pensamos como si fuera la clase de $[0+y]$ recuerda que cero pertenece a todos los espacios vectoriales y esta clase $[0+y]$ pertenece a $(F+G)/F$ para cualquiera que sea el $y$ del espacio vectorial $G$ 

$$

$$

Fijaros que $[0+y]$ siempre es un elemento de la suma de $(F+G)$ y al hacer cociente módulo $G$ queda correctamente definida, está claro que $f$ es lineal, la suma de clases la clase de la suma y la aplicación que acabamos de definir aquí es lineal

Para ver si es exhaustiva, si tomamos $x\in F$ y $y\in G$ está claro que $y$ es una anti-imagen de la clase de la suma es decir $f(y)$ es claramente una anti-imagen del conjunto $[x+y]$ cualquiera que sea $x$ de $F$ ya que la clase de $[x+y]$ por linealidad de las clases sería $[x]+[y]$ pero claro, la clase estamos haciendo módulo $F$ entonces como $x$ ya es un elemento de $F$ entonce $[x]=[0]$ y la clase de $[y]$ no sabriamos quién sería porque $y$ no es de $F$ aunque en general sea de $G$, entonces esto de aquí lo podríamos escribir como $[0+y]$ y esto como la hemos definido es $f(y)$ asi que cualquiera que sea el elemento $x$ de $F$ y la $y$ de $G$ tenemos una anti-imagen para $[x+y]$
por tanto es exhaustiva. Además para calcular el núcleo y ver el $\ker(f)$ estaría formado por los elementos de espacio de partida $\ker(f)=\{y\in G\}$ tal que la $f(y)$ fuera cero pero es que $f(y)=[0+y]$ que evidentemente es la clase de $[y]$ porque $0+y=y$ y esto sería cero, sería el conjunto de elementos que irían a parar a la clase del 0. $\ker(f)=\{y\in G|f(y)=[0+y]=[y]=[0]\}$, pero si pertenecen a la clase del cero eso querría decir que son de $F$ porque los elementos que van a parar al cero en un cociente son los que pertenecen a la relación por tanto esto serían los elementos $y\in G|y\in F$ pero si $y$ es de $F$ y $G$ el núcleo es la intersección de $F\cap G$, esto simplemente querría decir que $f(y)$ pertenezca a la clase del 0 significa que $y\in F$ de ahí se saca el resultado.

El primer teorema de isomorfia aplicado aquí nos diría, $G$ cociente por el núcleo que el núcleo acabámos de demostrar que es $F$ intersección $G$, $G/F\cap G$ sería isomorfo a la imagen que en este caso como la aplicación es exhaustiva toda la imagen sería $F+G/F$ y de aquí $G/F\cap G\cong (F+G)/F$ tendriamos demostrado el segundo teorema de isomorfia.

# Matriz asociada a una aplicación lineal

Existe una relación entre las aplicaciones lineales y las matricesy se llama rango de una aplicación lineal mientras que la palabra rango se había introducido por primera vez en el mundo de las matrices esto es porque va a existir lo que llamaremos la matriz asociada a una aplicación lineal, será como resumir una aplicación lineal en una matriz, en lugar de hablar de aplicaciones lineales podremos transformarla en una matriz y utilizar la matriz indistintamente del mismo modo a partir de una matriz podremos fabricar una aplicación lineal, será comoestablecer un isomorfismo por así decir entre aplicaciones lineales y matrices.

Antes de entrar en definiciones formales, se deducirá la forma de estas matrices con un ejemplo, veremos un ejemplo de deducir por qué tiene sentido hablar de matriz de una aplicación lineal o cómo a partir de una aplicación lineal se puede sacar una matriz.

**Ejemplo 2**

Sea $f:\mathbb{R}^2\longrightarrow \mathbb{R}^3$ la aplicación lineal dada por estas tres componentes

$$f(x,y) = (x+y,y-2x,x+y)$$

Nos piden lo siguiente:

1. Obtener las imágenes de los vectores de la base canónica $B_C$ de $\mathbb{R}^2$ (simplemente esta base canonica es como otra base cualquiera, podemos calcularle la imagen  a estos dos vectores de la base canonica)
2. Obtener las imágenes de los vectores de la base (Es una base cualquiera del espacio de partida) $B_E = \{(1,-1),(2,1)\}$ de $\mathbb{R}^2$, (estos vectores $(1,-1),(2,1)$ conforman una base de $\mathbb{R}^2$ podemos demostrara que generan $\mathbb{R}^2$ y que son linealmente independientes)
3. Obtener las imágenes de los vectores de la base canónica de $\mathbb{R}^2$ pero una vez que los hemos obtenidos los queremos expresados en la base de (Es una base cualqueira del espacio de lleagada) $\mathbb{R}^3$ que es $$B_F = \{(1,-1,0),(1,0,-1),(1,1,1)\}$$ (Que podriamos demostrar que se tratan de una base de de $\mathbb{R}^3$, son tres vectores linealmente independientes en una espacio de dimencion 3 por tanto serian una base de todo $\mathbb{R}^3$)
4. Obtener las imágenes de los vectores de la base $B_E$ (es una base cualquiera del espacio de partida) de $\mathbb{R}^2$ expresados en la base $B_F$ (es una base cualqueira del espacio de llegada) de $\mathbb{R}^3$

(Es incremento a nivel de dificultad cada vez que nos alejemos de la base canonica)

**Solución 1**

Como la base canónica de $\mathbb{R}^2$ es $B_C = \{(1,0),(0,1)\}$, entonces, calcular las imagenes de esos vectores:

$f(1,0)$ es meter un $1$ donde hay una $x$, un $0$ donde hay una $y$ y la $f(0,1)$ de forma analoga se sustiyue la $x$ por $0$, la $y$ por $1$ en la aplicacion lineal (la definida anteriromente) y veremos que este vector $(1,1,1)$

$$f(1,0) = (1+0,0-2\cdot 1, 1+0) = (1,-2,1);\qquad f(0,1) = (1,1,1)$$

Si se colocan las coordenadas de estos dos vectores que acabamos de calcular $f(1,0)$ y $f(0,1)$ como columnas de una matriz, lo que se obtiene es 

$$\begin{pmatrix}
1 & 1\\
-2 & 1\\
1 & 1
\end{pmatrix}$$

simplemente es las imagenes de los vectores de la base canonica en el orden que toca y en columna

Con lo cual, a nivel matricial podriamos escribir que el vector $f(1,0)$ (primer vector de la base canonica) y $f(0,1)$ (segundo vector de la base canonica), se podria expresar como el producto matrcial de los tres vectores de la base canonica de $\mathbb{R}^3$ que son $(1,0,0),(0,1,0),(0,0,1)$ multiplicados por la matriz

$$(f(1,0),f(0,1)) = ((1,0,0),(0,1,0),(0,0,1))\begin{pmatrix}
1 & 1\\
-2 & 1\\
1 & 1
\end{pmatrix}$$

Por tanto se calculan las imágenes de los vectores de la base canónica de $\mathbb{R}^2$ y estas imágenes vienen dadas en la base canónica de $\mathbb{R}^3$ y esta relacion matricial es la que nos permite obtener $f$ de los dos vectores de la base canonica en terminos de los vectores de la base canonica de $\mathbb{R}^3$ por multiplicar por esta mattriz

**Solución 2**

Análogamente, (Ahora tenemos una bse diferente de $\mathbb{R}^2$) dada la base $B_E = \{(1,-1),(2,1)\}$ y lo que queremos es obtener las imagenes de estos vectores en la base canonica de $\mathbb{R}^3$, sacamos las imaganes del $(1,-1)$ y $(2,1)$

$$f(1,-1) = (0,-3,0);\qquad f(2,1) = (3,-3,3)$$

De este modo, podemos colocarlos en columna de una matriz, aqui la primera columna es la imagen del vector $(1,-1)$ y la segunda columna es la imagen del vector $(2,1)$

$$\begin{pmatrix}
0 & 3\\
-3 & -3\\
0 & 3
\end{pmatrix}$$

Y entonces, matricialmente podemos escribir que los vectores $f(1,-1),f(2,1)$ se pueden expresar como el producto de matrices de los tres vectores de la base canonica multiplicados en columna por la matriz

$$(f(1,-1),f(2,1)) = ((1,0,0),(0,1,0),(0,0,1))\begin{pmatrix}
0 & 3\\
-3 & -3\\
0 & 3
\end{pmatrix}$$

Si se calculan las imágenes de los vectores de la base $B_E$ de $\mathbb{R}^2$ directamente al aplicar $f$, estas imágenes vienen dadas en la base canónica de $\mathbb{R}^3$ esto es lo que nos esta diciendo la segunda expresion matricial

**Solución 3**

Dada la base canónica de $\mathbb{R}^2$, sabemos que por el apartado uno sabemos que el primer vector de la base canonica es el $(1,-2,1)$ y la imagen del segundo vector es el $(1,1,1)$

$$f(1,0) = (1,-2,1);\qquad f(0,1) = (1,1,1)$$

Para pasar ahora de $B_C$  (base canonia) a la base $B_F$ de $\mathbb{R}^3$ (Espacio de llegada), se ha de hacer un cambio de base que pasara por buscar una matriz $P$ que nos tranforme vectores en la base canonina a vectores en la base $B_F$, en particular buscamos una matriz $P$ que multiplicada por el $(1,-2,1)$ en la canonica nos de un vector $(\alpha,\beta,\gamma)$ en la nueva base $B_F$ y que la matriz multiplicado por el $(1,1,1)$ de la canonica nos lo transforme al vector $(a,b,c)$ de la base $B_F$.

$$B_C \xrightarrow{P} B_F$$
$$(1,-2,1)_C\xrightarrow{P}(\alpha,\beta,\gamma)_{B_F}$$
$$(1,1,1)_C\xrightarrow{P}(a,b,c)_{B_F}$$

Según la definición de matriz de cambio de base $P$, será la matriz las columnas de la cual son las coordenadas de los vectores de la base $B_C$ expresados en la base $B_F$, cuando vimos el tema de cambio de base dijimos que la matriz de cambio de base es la matriz las columnas de la cual son las coordenadas de la base antigua (Base canonica) expresadas en la base nueva la $B_F$ a la que queremos ir

Se tiene justo lo contrario, es decir, tenemos las coordenadas de (La base nueva) $B_F$ en $B_C$ porque asi fua la base que nos han dado. Por lo tanto, se calculará esta matriz de cambio de base $Q$ al reves tal que nos lleva de losvectores en la base $B_f$ a los vectores en la base $B_C$ $B_F\xrightarrow{Q}B_C$ y la matriz $P$ será la inversa de la matriz $Q$

Por tanto $Q$ sera la inversa de $P^{-1}$, aqui tenemos en columna los 3 vectores de la base $B_F$ estos tres vectores $(1,0,0),(0,1,0),(0,0,1)$ si los colocamos en columna nos da la matriz de cambio de base $Q$ que nos transformaria vectores de la base $F$ a la base canonica y por tanto esta es la inversa de $P$

$$Q = P ^{-1} = \begin{pmatrix}
1 & 1 & 1\\
-1 & 0 & 1\\
0 & -1 & 1
\end{pmatrix}$$

Así pues, si invertimos la matriz $Q$ nos da la matriz $P$ y en este caso la matriz $P$ es esto

$$P = \frac{1}{3}\begin{pmatrix}
1 & -2 & 1\\
1 & 1 & -2\\
1 & 1 & 1
\end{pmatrix}$$

Con lo cual, significa que esta matriz de cambio de base $P$ es la que nos va a transformar vectores de la base canonica a la base $B_F$, asi multiplicamos la matriz $P$ por el vector $[1,-2,1]$ que era el primero que queriamos transformar a la base $B_F$ y en este caso nos va a dar el vector $[2,-1,0]$ que es el vector de la imagen del primer vector de la base canonica del espacio de partida expresada en la base $B_F$ del espacio de llegada

$$(1,-2,1)_C\xrightarrow{P}(\alpha,\beta,\gamma)_{B_F}$$ $$P\begin{pmatrix}
1\\
-2\\
1\end{pmatrix}_C = \begin{pmatrix}
\alpha\\
\beta\\
\gamma
\end{pmatrix}_{B_F}$$
$$\frac{1}{3}\begin{pmatrix}
1 & -2 & 1\\
1 & 1 & -2\\
1 & 1 & 1
\end{pmatrix}\begin{pmatrix}
1\\
-2\\
1\end{pmatrix}_C = \frac{1}{3}\begin{pmatrix}
6\\
-3\\
0
\end{pmatrix} = \begin{pmatrix}
2\\
-1\\
0
\end{pmatrix} = \begin{pmatrix}
\alpha\\
\beta\\
\gamma
\end{pmatrix}_{B_F}$$

Con lo cual, el primer vector es el:

$$(\alpha,\beta,\gamma) = (2,-1,0)$$

Por otro lado, la imagen del segundo vector de la base canonica del espacio de partida nos habia dado el vector $(1,1,1)$ de la base canonica del espacio de llegada de nuevo tomamos la matriz $P$ que acabamos de calcular la multiplicamos por ese vector y los que nos da son las coordenadas $(0,0,1)$ de modo que este vector es la imagen del segundo vector de la base canonica de partida expresada en la base $B_F$ del espacio de llegada

$$(1,1,1)_C\xrightarrow{P}(a,b,c)_{B_F}$$ 
$$\frac{1}{3}\begin{pmatrix}
1 & -2 & 1\\
1 & 1 & -2\\
1 & 1 & 1
\end{pmatrix}\begin{pmatrix}
1\\
1\\
1\end{pmatrix}_C = \frac{1}{3}\begin{pmatrix}
0\\
0\\
3
\end{pmatrix} = \begin{pmatrix}
0\\
0\\
1
\end{pmatrix} = \begin{pmatrix}
a\\
b\\
c
\end{pmatrix}_{B_F}$$

Con lo cual, $(a,b,c) = (0,0,1)$

Si ahora se colocan las coordenadas las imagenes de los vectores de la base canonica $f(1,0)$ y $f(0,1)$ como columnas de una matriz, obtenemos

$$\begin{pmatrix}
2 & 0\\
-1 & 0\\
0 & 1
\end{pmatrix}$$

Entonces, la podemos escribir vectorialmente o matricialmente como la imagen de los dos vectores de la base canonica son los vectores $(1,-1,0),(1,0,-1),(1,1,1)$ la nueva base multiplicada por las coordenadas de esos vectores que acabamos de hayar $(2,-1,0),(0,0,1)$

$$(f(1,0),f(0,1)) = ((1,-1,0),(1,0,-1),(1,1,1))\begin{pmatrix}
2 & 0\\
-1 & 0\\
0 & 1
\end{pmatrix}$$

Es decir se calculan las imágenes de los vectores de la base canónica $B_C$ de $\mathbb{R}^2$ y estas imágenes vienen dadas en la base $B_F$ de $\mathbb{R}^3$, cada ves esta matriz va cambiando del mismo modo que lo va haciendo las coordenadas de la base que vamos calculando o xepesando

**Solución 4**

Si se calcula la imagen de los vectores de la base cualquiera $B_E$ para la base $f$ que tampoco es canonica, se va a tener que hacer todo el cambio de base uno por uno, del ejercicio 2 habiamos visto que eran los vectores expresados en la canonica del espacio de llegada

$$f(1,-1) = (0,-3,0)_C\qquad f(2,1) = (3,-3,3)_C$$

Para ahora pasar de $B_C$ del espacio de llegada a $B_F$ del mismo espacio de llegada $\mathbb{R}^3$ ha de hacerse un cambio de base que tiene por matriz de cambio de base la matriz $P$ que habiamos calculado anteriormente entonces para hacer esto hay que multiplicar por delante la matriz $P$, de modo que si tomamos la matriz $P$ y mutliplicamos por el vecctor $(0,-3,0)$ que estaba en canonica ahora expresado en la $B_F$ tendra coordenadas $(2,-1,-1)$ y lo mismo si tomamos el vector $(3,-3,3)$ ontenemos el vector $(4,-2,1)$ en la base $B_F$ de llegada

$$B_C\xrightarrow{P} B_F$$
$$(0,-3,0)_C\xrightarrow{P}(\alpha,\beta,\gamma)_{B_F}$$
$$(3,-3,3)_C\xrightarrow{P}(a,b,c)_{B_F}$$

Si os fijáis, esta matriz de cambio de base la hemos calculado anteriormente. Por lo tanto

$$(\alpha,\beta,\gamma)_{B_F} = (2,-1,-1)$$
$$(a,b,c)_{B_F} = (4,-2,1)$$

Si se colocan las coordenadas de $f(1,-1)$y $f(2,1)$ expresadas en la nueva base $B_F$ como columnas de una matriz se obtiene

$$\begin{pmatrix}
2 & 4\\
-1 & -2\\
-1 & 1
\end{pmatrix}$$

Entonces, esto significa que podemos escribir los nuevos vectores $f(1,-1)$y $f(2,1)$ de la base del espacio vectorial de partida como combinacion lineal o igual a los vectores de la base de llegada (la nueva base $B_F$) $(1,-1,0),(1,0,-1),(1,1,1)$ multiplicados por los coeficientes que habriamos encontrados

$$(f(1,-1),f(2,1)) = ((1,-1,0),(1,0,-1),(1,1,1))\begin{pmatrix}
2 & 4\\
-1 & -2\\
-1 & 1
\end{pmatrix}$$

Es decir se calculan las imágenes de los vectores de la base $B_E$ de $\mathbb{R}^2$ y estas imágenes vienen dadas en la base $B_F$ de $\mathbb{R}^3$

En todos los casos anteriores comparten algo, es que se han calculado las imágenes de los vectores de una base del espacio de partida que nos dieran (En unicio era la $B_C$ y despues $B_E$) y se han expresado en una cierta base del espacio vectorial de destino (al principio la canonica pero al final fue la base $B_F$).

Estas matrices que hemos calculado son las matrices asociadas a la aplicación lineal y cada vez nos da matrices diferentes, en estes caso viendo las matrices asociadas a la aplicacion lineal en los 4 casos tenemos 4 matrices diferentes asociados a la aplicacion lineal, en el proximo tema veremos que cuando nos definan matriz asociada a una aplicacion lineal esa matriz va a depender de la baseque estemos utilizando y de hecho aqui todas las matrices que tenemos son semejantes en el sentido de que una se puede transformar en la otra. Así que una aplicación lineal puede tener muchas matrices asociadas a ella dependiendo de qué base está usando, la canónica de salida, la canónica de llegada, la canónica de salida pero no de llegada o de llegada pero no de salida, la que se nos dé la gana. Tendrá muchísimas matrices asociadas que todas ellas van a ser semejantes y precisamente en la próxima clase empezaremos por definir la matriz asociada a una aplicación lineal y entender cómo se puede hallar correctamente sin necesidad de tener que hacer todo este desmadre que hemos visto anteriormente detalladamente para que entendamos la nececidad de definir correctamente la matriz asociada a una aplicación lineal en una base.

## Matriz asociada a una aplicación lineal

Después de este ejemplo, empecemos con un resultado que nos asegura que toda aplicación lineal $f:E\longrightarrow F$ queda totalmente determinada si conocemos la imagen de los vectores de una base de $E$ y que esta puede estar dada por $n$ vectores cualesquiera, diferentes o no, de $F$, es decir con la imagen de los vectores de la base de $E$ tenemos determinada la aplicacion lineal y la imagen de estos vectores son $n$ vectores de $F$ no necesariamente diferentes

**Proposición.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales, $E$ de dimensión finita, $\{u_1,\dots,u_n\}$ una base de $E$ y $\{v_1,\dots,v_n\}$ $n$ vectores cualesquiera de $F$. Entonces existe una aplicación lineal $f:E\longrightarrow F$ tal que:

$f$ de los vectores de la base de $E$ son los $n$ vectores cualesquiera que nos habiamos imaginado de $f$ 

$$f(u_i) = v_i\ \forall i= 1,\dots,n$$

esto significa que nos podemos inventar $n$ vectores cualesquiera de $F$ y siempre existira una aplicacion lineal que transforma una base de $E$ en esos $n$ vectores 

y que además la aplicación $f$ verifica que:

- $f$ es monomorfismo si, y solo si, $v_1,\dots,v_n$ son LI dentro del espacio vectorial $F$
- $f$ es epimorfismo si, y solo si, $v_1,\dots,v_n$ generan $F$, son un sistema generador del espacio vectorial $F$
- $f$ es isomorfismo si, y solo si, $v_1,\dots,v_n$ son base de $F$

Este resultado nos dice que apartir de una base del espacio de partida, este siempre se puede transoformar en $n$ vectores cualesquiera de $F$ que hayamos pensado y que estas aplicacion podra ser inyectiva si los vectores que hayamos elegido son independientes, exahustiva si son un sistema generador o biyectiva si son una base de $F$

**Ejercicio 14**
Demostrar formalmente esta Proposición.

**Observación.** Este resultado también es válido para $E$ de dimensión infinita.

De este modo, si (si $u_i$ con $i$ perteneciendo a un conjunto de indicees) $\{u_i\ :\ i\in I\}$ es una base de $E$ y $\{v_i\ :\ i\in I\}$ son vectores cualesquiera pertenecientes a $F$, entonces existe una aplicación lineal $f:E\longrightarrow F$ tal que $f(u_i) = v_i$ para todo $i\in I$ (siendo $i$ para el conjunto de indices que hayamos elegido)

**Definición**

Sean $E$ y $F$ $\mathbb{K}$-espacios vectoriales de dimensión finita, $p$ y $q$ respectivamente las dimenciones con $B_E = \{\vec{u}_1,\dots,\vec{u}_p\}$ una base de $E$, $B_F = \{\vec{v}_1,\dots,\vec{v}_q\}$ una base de $F$ y $f: E\longrightarrow F$ una aplicación lineal.

**Matriz de una aplicación lineal.** Se denomina matriz de $f$ (de la aplicacion lineal) respecto de las bases $B_E,B_F$ a aquella que tiene por columnas las coordenadas de los vectores  

$$(f(\vec{u}_1),f(\vec{u}_2),\dots,f(\vec{u}_p))$$ 

expresados en la base $B_F= \{\vec{v}_1,\dots,\vec{v}_q\}$

En particular una matriz de una aplicacion lineal donde el espacio de partida tenga dimencion $p$ y el espacio de llegada tengaa dimencion $q$ sera una matriz $qxp$, tendra $q$ filas (porque cada uno de esos vectores en el espacio de llegada tiene $q$ coordeadas) y $p$ columnas (porque cada columna tiene la imagen de un vector de la base del espacio de partida)

Las imágenes de los vectores de la base $B_E$ en la base $B_F$ vienen dados por 

$f(\vec{u}_1)$ tendra por coordenadas

$$f(\vec{u}_1)\in F\Rightarrow f(\vec{u}_1) = a_{11}\vec{v}_1+a_{21}\vec{v}_2+\cdots+a_{q1}\vec{v}_q$$
$$f(\vec{u}_2)\in F\Rightarrow f(\vec{u}_2) = a_{12}\vec{v}_1+a_{22}\vec{v}_2+\cdots+a_{q2}\vec{v}_q$$
$$\vdots$$
$$f(\vec{u}_p)\in F\Rightarrow f(\vec{u}_p) = a_{1p}\vec{v}_1+a_{2p}\vec{v}_2+\cdots+a_{qp}\vec{v}_q$$

Esta expresión en forma matricial sería una vez que tenemos estos vectores, podemos decir que los vectores hasta $f(\vec{u}_p)$ sepuede obtener como producto matricial de los vectores de la base del espacio de llegada que son hasta $\vec{v}_q$ y las columnas de la matriz formadas como se ve abajo en el que cada columna es uno de los vectores

$$(f(\vec{u}_1),f(\vec{u}_2),\dots,f(\vec{u}_p)) = (\vec{v}_1,\vec{v}_2,\dots,\vec{v}_q)\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1p}\\
a_{21} & a_{22} & \cdots & a_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
a_{q1} & a_{q2} & \cdots & a_{qp} 
\end{pmatrix}$$

Donde la columna $i$ contiene las coordenadas del vector $f(\vec{u}_i)$ expresado en la base $B_F$. 

La matriz $A$ será de orden $q\times p$ con $\dim(E) = p$ y (Numero de filas) $\dim(F) = q$, asi podemos escribir que las imagenes de los vectores de la base del espacio de partida es igual a la base de llegada multiplicado por esta matriz

$$(f(\vec{u}_1),f(\vec{u}_2),\dots,f(\vec{u}_p)) = (\vec{v}_1,\vec{v}_2,\dots,\vec{v}_q)A$$

En caso de querer explicitar cuales son las bases consideradas en cada espacio vectorial, lo denotaremos por un subindice, pondremos el espacio vectorial $E$ en su base $B_E$ al espacio vectorial $F$ en su base $B_F$

$$f:E_{B_E}\longrightarrow F_{B_F}$$

Esto es en el caso que queramos decir explicitamente de que base estamos hablando en cada espacio porque como vimos en la **Definicion** va a depender de que base hayamos elegido en el espacio vectorial de partida y la base elegida en el espacio vectorial de llegada, la matriz asociada a la aplicacion lineal va a depender de las bases elegidas

Nótese que se satisface que el rango de la aplicacion lineal por definicion es la $\dim(\text{Im}(f))$ pero la dimencion de la imagen es la dimencion de una base suya que esta es la imagen de los vectores que hayamos elegidos en particular los de la base canonica o no seria la imagen de la dimencion del espacio vectorial generado por las imagenes de los vectores de una base del espacio vectorial de partida y que asi como esta expresado es el rango de la matriz que tiene por columnas las imagenes de esos vectores del espacio vectorial de partida.

$$\text{rg}(f) = \dim(\text{Im}(f)) = \dim(\langle f(e_1),\dots,f(e_n)\rangle) = \text{rg}(A)$$

asi que el concepto de tango de una aplicacion lineal es el rango de la matriz asociada a la aplicacion lineal por tanto el rango de una aplicacion lineal se puede calcular como el rango de una matriz asociada a la aplicacion lineal que es la matriz contruida a partir de
las imagenes de los vectores de la base de partida en columna y expresados en la base de llegada

## Ejercicio 14

La proposición decía que si $E$ y $F$ $\mathbb{K}-e.v$ sobre el mismo cuerpo, el primero decia que $E$ es de dimensión finita con base $\{u_1,u_2,...u_n\}$ y tomamos $\{v_1,v_2,...v_n\}$ vectores cualesquiera de $F$ no necesariamente base de hecho $F$ no tendría por qué ser de dimensión finita simplemente estos son vectores de $F$. Entonces existe una única aplicación lineal $\exists! f:E\longrightarrow F$ tal que $f(u_i)=v_i,\ i=1,...,n$, Ese es el resultado y además esta aplicación verifica qué es monomorfismo si los $v_i$ son independientes, es epimorfismo si $v_i$ generan $F$ y es isomorfismo si $v_i$ son una base de $F$.

vamos a demostrar **primero** que existe una única aplicación lineal que hace este tipo de transformación.

Para cualquier $x\in E$ como los $u_i$ son una base sabemos que $x$ se puede escribir como $\sum_{i=1}^{n} a_iu_i$ que forman la base del espacio vectorial $E$, como toda aplicación lineal transforma una combinación lineal en la combinación lineal de las imágenes por un resultado que habíamos demostrado anteriormente. La aplicación que estamos buscando, la $f$ esta única que existe deberá verificar que $f(x)=f(\sum a_iu_i)=\sum_{i=1}^{n}a_if(u_i)$ por ser $f$ lineal pero lo que estamos buscando es este sumatorio $\sum_{i=1}^{n}a_if(v_i)$, entonces la única posible va a ser $\sum_{i=1}^{n}a_if(u_i)$ va a ser que $f(u_i)$ va a parar al $v_i$ respectivo, tal cual la hemos definido es lineal y cumple la condición de que solo con la imagen de los vectores de la base queda determinada unívocamente la aplicación lineal, solo con decir donde mandamos cada uno de los vectores de la base del espacio vectorial de partida y así la aplicación lineal queda unívocamente determinada.

Para los otros apartados.

$F$ será monomorfismo (inyectiva) si y sólo si los $v_i$ son linealmente independientes. Lo que ocurre es que los $v_i$ son independientes por definición, si $F$ es monomorfismo es linealmente independiente, esto es totalmente trivial y recíprocamente la vuelta (Si vamos parael otro lado), si tomamos $x=\sum_{i=1}^{n}a_i u_i$ un vector cualquiera del espacio vectorial $E$ que pertenezca al $\ker(f)$ entonces por pertenecer al kernel $0=f(x)$ pero $f(x)=f(\sum_{i=1}^{n}a_i u_i)$ y por la forma en la que hemos definido la única aplicación lineal $f(\sum_{i=1}^{n}a_i u_i)=\sum_{i=1}^{n}a_i v_i$ por hipótesis los $v_i$ son linealmente independientes y como son linealmente independientes esto implica que la combinación lineal debería ser cero, $a_i=0$ por tanto el único vector que pertenece al kernel es el vector $0$, $x$ tendría que ser cero y tenemos demostrado de este modo que la equivalencia entre epimorfismo y vectores linealmente independientes de imagen es inmediata.

la segunda parte decía que $F$ es un epimorfismo si y sólo si los $v_i$ generan todo el espacio de llegada $F$, del mismo modo si $F$ es epimorfismo por una proposición anterior que ya habíamos demostrado los $(v1,v2,...,vn)$ tal cual los hemos construido son un sistema generador de $F$, sólo nos falta demostrar la vuelta si los $v_i$ generan $F$ entonces $f$ es un epimorfismo pero si tomamos un vector $y\in F$, sabemos que existen unos escalares $a_1,a_2,...,a_n$ tales que $y$ se puede escribir como la combinación lineal de estos $\sum_{i=1}^{n} a_i v_i$ porque al ser un sistema generador multiplicando por ciertos escalares obtendríamos ese vector y además por construcción tal cual lo tenemos hasta ahora los $v_i$ son la imagen de los $u_i$, $\sum_{i=1}^{n} a_i f(u_i)$ y al ser $f$ lineal se podría escribir como $$f(\sum_{i=1}^{n} a_i u_i)$$, en cualquier caso dado, un elemento cualquiera $y$ que pertenece al espacio vectorial de llegada hemos encontrado un elemento del espacio de partida $\sum_{i=1}^{n} a_i u_i$ tal que $F$ del elemento de partida al de llegada, por tanto cualquier elemento $y$ del espacio de llegada tiene una o es imagen de un vector de $x$ y esto significa que $f$ es exhaustiva, sin importar quién era ese vector del espacio de llegada.

Si $f$ es un isomorfismo si y sólo si, evidentemente $v_i$ es una base, es trivial porque si $f$ es un isomorfismo, $f$ es un monomorfismo y $f$ es un epimorfismo. Si es un monomorfismo hemos demostrado en el primer apartado si y sólo si los $v_i$ son linealmente independientes, $f$ epimorfismo si y sólo si $v_i$ es un sistema generador (genera $F$) y cumplir las dos cosas: linealmente independientes y generar el espacio es equivalente a decir que los $v_i$ son una base del espacio vectorial de llegada.

Tenémos aquí demostrado que básicamente una aplicación lineal queda determinada única y exclusivamente por las imágenes de los vectores de una base cualquiera del espacio de partida y esto va a ser crucial porque durante este apartado nos va a dar igual definir una aplicación lineal mediante una expresión analítica, que hacerlo mediante la imagen de los vectores de una cierta base, porque el resultado va a ser único ya que existe una única aplicación lineal que envía la imagen de los vectores de la base del espacio de partida a ciertos vectores del espacio de llegada, lo cual nos permitirá decir que será monomorfismos si y sólo si la imagen de los vectores del espacio de llegada son linealmente independientes, será un epimorfismos si la imagen de los vectores del espacio de llegada es un sistema generador y será un isomorfismo en el caso que esos vectores sean una base.

# Ecuación matricial de una aplicación lineal

Sea $f: E\longrightarrow F$ una aplicación lineal, $A$ es la matriz asociada (En el contexto qque hemos visto) a $f$ respecto de las dos bases $B_E$ y $B_F$ de los espacios vectoriales $E$ y $F$ respectivamente.

Se va a hallar una relación entre las coordenadas de un vector en la base $B_E$ de un vector $\vec{x}\in E$ y las coordenadas en la base $B_F$ del vector $f(\vec{x})\in F$ a travez de utilizar la matriz asociada a la aplicacion lineal

**Ejemplo 3**

Del ejemplo anterior, sea $f:\mathbb{R}^2\longrightarrow\mathbb{R}^3$ la aplicación lineal tal que habiamos obtenido su matriz asociada en la base canónica de partida $\mathbb{R}^2$ y en la base canónica de llegada $\mathbb{R}^3$ es

$$\begin{pmatrix}
1 & 1\\
-2 & 1\\
1 & 1
\end{pmatrix}$$

(Ahora no queremos siempre calcular la imagen del vector de la base canonica o de una base, queremos calcular la imagen del vector que nos de la gana), En este caso calcularemos las coordenadas del vector imagen de $\vec{c} = (2,-1)_C\in\mathbb{R}^2$ expresadas en la base canónica del espacio vectorial de llegada en $\mathbb{R}^3$

Nosotros lo que tenemos es que el vector $(2,-1)$ en la canonica es $2$ veces el vector de la base canonica de $\mathbb{R}^2$ mas $-1$ vez su segundo vector o lo que es los mismo se pone se ponen los vectores de la base canonica en fila multiplicado por las coordenadas del vector $(2,-1)$ en columnas si hacemos ese porducto nos da este vector de la primera igualda que tenemos

$$(2,-1)_C = 2(1,0)+(-1)(0,1) = ((1,0),(0,1))\begin{pmatrix}
2\\
-1
\end{pmatrix}$$

Aplicando $f$ en los dos lados de la igualdad, como ambos miembros son iguales y $f$ es una aplicación (un mismo elemento de origen no puede tener dos imágenes diferentes), sus imágenes también serán iguales, solo se pone dentro de $f()$:

$$f((2,-1))_C = f( 2(1,0)+(-1)(0,1)) = 2f(1,0)+(-1)f(0,1)= (f(1,0),f(0,1))\begin{pmatrix}
2\\
-1
\end{pmatrix}$$

Para la segunda igualdad como $f$ es lineal podremos sacar el $2$ y expresado tal cual lo tenemos aqui $2f(1,0)+(-1)f(0,1)$ seria multiplicar a nivel de vector $(f(1,0),f(0,1))$ por el vector columna como se ve arriba, Seria multiplicar las imagenes de los vectores de la base canonica por el vector el cual queremos hayar las coordenadas en columna

Por otro lado, por definición de la matriz asociada a $f$ respecto a dos bases canonicas $B_E$ y $B_F$, se sabe que por lo que habiamos visto en el ejemplo en cuestion,los vectores de la base canonica $f(1,0),f(0,1)$ se podrian escribir como los vectores $(1,0,0),(0,1,0),(0,0,1)$ de la base canonica de $\mathbb{R}^3$, multiplicado por esta matriz que recuerda que eran las imagenes de los vectores de la base canonica de partida expresados en la base caonica de llegada

$$(f(1,0),f(0,1)) = ((1,0,0),(0,1,0),(0,0,1))\begin{pmatrix}
1 & 1\\
-2 & 1\\
1 & 1
\end{pmatrix}$$

Y sustituyendo el resultado de la expresión en la anterior (No de la expresion que esta arriba, si no la que esta anterior a esta) $(f(1,0),f(0,1))$, se tiene que sustituir por toda la exprecion que tenemos arriba y mutliplica ahora por las coordenadas del vector del cual queremos hayar las nuevas coordenadas de su imagen en la base canonica

$$f((2,-1))_C = ((1,0,0),(0,1,0),(0,0,1))\begin{pmatrix}
1 & 1\\
-2 & 1\\
1 & 1
\end{pmatrix}\begin{pmatrix}
2\\
-1
\end{pmatrix}$$

Ahora, si se denota por $Y_C$ las coordenadas del vector $f(2,-1)_C$ en la base canónica de llegada $\mathbb{R}^3$, se puede escribir que las coordenadas van a ser el producto de la matriz por el vector, eliminamos simplemente estos vectores $((1,0,0),(0,1,0),(0,0,1))$ para hacer el producto matricial de las coordenadas de las imagenes de la base canonica de partida en la base canonica de llegada por las coordenadas del vector que queremos transformar (Que es el que queremos saber las coordenadas despues de aplicarle $F$)

$$Y_C = \begin{pmatrix}
1 & 1\\
-2 & 1\\
1 & 1
\end{pmatrix}\begin{pmatrix}
2\\
-1
\end{pmatrix} = \begin{pmatrix}
1\\
-5\\
1
\end{pmatrix}$$

En otras palabras las coordenadas del vector $(2,-1)_C$ en base canonica de partida, cuando le aplicamos $f$ su imagen tiene por coordenadas el vector $(1,-5,1)$ en la base canonica de llegada y aqui para calcular esas coordenadas del espacio vectorial de llegada hemos utilizado la matriz como columnas $(1,-2,1),(1,1,1)$ que nos convierte las imagenes de la base canonica de partida en la base canonica de llegada, solo con multiplicar esa martiz por un vector en la base canonica de partida nos dara las coordenadas de su imagen en la base canonica de llegada.

**Construcción**

Ya podemos definir formalemte como se puede calcular la imagen de un vector del espacio de partida utilizando la matriz asociada a la aplicacion lineal $F$

Sea $f: E\longrightarrow F$ una aplicación lineal y tiene $A$ la matriz asociada a $f$ respecto de $B_E$ (del espacio de partida) y $B_F$ (del espacio de llegada). Sean

- $X_{B_E}$ las coordenadas en base $B_E$ del vector $\vec{x}\in E$
- $Y_{B_F}$ las coordenadas en base $B_F$ del vector $f(\vec{x})\in F$

Y ademas se tiene la relacion, la base que esta multiplicando la matriz $A$ recuerda que es la que tiene por columnas las imagenes de los vectores de la base $B_E$ expresados en la base $B_F$

$$f(B_E) = B_FA$$

Se puede demostrar que, si multiplicamos a matriz $A$ por las coordenadas del vector $X$ (un punto (vector) del espacio vectorial que este expresado en esa base que tenga las coordenadas escritas en la base $E$) en la base $E$ nos da las coordenadas del vector $f(x)$ en la base $B_F$ (De resultado nos da la imagen del vector $f(x)$ expresado en la base $F$) 

$$AX_{B_E} = Y_{B_F}$$

**Ejercicio 15**
Demostrar que, efectivamente se cumple $AX = Y$

Viendo este dibujo, en el espacio vectorial de partida $E$ tenemos una base $B_E$, en el espaciovectorial de llegada $F$ tenemos una base $B_F$, tenemos $A$ la matriz asociada a la aplicacion lineal que tiene por columnas las imagenes de los vecctores de $B_E$ expresados en la base $B_F$ y abajo tenemos un punto encontreto, tenemos un $x\in E$ que cuando le aplicamos $f$ va a parar a $f(x)$ co sus coordenadas $x_F$, este es el esquema del planetemaiento general de la ecuacion matricial de una alicacion lineal y basicamente para calcular $f(x)$ en el espacio de llegada hay que multiplicar la matriz $A$ por el vector $x_E$ de partida, es decir las coordenadas de $f(x)$ son eso.

![<l class = "phototext">Esquema del planteamiento general de la ecuación matricial de una aplicación lineal</L>](Images/aplin5.png)

**Definición**

**Ecuación matricial de una aplicación lineal.** ($A$ por las coordenaas de $X,Y$)$AX_{B_E} = Y_{B_F}$ es la ecuación matricial de la aplicación lineal que relaciona las coordenadas de un vector $\vec{x}\in E$ en una base $B_E$ con las coordenadas $f(\vec{x})$ en una base $B_F$, esta es la relacion existente ente un vector y su imagen a traves de pasar por la matriz asociada a la aplicacion lineal. (Nos transforma las coordenadas de un vector $x$ en la base de partida en su imagen en la base de llegada)

**Observación.** Entonces, cualquier matriz $A\in\mathcal{M}_{m\times n}$ se puede considerar como la matriz asociada a una aplicación lineal $f:E\longrightarrow F$, donde $\dim(E) = n$ (La dimencion del espacio de partida coincida con el numero de columnas), $\dim(F)= m$ (El espacio de llegada coincia con las filas de la matriz), con respecto a unas bases dadas. Dada una matriz cualquiera podemos construir una aplicacion lineal que tenga esa matriz asociada

Si no especificamos nada, cuando digamos que $A$ es la matriz asociada a una aplicación lineal $f$, nos referimos a que lo es con respecto a las bases canónicas de $E$ y $F$ respectivamente, Si no haremos uso explicito de decir que esta es la matriz de la aplicacion lineal en la base que sea de partida y en la base que sea de llegada

**Teorema.** Sea $A\in\mathcal{M}_n(\mathbb{K})$ una matriz cuadrada de orden $n$. Son equivalentes

- $A$ es invertible
- Los vectores columna de la matriz $A$ son una base de $\mathbb{K}^n$ (Sus columnas son vectores de esa base)
- La aplicación lineal definida por 

$$\begin{matrix}
f_A: & \mathbb{K}^n&\longrightarrow &\mathbb{K}^n\\
 & X & \mapsto & AX
\end{matrix}$$ 

es biyectiva, Aqui esta los 3 elemento que hemos visto hasta el momento en el curso (Matrices,Vectores,Aplicaciones Lineales) estan totalemente relacionadas, es lo misma hablar de matrices invertivles que de una base que de aplicaciones lineales, es el mismo concepto, por eso en este tema salio lo del rango. Una matriz es como tener vectores en columna de una Base o es como tener una aplicacion lineal asociada a esa matriz que sea biyectiva

**Propiedades que salen de la ecuacion Matricial**

**Proposición.** Sean $E,F,G$ $\mathbb{K}$-espacios vectoriales de dimensión finita y sean $f:E\longrightarrow F$, $g:F\longrightarrow G$ aplicaciones lineales. Sean bases de n vectores $B_E = \{e_1,\dots,e_n\}$, $B_F = \{u_1,\dots,u_n\}$ y $B_G = \{v_1,\dots,v_n\}$ bases de $E,F$ y $G$ respectivamente.

Entonces la compocicon de $F$ con $G$ que basicamente es tomar un vector expresado en la base de $E$, aplicarle $f$ para irnos a la base en $F$ y aplicarle $g$ para irnos a la base de $G$

$$\underbrace{E_{B_E}\xrightarrow{f}F_{B_F}\xrightarrow{g}G_{B_G}}\\g\circ f$$

La compocicion es igual de trabajable que el producto de matrices o lo que es lo mismo, Sean $A,C,D$ las matrices asociadas de las aplicaciones lineales $f,g$ y a la aplicacion compocicion $g\circ f$ respectivamente. Entonces, la composicion $g\circ f$ tiene como matriz asociada el producto $CA$, La primera que multiplicamos seria la $A$ (que seria la matriz asociada a $f$) se pone al final y luego vendria la $g$ (Esto se lee al reves)

$$D = CA$$

La matiz asociada a la compocicion de funciones es el producto de matrices cambiado de orden de la propia compocicion

**Ejercicio 16**
Demostrar formalmente esta Proposición.

**Corolario.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales de dimensión finita y sean $B_E=\{e_1,\dots,e_n\}$, $B_F = \{v_1,\dots,v_n\}$ bases de $E$ y $F$ respectivamente. Sea $f:E\longrightarrow F$ un isomorfismo y sean $A,C$ las matrices asociadas a $f$ y $f^{-1}$ (Si $f$ es un isomorfismo su inversa tamien lo es) repecto a estas bases respectivamente que correspondan. Entonces, $A$ y $C$ son invertibles y $C = A^{-1}$

**Ejercicio 17**
Demostrar formalmente este Corolario. que utiliza la propocicion anterior 

**Proposición.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales de dimensión finita $n$ y $m$ respectivamente. Entonces, el espacio vectorial de las aplicaciones lineales de $E$ a $F$ (Que empiezan en $E$ y acaban en $F$ que es esta $\mathcal{L}$), $\mathcal{L}(E,F)$ es de dimensión finita $m\times n$, es decir el conjunto de todas las aplicaciones lineales que existen entre un espacio vectorial de dimencion $n$ y otro espacio vectorial de dimencion $m$ es el total de las aplicaciones lineales que podemos encontrar viven en un espacio vectorial de dimencion finita $m\times n$

**Ejercicio 18**
Demostrar formalmente esta Proposición. (Porque el total de estas aplicaciones lineales de $E$ a $F$, cada aplicacion lineal se asocia a una matriz y el conjunto de todas las aplicaciones lineales sera igual al cunjunto de todas las matrices de orden $m\times n$, de ahi que la dimencion sea esa, estamos traduciendo de conjunto de espacios vectoriales de todas las aplicaciones lineales a las matrices asociadas a la aplicacion lineal)

**Teorema.** Sea la aplicación lineal $f: E\longrightarrow F$

- $A$ la matriz de la aplicación lineal en las bases $B_E$ y $B_F$
- $C$ la matriz de la aplicación lineal en otras bases cualesquiera $B'_E$ y $B'_F$
- $P$ la matriz de cambio de base de $B'_E$ a $B_E$ (Es decir le damos un vector $B'_E$ y lo transforma a vector en la base $B_E$) ($B'_E\xrightarrow{P} B_E$)
- $Q$ la matriz de cambio de base de $B'_F$ a $B_F$ (Le damos vectores expresados en la base $B'_F$ y los multiplicamos por $Q$ nos da vectores en la base $B_F$) ($B'_F\xrightarrow{Q}B_F$)

Entonces resulta que la matriz $C$ (La matriz de la aplicacion lineal en la base $B'_E$ a $B'_F$) se puede expresar como el producto:

$$Q^{-1}AP = C$$

Para verlo mejor

![<l class = "phototext">Esquema general de matrices asociadas a aplicación lineal y cambios de base</l>](Images/aplin6.png)

Nosotros teniamos de $B_E$ a $B_F$ una matriz $A$ asociada a la aplicacion lineal, de $B'_E$ a $B'_F$ tenemos la matriz asociada a la misma aplicacion lineal llamada $C$, estas dos matrices seran diferentes  porque las bases de partida y llegada son diferentes y dentro de cada mismo espacio tenemos la matriz que nos lleva de $B'_E$ a $B_E$ que es multiplicar por el vector $P$ y tenemos la matriz que nos lleva de $B'_F$ a $B_F$ que es multiplicar por la matriz $Q$, La matriz de la aplicacion lineal $C$ la que nos lleva de $B'_E$ a $B'_F$ es lo mismo que hacer el recorrido de primero multiplicar por $P$ y luego multiplicar por $A$ y luego multiplicar por $Q^{-1}$ porque el camino de la $Q$ lo hacemos al revez, Este recorido si lo escribimos al revez:

$$Q^{-1}AP$$

Para ir de un vector en la base $B'_E$ calcularle la imagen en la base $B'_F$, solo nos queda por hacer primero multiplicar ese vector por $P$ para llevarlo a $B_E$ y luego aplicar la matriz $A$ para llevarlo al espacio $B_F$ y luego aplcar $Q^{-1}$ para deshacer el cambio de base

**Demostración**

La ecuación matricial de la aplicación lineal para La matriz $A$, que esta entre las bases $B_E,B_F$, cumple que: ($X$ y $Y$ serian vectores)

$$AX_{B_E} = Y_{B_F}$$

La ecuación matricial de la aplicación lineal para la matriz $C$ en las bases,$B'_E,B'_F$:

$$CX_{B'_E} = Y_{B'_F}$$

La ecuación de cambio de base de $B'_E$ a $B_E$: Si tomamos un vector $X_{B'_E}$ y lo multiplicamos por $P$ por delante nos da un vector $X_{B_E}$

$$X_{B_E} = PX_{B'_E}$$

La ecuación de cambio de base de $B'_F$ a $B_F$: 

$$Y_{B_F} = QY_{B'_F}$$

(Esto anterior es de la definicion de estas 4 matrices $A,C,P,Q$, salen estas 4 formulas)

Por lo tanto, con la primera ecuación y la tercera, sustituyendo el $X_{B_E}$ de la primera ecuacion por su igualdad en la tercera, nos queda

$$A(PX_{B'_E}) = Y_{B_F}$$

Además, por la última tenemos que $Y_{B_F}$ es igual a la ecuacion ultima $QY_{B'_F}$

$$A(PX_{B'_E}) = QY_{B'_F}$$

Multiplicando a ambos lados por (La inversa de $Q$ existe porque esta es una matriz de cambio de base) $Q^{-1}$ por la izquierda y aplicando la propiedad asociativa del producto de matrices, se obtiene que las coordenadas de $Y_{B'_F}$:

$$(Q^{-1}AP)X_{B'_E} = Y_{B'_F}$$

De modo que la matriz $C$, Que es la ecuación matricial de $f$ en $B'_E$ y $B'_F$. Con lo cual, $Q^{-1}AP$ será la matriz asociada a $f$ en estas bases:

$$Q^{-1}AP = C$$

Este es el resultado de como se puede pasar de una base a otra en la coordenadas de una matriz asociada a una aplicacion lineal

**Corolario.** Sea la aplicación lineal de un espacio vectorial en si mismos (Endomorfismo) $f: E\longrightarrow E$ y sean

- $A$ la matriz de la aplicación lineal en la base $B_E$
- $C$ la matriz de la aplicación lineal en la base $B'_E$
- $P$ la matriz de cambio de base de $B'_E$ a $B_E$

Aqui solo podemos cambiar de una base, ya que solo tenemos un espacio vectorial

Entonces se cumple que la matriz $C$

$$P^{-1}AP = C$$

![<l class = "phototext">Esquema general de la idea del `Corolario` anterior</l>](Images/aplin7.png)

En los dos tenemos el mismo espacio vectoria de partida y de llegada $E$, en el $B_E$ de las mismas bases tenemos la aplicacion lineal $A$, de las bases $B'_E$ tenemos otra forma de la matriz asociada a la aplicacion lineal $C$ y tenemos para ir de $B'_E$ a $B_E$ la $P$ de cambio de base en este caso la matriz $C$ se puede obtener como el producto de $P\cdot A\cdot P^{-1}$ al revez por tanto que es como la fomula de arriba

## Ejemplos de Ecuaciones Matriciales Asociadas a una Aplicacion Lineal

Segun lo visto de matriz asociada a una aplicación lineal y la relación que existe, se que para calcular la imagen de un vector basta con conocer la imagen de los vectores de una base del espacio de partida expresada en una base del espacio de llegada plasmarlo.

Para empezar tomemos una aplicación $f:\mathbb{R}^3\longrightarrow\mathbb{R}^2$ de modo que $f(x,y,z$ es la aplicación $(x+y,y-z)$ y queremos hallar la matriz asociada a $f$ respecto de la base canónica $e$ del espacio de partida $\mathbb{R}^3_e$ y la base canónica del espacio de llegada de $\mathbb{R}^3_e$. Lo único que tenemos que hacer es calcular la imagen de estos tres vectores la imagen, (Serian los vectores de la base canónica en $\mathbb{R}^3$):
- $f(e_1)=f(1,0,0)=(1,0)$
- $f(e_2)=f(0,1,0)=(1,1)$
- $f(e_3)=f(0,0,1)=(0,-1)$

Esos serían las imágenes de los vectores de la base canónica de partida expresadas en la base canónica de llegada y la matriz asociada a la aplicación lineal serían estos tres vectores en columna.

$$A=\left (\begin{matrix}
1 & 1 & 0\\
0 & 1 & -1
\end{matrix}\right)$$ 

En este caso ahora podemos decir y aplicando las operaciones: 

$$f(x,y,z)=A\cdot \left (\begin{matrix}
x\\
y\\
z
\end{matrix}\right) = \left (\begin{matrix}
1 & 1 & 0\\
0 & 1 & -1
\end{matrix}\right)\left (\begin{matrix}
x\\
y\\
z
\end{matrix}\right) = (x+y,y-z)$$


Así sería como se calcular la matriz asociada de la aplicación lineal, cualquier matriz de m filas y n columnas, se puede considerar como matriz asociada a una aplicación lineal entre $E$ y $F$ donde $E$ sea un espacio vectorial de dimensión n (el número de columnas que tenga la matriz) y $F$ tenga dimensión m (el número de filas) respecto de unas bases fijas, si no se dice lo contrario, cuando pongamos una matriz de m filas y n columnas, será la matriz asociada a una aplicación lineal en bases canónicas. Fijensen que el espacio vectorial de partida $\mathbb{R}^3$ por eso la matriz asociada tiene tres columnas, el espacio de llegada es $\mathbb{R}^2$ por eso tiene dos filas y en principio tanto nos da a pasar de matriz a aplicación lineal como de aplicación lineal a matriz.

**Otro Ejemplo**

Por ejemplo tenemos la matriz $A_{3x4}$ 

$$
A_{3x4}=\left (\begin{matrix}
1 & 2 & -1 & 3\\
0 & 1 & 1 & 0\\
1 & 0 & 0 & 1
\end{matrix}\right)
$$

Que a la vez es la matriz asociada a una aplicación lineal,y por lo que vimos anteriormente tendría como espacio vectorial de partida de $\mathbb{R}^4$ porque tiene cuatro columnas y un espacio vectorial de llegada $\mathbb{R}^3$ porque tiene tres y evidentemente si no nos dicen nada se supone que tomamos la base canónica $e$ de partida y la base canónica de llegada, $f: \mathbb{R}^4_e\longrightarrow \mathbb{R}^3_e$, ¿Cuál sería esa aplicación lineal asociada?. Habría que juntar todo y hacer el producto matricial:

$$f(x,y,z,t)=
\left (\begin{matrix}
1 & 2 & -1 & 3\\
0 & 1 & 1 & 0\\
1 & 0 & 0 & 1
\end{matrix}\right)
\cdot \left (\begin{matrix}
x\\
y\\
z\\
t
\end{matrix}\right) = 
\left (\begin{matrix}
x+2y-z+3t\\
y+z\\
x+t
\end{matrix}\right)$$

Ese sería el vector asociado, sería la aplicación lineal en forma en forma analítica, por tanto nosotros en lugar de escribirlo en columna tal vez estemos más acostumbrados a leerlo como: $f(x,y,z,t)=(x+2y-z+3t,y+z,x+t)$ así de fácil sería la traducción de vuelta desde una matriz a la aplicación lineal asociada.

**Otro Ejemplo**

$E$ es $\mathbb{K}-e.v$ de $dim(E)=n$ y consideramos $B$ una base formada por los vectores $B=\{u_1,u_2,...,u_n\}$ entonces la matriz de la aplicación identidad que es:

$$
\begin{matrix}
Id:&E\longrightarrow E\\
&x\rightarrow x
\end{matrix}
$$

Con respecto de esta base $B$ elegida en el espacio de partida y de llegada: 

$$
\begin{matrix}
Id:&E_B\longrightarrow E_B\\
&x\rightarrow x
\end{matrix}
$$

Sería que la aplicación identidad tiene por matriz asociada la identidad de orden n, es decir la matriz de n filas y n columnas que tiene ceros por todos lados menos la diagonal. $Id~I_n$, ya que la imagen de cada vector (la identidad aplicada a cualquier vector), $Id(u_i)=u_i$, así que la matriz asociada a la aplicación identidad es la matriz identidad para cualquier base de un espacio vectorial fijo.

Además supongamos que queremos considerar la aplicación identidad $Id:E\longrightarrow E$ bajo las mismas hipótesis, pero esta vez queremos calcular la matriz asociada respecto a unas bases en principio cualesquiera: $Id:E_B\longrightarrow E_{B'}$, $B=\{u_1,...,u_n\}$ y $B'=\{v_1,...,v_n\}$, aquí no se habla ni de canónicas ni de nada, simplemente son dos bases, entonces cada columna de la matriz corresponderá que al escribir, $Id(u_j)=u_j$, pero hay que expresarlo en la combinación lineal de la nueva base es decir hay que expresar esto como $\sum_{i=1}^{n}a_{ij}u_i$, es decir hay que traducir a la nueva base el resultado de haber cambiado de una base a otra, por tanto esta matriz será la matriz que hemos llamado la matriz de cambio de base de $B'$ a $B$, si quieremos la matriz en este caso en bases diferentes, la matriz asociada a la aplicación identidad en una base $B$ de partida y que va a parar a una base $B'$ de llegada, es la matriz de cambio de base de $B'$ a $B$ que es la matriz de cambio de base en este orden.

Por último en el caso en que además $f$ sea un isomorfismo entre dos espacios vectoriales, $f:E\longrightarrow F$, si consideramos una base cualquiera del espacio de partida $E$, $B=\{u_1,...,u_n\}$  y por el teorema que ya sabémos, $f(B)=\{f(u_1),...,f(u_n)\}$ será una base de $F$, entonces la matriz asociada a $F$ respecto a estas bases sea quien sea $F$ siempre será la matriz identidad de orden n, $f\text{~} A = I_n$ porque tenemos como base de partida una base cualquiera de $F$ pero la imagen de esos vectores de la base de $E$ son son la base de $F$ entonces bajo esto, la matriz asociada a la aplicación lineal $f$ será también la identidad.

**Otro Ejemplo**

Si la base $B'=\{u_1,u_2,...,u_n\}$ de $\mathbb{R}^3$ donde los vectores: 
- $u_1=(1,0,0)$
- $u_2=(1,1,0)$
- $u_3=(1,1,1)$

Queda como ejercicio demostrar que esto es una base, simplemente calcular el determinante de estos vectores y que nos de cero.

$B$ es la canónica $B=\{e_1,e_2,e_3\}$ de $\mathbb{R}^3$

Queremos hallar cuál sería la matriz asociada a la identidad de $\mathbb{R}^3$ en $\mathbb{R}^3$ donde la base de partida es $B'$ y la base de llegada es $B$ 

$$Id:\mathbb{R}^3_{B'}\longrightarrow\mathbb{R}^3_{B}$$

Y para ver como funciona podríamos calcular las coordenadas de un vector concreto, $f(-1,-2,7)_{B'}$ expresado en la base $B'$, es decir este es el vector $-1$ vez el $u_1$, $-2$ veces $u_2$ y $7$ veces $u_3$, quien sería la imagen de este vector una vez que tengamos la matriz asociada.

Para calcular la matriz, sólo hay que calcular cada elemento $u_j$ como combinación lineal en este caso de la base canónica. $f(u_1)=u_1$ pero en términos de la base canónica es $e_1$, $f(u_2)=u_2$ que en términos de la base canónica sería $e_1+e_2$ y $f(u_3)=u_3$ porque la aplicación es la identidad y el vector es $e_1+e_2+e_3$, esto significa que si ponemos las imágenes de estos tres vectores en columna, la matriz asociada a la aplicación lineal es:

$$
A=\left (\begin{matrix}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{matrix}\right)
$$

Esta sería la matriz de cambio de base y esto coincide con la matriz de cambio de base $B\longrightarrow B'$ y que en este caso es la matriz de la aplicación lineal donde la base del espacio de partida es $B'$ y la base del espacio de llegada es $B$.

Además para calcular las coordenadas del vector $(-1,-2,7)$ en la base $u_1,u_2,u_3$, lo que podríamos hacer es expresar 

$$\left (\begin{matrix}
-1\\
-2\\
7
\end{matrix}\right)$$ 

como el producto de la matriz 

$$\left (\begin{matrix}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{matrix}\right)$$

aplicado a un cierto vector 

$$\left (\begin{matrix}
x'\\
y'\\
z'
\end{matrix}\right)$$ 

lo podríamos despejar:

$$
A^{-1} \left(\begin{matrix}-1\\-2\\7\end{matrix}\right)
$$

Queda como ejercicio que calculemos la inversa que sería la matriz y si la multiplicamos por el vector:

$$
\left(\begin{matrix}
1&-1&0\\
0&1&-1\\
0&0&1
\end{matrix}\right )
\left(\begin{matrix}-1\\-2\\7\end{matrix}\right)
$$


Nos da la solución del vector que estámos buscando $f(-1,-2,7)$, seria el:

$$\left(\begin{matrix}1\\-9\\7\end{matrix}\right)$$

Así es como podríamos calcular la imagen de un vector expresado en la base $B'$ a partir de su expresión matricial, así es como hemos calculado las coordenadas del vector $f(-1,-2,7)$ respecto de la base $B'$.

## Ejercicio 15

Si la aplicacion lineal $f:E\longrightarrow F$, $f$ tiene por matriz asociada $A$ respecto de una base $B_E$ de partida y $B_F$ de llegada, de modo que un vector $x\in E$ tiene por coordenadas $X_{B_{E}}$ y la imagen $f(x)\in F$ tiene por coordenadas $Y_{B_F}$ entonces se puede demostrar que la matriz asociada $A$, a la aplicación lineal por las coordenadas en la base de partida $X$ da las coordenadas $Y$ de la base de llegada.

$$A\cdot X_{B_{E}} = Y_{B_F}$$

Se trata de una ecuación matricial que nos permitirá ahorrar trabajo a la hora de calcular imágenes.

Si tomamoa $x\in E$ siendo $B=\{u_1,u_2,...,u_n\}$ una base de ese espacio vectorial, $x$ se podría escribir como una combinación lineal (vamos a usar la $j$): 

$$x=\sum_{j=1}^{n}x_ju_j$$

Resultaría que

$$f(x)=f\left(\sum_{j=1}^{n}x_ju_j\right )$$

Y al ser una aplicación lineal esto se traduciría en que las coordenadas: 

$$\sum_{j=1}^{n}x_jf(u_j)$$

De hecho como les hemos llamado $X_{B_E}$ por esos le llamamos $x_j$ y así tenemos una representación como el vector $\vec{x}=\{x_1,x_2,...,x_n\}_{B_{E}}$, será más fácil seguir la demostración si utilizamos las $x$ en lugar de las $a$ entonces las $x$ son del espacio de partida y las coordenadas $y$ son las del espacio de llegada y como las coordenadas de cada imagen del vector de la base de partida $f(u_j)$ en la nueva base llamémosle $v_i$ del espacio vectorial de llegada vienen dadas por las columnas respectivas de la matriz $A$, podría escribir ahora si con las entradas $a_{ij}$ de la matriz (cada columna) multiplicado por los vectores de la base de $F$,($v_i$) 

$$f(u_j)=\sum_{i=1}^{n}a_{ij}v_i$$

En este contexto de la demostración los $u_i$ son la base de $E$ y en este caso los $v_i$ son la base de $F$, pues juntandolo todo:

$$y=f(x)=\sum_{j=1}^{n}x_j\left(\sum_{i=1}^{n}a_{ij}v_i\right)$$

Si sacamos los $v_i$ factor común y reordenamos los miembros dado que podemos aplicar la distributiva:

$$\sum_{i=1}^{n}\left(\sum_{j=1}^{n}a_{ij}\cdot x_j\right)v_i$$

Esto nos dice que las coordenadas de el vector $y=f(x)$ tiene por coordenadas en la base $v_i$ los elementos $\sum_{j=1}^{n}a_{ij}\cdot x_j$ o sea en la base de $F$, pero tendría por coordenadas $\vec{y}=\{y_1,y_2,...,y_3\}_{B_F}$, Lo que nos estan diciendo es que cada coordenada: 

$$y_i=\sum_{j=1}^{n}a_{ij}\cdot x_j$$

Que es lo que queremos demostrar por qué si ahora lo escribimos de forma matricial el vector $y$ en columna, es el vector $f(x)_{B_E}$ en la base $Y_{B_F}$, (se les puso $m$ porque no tendiran por que ser del mismo tamaño) 

$$\left(\begin{matrix}y_1\\y_2\\\vdots\\y_m\end{matrix}\right)$$

Esto sería el producto matricial de: 

$$
\left(\begin{matrix}
a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}
\end{matrix}\right )
$$

Esto se verificaría simplemente anteriormente cuando hemos hecho el sumatorio se ha puesto $n$ por todos lados, podríamos tomar que la matriz no fuera cuadrada y por tanto en el espacio de llegada no hubiera $n$ vectores sino que hubiera $m$ vectores 

$$f(u_j)=\sum_{i=1}^{m}a_{ij}v_i$$
y ésta $m$ se pondira en:

$$y=f(x)=\sum_{j=1}^{n}x_j\left(\sum_{i=1}^{m}a_{ij}v_i\right)=
\sum_{i=1}^{m}\left(\sum_{j=1}^{n}a_{ij}\cdot x_j\right)v_i$$

El vector en la base $B_F$ tendría $m$ componentes,$\vec{y}=(y_1,y_2,...,y_m)_{B_F}$, y se podría expresar como combinación lineal.

Entonces sería el producto de la matriz multiplicado por el vector columna $x$ ahora si hasta $n$.

$$\left(\begin{matrix}y_1\\y_2\\\vdots\\y_m\end{matrix}\right)
=
\left(\begin{matrix}
a_{11}&a_{12}&\cdots&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2n}\\
\vdots&\vdots&&\vdots\\
a_{m1}&a_{m2}&\cdots&a_{mn}
\end{matrix}\right )
\left(\begin{matrix}x_1\\x_2\\\vdots\\x_n\end{matrix}\right)$$

Esta expresión $y_i=\sum_{j=1}^{n} a_{ij}\cdot x_j$ sería por ejemplo que para $y=1$ nos diría que $y_1$ (del vector de arriba) es igual a $a_{11}x_{1}$ más (fijaros que el $1$ es fijo vamos cambiando la $j$) $a_{12}x_{2}$  y así sucesivamente hasta completar el producto por filas y columnas $a_{1n}x_{n}$ así que tenemos el resultado matricial $y_i=\sum_{j=1}^{n} a_{ij}\cdot x_j$ en forma de ecuación lineal, así tenemos demostrado el teorema que básicamente el producto de la matriz asociada a la aplicación lineal por las coordenadas de un vector en una base del espacio de partida da como lugar las coordenadas de $F$ de ese vector en la base del espacio de llegada.

## Ejercicios 16,17 y 18

**Primero: **

Si $E,F,G \ \mathbb{K}-e.v$ de dimensión finita y consideramos las aplicaciones lineales, $f:E\longrightarrow F,g:F\longrightarrow G$  y tomamos bases ordenadas, $\{e_1,e_2,...,e_n\}$ base de E,$\{u_1,u_2,...,u_n\}$ base de F,$\{v_1,v_2,...,v_n\}$ base G, podemos hacer lo siguiente: 

Si $E$ está con una base $\{e_i\}$, le aplicamos la aplicación lineal $f$, nos iríamos a $F$ de la base $\{u_i\}$ y le aplicamos $g$ para irnos al espacio vectorial $G$ en la base $\{v_i\}$

$$E_{\{e_i\}}\xrightarrow{f} F_{\{u_i\}}\xrightarrow{g} G_{\{v_i\}}$$

Si $A,B,C$ son las matrices asociadas $A$ a $f$ por un lado, $B$ a $g$ por otro lado y a la composición que sale de aquí de componer $f\circ g$ (Se lee al revez) tiene por matriz asociada $C$, 

$$\underbrace{E_{\{e_i\}}\xrightarrow{f}F_{\{u_i\}}\xrightarrow{g}G_{\{v_i\}}}\\g\circ f\backsim C$$

En estas bases resulta que $C=B\cdot A$ es decir la matriz asociada a la composición es el producto de matrices en orden inverso (hacia atrás) primero multiplicamos por $A$ y luego por $B$.

Para demostrar este primer resultado sea la matriz $A=(a_{ij})\backsim f$ respecto de las bases $e_j$ y $u_i$ respectivamente y tomemos también la matriz $B=(b_{jk})\backsim g$
asociada a la aplicación lineal $g$ respecto de las bases $u_i$ y $v_k$ respectivamente. Bajo este contexto, ¿Quién sería $f$ de cualquier vector de la primera base ($f(e_j)$)? sería el sumatorio hasta $m$ porque el espacio de llegada tiene $m$ dimensiones, es $u_i$ porque vamos del espacio vectorial $E$ a $F$:

$$f(e_j)=\sum_{i=1}^{m}a_{ij}u_i$$
Esto ya lo hemos hecho muchas veces, es cambiar entre la expresión analítica y la expresión matricial y ¿Quien sería $g(u_i)$?, sería el sumatorio, (Es hasta $s$ porque es la dimencion del ultimo espacio vectorial $G$): 

$$g(u_i)=\sum_{k=1}^{s}b_{ki}v_k$$

Por tanto la composición $g\circ f$ aplicado a un vector $e_j$ cualquiera, sería $g$ aplicado a $f(e_j)$, pero esto sería $g$ aplicado a la combinación lineal de $f(e_j)$ que es $\sum_{i=1}^{m}a_{ij}u_i$ por linealidad podemos sacar el sumatorio y los $a_{ij}$ fuera de $g$ y ahora nos queda $g(u_i)$ que lo tenémos expresado como el sumatorio $\sum_{k=1}^{s}b_{ki}v_k$:

$$
(g\circ f)(e_j)=g(f(e_j))=g\left (\sum_{i=1}^{m}a_{ij}u_i\right)=\sum_{i=1}^{m}a_{ij}g(u_i)=\sum_{i=1}^{m}a_{ij}\sum_{k=1}^{s}b_{ki}v_k
$$

Nos quedaría todo esto el cual lo podriamos reescribir correctamente sacando el sumatorio más interno fuera:

$$\sum_{k=1}^{s}\left(\sum_{i=1}^{m}b_{ki}\cdot a_{ij}\right)v_k$$

Pero esto es la combinación lineal que podríamos llamar como este sumatorio de cierta matriz $C$:

$$\sum_{k=1}^{s}C_{kj}v_k$$


Donde la matriz $C$ tiene por entradas $C_{kj}$ definidas por $\left(\sum_{i=1}^{m}b_{ki}\cdot a_{ij}\right)$ pero cuando definimos el producto matricial nos pusieron la fórmula que es exactamente esta que tenemos aquí, es decir cada elemento $C_{kj}$ de la matriz $C$ resultante de la composición, coincide con la expresión $\left(\sum_{i=1}^{m}b_{ki}\cdot a_{ij}\right)$ que es el producto de la fila k-esima de la matriz $B$ multiplicada por la columna j-ésima de la matriz $A$,$C=BA$, así que en efecto la composición de funciones de aplicaciones lineales resulta en el producto de las matrices asociadas a cada una de ellas girado el signo, además este resultado demuestra que en particular el producto de matrices es asociativo ya que se corresponde con la composición de aplicaciones lineales y ya sabemos que la composición de aplicaciones lineales es asociativa, por tanto si en lugar de querer demostrar que el producto de matrices será asociativo en el **tema 1** que es una cosa muy pesada de demostrar, ahora por isomorfismo lo tenémos de forma trivial. Todo producto matricial se corresponde con una composición de aplicaciones lineales y la composición de aplicaciones lineales es asociativa por tanto el producto matrices también lo es.

De aquí se deduce **un corolario**, con la misma notación,$E,F\ \mathbb{K}-e.v$ que tengan dimensión finita y además son isomorfos $E\cong F$ y tomamos dos bases: $\{e_1,...,e_n\}$ base de $E$,$\{v_1,...,v_n\}$ base de $F$ (si son isomorfos tendrá el mismo número de elementos), si $F$ es uno de esos isomorfismo cualquiera podemos crear entre $f:E\longrightarrow F$, y tenemos la matriz asociada a ese isomorfismo $f\backsim A$ y existe el isomorfismo contrario $f^{-1}$ también será isomorfismo y tiene por matriz asociada $B$, $f^{-1}\backsim B$ respecto de las bases respectivas que correspondan en cada caso, entonces siempre resultará que $A$ y $B$ invertibles y las relacion es que $B^{-1}=A$ indistintamente $B=A^{-1}$, del mismo modo en que $f$ y $f^{-1}$ son aplicaciones inversas la una de la otra pues las matrices también son inversas la una de la otra.

La demostración es, si denotamos en este caso la matriz $M$ asociada a la aplicación lineal $f$ entre el espacio $E$ con coordenadas $\{e_i\}$ y el espacio $F$ con coordenadas $\{v_i\}$ y hacemos el producto con la matriz asociada $M$ a la aplicación inversa $f^{-1}$ definida entre $F$ en coordenadas o en base $\{v_i\}$ hasta $E$ en base $\{e_i\}$: 

$$M(f:E_{\{e_i\}}\longrightarrow F_{\{v_i\}})\cdot M(f^{-1}:F_{\{v_i\}}\longrightarrow E_{\{e_i\}})$$

Así tal cual, por el teorema anterior esto sería como hacer la composición de funciones, esto será la matriz $M$ asociada a la composición, pero es que la composición de $f$ con $f^{-1}$ es la identidad definida en este caso desde el espacio vectorial $F$ con coordenadas $\{v_i\}$ hacia el espacio vectorial $F$ con coordenadas $\{v_i\}$ y tiene por matriz asociada a la identidad:

$$M(I:F_{\{v_i\}}\longrightarrow F_{\{v_i\}})=I$$

Pero es que esto $M(f:E_{\{e_i\}}\longrightarrow F_{\{v_i\}})$ es lo que hemos llamado la matriz $A$ y esto $M(f^{-1}:F_{\{v_i\}}\longrightarrow E_{\{e_i\}})$ es lo que hemos llamado la matriz $B$, y si el producto $AB=I$ de orden n, es que despejando una de ellas $B=A^{-1}$, recuerda que esto no es suficiente, habría que demostrar también lo contrario es decir que la matriz de $f^{-1}$ por la matriz de asociada a $f$ también es la identidad.

Si en este corolario tomamos $E=F$, si los dos espacios vectoriales de salida y llegada son el mismo y tomámos dos bases respectivas $e_i$ y $v_i$ bases del mismo espacio vectorial $E$, lo que obtenemos es la matriz de cambio de base de $e_i$ a las $v_i$ y la matriz de cambio de base $v_i$ a $e_i$, hay que demostráe que esas matrices de cambio de base si el espacio vectorial es el mismo son inversas la una de la otra.

Finalmente vamos con el último resultado, sean $E,F\ \mathbb{K}-e.v$,$dim(E)=n$ y $dim(F)=m$, resulta que el espacio vectorial de todas las aplicaciones lineales que se pueden definir entre el espacio vectorial $E$ y $F$ es isomorfo al de las matrices $M_{mxn}(\mathbb{K})$: 

$$l(E,F)\cong M_{mxn}(\mathbb{K})$$

El espacio vectorial de las aplicaciones lineales tendrá dimensión finita y además la dimensión de el espacio vectorial de todas las aplicaciones lineales que existen entre $E$ y $F$ será gracias a este isomorfismo a $mxn$:

$$dim(l(E,F))=mxn$$

Entonces vamos a establecer explícitamente quién es ese isomorfismo y a demostrar este resultado. Para esto lo único que tenemos que hacer es definir una base de $E$ llamémosle $\{u_i\}$ de $n$ elementos y $\{v_i\}$ una base de $F$ que tendrá $m$ elementos. A partir de una aplicación definida entre esas bases: $f:E_{\{u_i\}}\longrightarrow F_{\{v_i\}}$ lo que podemos hacer es construir una aplicación que sea $\phi:l(E,F)\longrightarrow M_{mxn}(\mathbb{K})$ de modo que $\phi$ de una aplicación lineal $f$ cualquiera va a parar a la matriz asociada de esa aplicación lineal, es decir:

$$\phi(f)=M(f:E_{\{u_i\}}\longrightarrow F_{\{v_i\}})$$

Resulta que esto es un isomorfismo de espacios vectoriales, claramente $\phi$ es lineal porque una vez que hemos fijado las bases, simplemente la matriz asociada $M(f+g)$ entre el espacio $E$ y $F$ respectivamente con base fija es la matriz asociada $M(f)+M(g)$, $M(\alpha f)=\alpha M(f)$ donde $f$ es una alpicacion lineal, es sencillo demostrar esa linealidad como la linealidad de las aplicaciones lineales se traduce a la linealidad de las matrices.

Lo siguiente es ver que $\phi$ es inyectiva, $\phi(f)=0$ o sea $\phi=0$, eso significaría que la matriz asociada a la aplicación lineal entre $E$ y $F$ también sería la matriz cero: $M(E\longrightarrow F)=0$ y esto queire decir que la imagen $f$ de todos los vectores de la base de partida serían cero $f(u_i)\forall i=1,...,n$ porque las columnas de la matriz asociada a la aplicación lineal son las imágenes en columna de los vectores de la base de partida entonces $f(u_i$ sería cero para todo $i$, es decir $f$ enviaria todo vector al 0, $f(x)=0\ \forall x\in E$ o lo que es lo mismo $f$ es la aplicación totalmente nula, $f=0$ y por tanto $\ker(\phi)=\{0\}$ y que es exhaustiva ya lo tenemos bastante demostrado porque hemos visto que cualquier matriz $A$ del conjunto de matrices $M_{mxn}(\mathbb{K})$ es la matriz asociada a alguna aplicación lineal respecto de unas bases fijas entonces directamente cualquier matriz puede construir la aplicación lineal asociada así que trivialmente es exhaustiva.

Así que habémos hallado claramente un isomorfismo entre el conjunto de aplicaciones lineales entre $E$ y $F$ y las matrices de m filas y n columnas, estos dos espacios vectoriales son isomorfos y por tanto tienen la misma dimensión es decir $mxn$.

## Ejemplos de matriz asociada a una aplicación lineal en distintas bases

Si nos dan una aplicación lineal $f:\mathbb{R}^4\longrightarrow\mathbb{R}^4$ lo que llamamos un endomorfismo que venga definido por $f(x,y,z,t)=(2z,x+z,y-2z,t)$ nos piden calcular la matriz asociada a $f\sim A$, tomando como base la canónica $\{e_1,e_2,e_3,e_4\}=B_C$, lo único que hay que hacer es calcular las imágenes de los cuatro vectores la base canonica de $\mathbb{R}^4$, $f(e_1),f(e_2),f(e_3),f(e_4)$ para lo cual hay que calcular:
- $f(e_1)=f(1,0,0,0)=(0,1,0,0)$
- $f(e_2)=f(0,1,0,0)=(0,0,1,0)$
- $f(e_3)=f(0,0,1,0)=(2,1,-2,0)$, lo unico que hacemos es sustituir en la formula de la definicion de la aplicacion lineal
- $f(e_4)=f(0,0,0,1)=(0,0,0,1)$

De donde la matriz asociada a la aplicación lineal respecto en esta base serían estos cuatro vectores que acabamos de calcular en columna:

$$
A=\left(\begin{matrix}
0&0&2&0\\
1&0&1&0\\
0&1&-2&0\\
0&0&0&1
\end{matrix}\right )
$$

¿Qué pasaria si nos piden con respecto a una base que no sea la canónica? Nos piden la matriz asociada a la aplicación lineal $f\sim B$ pero utilizando en este caso la base
$B_u=\{u_1,u_2,u_3,u_4\}$ donde cada: $u_i=\sum_{j=1}^{i}e_i$, 
- El $u_1$ sería sumar desde 1 hasta 1 el vector $e_i$ que seria $u_1=e_1$.
- El $u_2$ sería sumar desde 1 hasta 2 los vectores $e_i$ por tanto sería $u_2=e_1+e_2$ 
- El vector $u_3$ sería sumar los tres primeros vectores de la base canónica, $u_3=e_1+e_2+e_3$
- $u_4$ sería sumar los 4 vectores de la base canónica que van a ser $u_4=e_1+e_2+e_3+e_4$

Como tenemos esta base, si llamamos $B$ a esta matriz una forma de expresarla sería como el producto de la matriz de cambio de base de la canónica a $u_i$, es decir ya tenemos de $\mathbb{R}^4$ en $\mathbb{R}^4$ la aplicación lineal $f$, con su expresión en matriz $A$, $f\sim A$, esto venía dado por la base canónica $B_C$ tanto de partida como de llegada, ahora lo que queremos es que del mismo espacio de $\mathbb{R}^4\longrightarrow \mathbb{R}^4$ la expresión de $f$ a la matriz asociada $f\sim B$ pero en este caso en la base $B_u$ y esto sería lo mismo que buscar la matriz de cambio de base que nos llevara de $B_u$ a la base canónica que la llamaremos la matriz $P$ multiplicar por la matriz $A$ al seguír el diagrama y como quieremos bajar hay multiplicar por $P^{-1}$.

$$
\begin{matrix}
\mathbb{R}^4_{B_c} & \xrightarrow{f\sim A} & \mathbb{R}^4_{B_c}\\
\uparrow{P} & & \downarrow{P^{-1}}\\
\mathbb{R}^4_{B_u} & \xrightarrow{f\sim B} & \mathbb{R}^4_{B_u} 
\end{matrix}
$$

Como este diagrama conmuta para calcular la matriz $B$ habría que hacer primero la matriz $P$ por la matriz $A$ por la matriz $P^{-1}$, $B=P^{-1}AP$, donde $P$ es la matriz de cambio de base de $\langle e_i\rangle$ a $\langle u_i\rangle$, siempre es de donde querémos llegar al sitio de partida, esto es porque nos devuelve vectores en las coordenadas $\langle e_i\rangle$ a partir de vectores que se encuentran en las coordenadas $\langle u_i\rangle$. Entonces hay que buscar esta matriz de cambio de base pero simplemente se trata de obtener

las coordenadas de los vectores de la base $u$ en la base $e$ en columna, por tanto esta matriz de cambio de base $P$ sería: ($(1,0,0,0)$ porque el primer vector $u_1=e_1$ este sería las coordenadas del vector $u$ en la base canónica $(1e_1+0e_2+0e_3+0e_4)$, el segundo sería $(1,1,0,0)$ que correspondería con el segundo vector de la base $u$ expresado en términos de la base canónica $u_2=e_1+e_2$ y así sucesivamente)

$$
P=\left(\begin{matrix}
1&1&1&1\\
0&1&1&1\\
0&0&1&1\\
0&0&0&1
\end{matrix}\right )
$$

Bajo estas hipótesis podríamos calcular $P^{-1}$ ya que si $P$ es una matriz de cambio de base esta siempre se invertible.

$$
P=\left(\begin{matrix}
1&-1&0&0\\
0&1&-1&0\\
0&0&1&-1\\
0&0&0&1
\end{matrix}\right )
$$

Entonces como tenemos la matriz de cambio de base de $e_i$ a $u_i$, ahora para calcular la matriz $B$ hacemos el producto $B=P^{-1}AP$ que nos va a devolver la matriz:

$$
B=\left(\begin{matrix}
-1&-1&0&0\\
1&0&3&3\\
0&1&-1&-2\\
0&0&0&1
\end{matrix}\right )
$$

Aquí tenemos la expresión matricial de $F$ pero respecto en este caso de la base $B_u$ por tanto en canónica que es trivial como en una base diferente fijensen que los cálculos siempre son los mismos de que siempre pasaremos por la matriz de cambio de base para llevarlo todo a la canónica por qué en la canónica todo es mas fácil de calcular. Pero no hay que pensar que esto sólo es cierto para endomorfismos.

Por ejemplo si tuvieramos $f$ definida de $\mathbb{R}^3$ en $\mathbb{R}^2$ y tuvieramos por expresión respecto de las bases canónicas de $\mathbb{R}^3$ y $\mathbb{R}^2$ donde tendríamos la canónica de $\mathbb{R}^3_c$ y $\mathbb{R}^3_c$ y $f$ tuviera respecto a las bases canónicas de partida y de llegada $f\sim A$ la matriz a continuacion 

$$
\begin{matrix}
f: & \mathbb{R}^3_c & \longrightarrow & \mathbb{R}^2_c\\
 & f\sim A & \cdot & \left(\begin{matrix}
2&1&0\\
0&1&-1\\
\end{matrix}\right )\\
\end{matrix}
$$

Ahora podríamos considerar otras bases totalmente diferentes en el espacio de partida y en el espacio de llegada, por ejemplo en el espacio de partida de $\mathbb{R}^3$ consideremos la base formada por los vectores $B_u=\{u_1,u_2,u_3\}$ que venga definido por:
- $u_1=e_1+e_2+e_3$
- $u_2=e_2+2e_3$
- $u_3=2e_2+e_3$

Estos tres vectores linealmente independientes en un espacio de dimensión $3$ forman una base de $\mathbb{R}^3$ y en $\mathbb{R}^2$ consideremos la base formado por los vectores $B_v=\{v_1,v_2\}$  donde en este caso 
- $v_1=2e_1+e_2$
- $v_2=e_1$

Recuerda que cuando se pone el $e_1$ y $e_2$ dentro de $\mathbb{R}^2$ son vectores con $2$ coordenadas y cuando se coloca el $e_1, e_2, e_3$ dentro de $\mathbb{R}^2$ son vectores de 3 coordenadas por tanto no hay que complicarse con que todo sea $e_i$, simplemente una es la base canónica de $\mathbb{R}^2$ y la otra es la base canónica de $\mathbb{R}^3$.

En nuestro caso la matriz de $f$ asociado a estas bases respectivamente, tenemos $\mathbb{R}^3_c$ en la canónica de partida y vamos a parar a $\mathbb{R}^2_c$ en la canónica de llegada y tenemos por matriz asociada a $f$ una tal matriz $A$, $\mathbb{R}^3_c\xrightarrow{f\sim A} \mathbb{R}^2_c$ y ahora estamos buscando $\mathbb{R}^3_{B_u}\xrightarrow{f\sim B} \mathbb{R}^2_{B_v}$ en este caso lo que será fácil de calcular es la matriz de cambio de base de $e_i$ a $u_i$. Desde aquí podríamos calcular la matriz $P$ que nos transforma la base canónica de $\mathbb{R}^3$ en la base $u_i$ y también podríamos calcular la matriz $Q$ que nos lleva de la base canónica de $\mathbb{R}^2$ hasta la base $v$ de R2.

$$
\begin{matrix}
\mathbb{R}^3_c & \xrightarrow{f\sim A} & \mathbb{R}^2_{B_c}\\
\uparrow{P} & & \downarrow{Q}\\
\mathbb{R}^3_{B_u} & \xrightarrow{f\sim B} & \mathbb{R}^2_{B_v} 
\end{matrix}
$$

Si queremos completar el camino de $f\sim B$ para dar toda la vuelta, sería hacer primero $P$ luego $A$ y la $Q$ la hacemos al revés nos quedaría $Q^{-1}$, $B=Q^{-1}AP$, la matriz $P$ la que transforma las coordenadas de base canónica a la base $u$ es la matriz que tiene por columnas las coordenadas de los vectores $u_i$ en coordenadas en base canónica que serían las listadas anteriormente con esas formaríamos la matriz $P$ de modo que:

$$
B=\left(\begin{matrix}
1&0&0\\
1&1&2\\
1&2&1\\
\end{matrix}\right )
$$

Esta sería la matriz de cambio de base de $e_i$ a $u_i$ en $\mathbb{R}^3$. la matriz $Q$ seria la de cambio de base de $e_i$ a $v_i$ pero en este caso en $\mathbb{R}^2$, que serian las listadas anteriormente pero solo las $v$ y las coordenadas de $v_i$ en la base canónica en columna sería:

$$
Q=\left(\begin{matrix}
2&1\\
1&0\\
\end{matrix}\right )
$$

De aquí podémos sacar la inversa

$$
Q=\left(\begin{matrix}
0&1\\
1&-2\\
\end{matrix}\right )
$$

La matriz $B$ sería el producto $B=Q^{-1}AP$ si llevamos a cabo estos cálculos nos queda la matriz

$$
Q=\left(\begin{matrix}
0&-1&1\\
3&3&0\\
\end{matrix}\right )
$$

En cualquier caso habremos podido encontrar cuál sería la expresión de la aplicación lineal $f$ en este caso en otras bases, como la base $u_i$ dentro de $\mathbb{R}^3$ y la base $v_i$ dentro de $\mathbb{R}^2$ y fijese que nos habremos ayudado de la canónica porque nosotros habíamos partido de la expresión $A$ de $f\sim A$ que esta expresión viene tanto en la base canónica de partida como en la base canónica de llegada.

Cpn esto ya sabémos como hacer la expresión matricial de una aplicación lineal y expresarla en la base del espacio de partida que nos den y en la base del espacio de llegada pertinente.

## Ejercicio calcular la matriz asociada a f en varias bases

Vamos a tomar una base de $\mathbb{R}^{3}$ formada por los vectores, $B_1=\{(1,1,0),(-1,1,2),(0,2,1)\}$ podémos comprobar que es una base calculando el determinante de estos tres vectores y que no dan cero. Consideremos esta base de $\mathbb{R}^2$ dada por los vectores $B_2=\{(1,1),(-1,1)\}$ y consideremos $f$ una aplicación lineal definida $f:\mathbb{R}^3\longrightarrow \mathbb{R}^2$ en la base canónica tal que $f(x,y,z)=(x+y-2z,x-y+z)$. La idea es obtener la matriz asociada $f$ respecto de la base $B_1$ de partida y la base $B_2$ llegada, nos preguntamos cual es la matriz $B$ asociada a la aplicación lineal $f$ donde en $\mathbb{R}^3$ de partida tomamos la base $B_1$ y en $\mathbb{R}^2$ de llegada tomamos la base $B_2$. 

$$f\sim B(f:\mathbb{R}^3_{B_1}\longrightarrow  \mathbb{R}^2_{B_2})$$.

Para hacerlo hay que hacer el diagrama, Y es que en base canónica $\mathbb{R}^3_c\xrightarrow{A} \mathbb{R}^2_c$ conocemos exactamente la matriz asociada a la aplicación lineal es la matriz $A$, donde tenemos las imágenes de los vectores de la base canónica de partida en columnas. Entonces la matriz $A$ estaría formada por:
- La imagen del $(1,0,0)$ que sería el vector $(1,1)$
- La imagen del $(0,1,0)$ en columna sería $(1,-1)$
- La imagen del vector $(0,0,1)$ el tercer vector de la base canónica en columna sería el $(-2,1)$.

$$
A=\left(\begin{matrix}
1&1&-2\\
1&-1&1\\
\end{matrix}\right )
$$

Pequeño truco como en la primera fila están los coeficientes $(x,y,z)$ en orden y en la segunda está los coeficientes de $(x,y,z)$ de la segunda componente en fila cada uno de ellos. Esto sería la matriz asociada a la aplicación lineal en la base canónica 

Pero queremos calcular la matriz asociada a la base $B_1$ de partida y a la base $B_2$ de llegada, esto será una matriz $B$, $\mathbb{R}^3_{B_1}\xrightarrow{B} \mathbb{R}^2_{B_2}$ que la podemos calcular a partir de la matriz $P$ que nos transforma la base canónica de partida en la base $B_1$ de llegada y la matriz $Q$ que transforma la base canónica $\mathbb{R}^2_{c}$ de partida en la base $B_2$ de llegada.

$$
\begin{matrix}
\mathbb{R}^3_c & \xrightarrow{A} & \mathbb{R}^2_{c}\\
\uparrow{P} & & \uparrow{Q}\\
\mathbb{R}^3_{B_1} & \xrightarrow{B} & \mathbb{R}^2_{B_2} 
\end{matrix}
$$

La matriz $B$ se podría calcular $B=Q^{-1}AP$ puesto que para hacer el camino $B$ lo hacemos en el sentido de las manecillas del reloj y la flecha de la $Q$ que esta hacia arriba, en nuestro caso habríamos que calcular las dos matrices de cambio de base.

Necesitaríamos la matriz de cambio de base $P$ que es de la canónica $B_1$ que por tanto son los vectores de la base $B_1$ expresados en la base canónica en columna.

$$
P=\left(\begin{matrix}
1&-1&0\\
1&1&2\\
0&2&1\\
\end{matrix}\right )
$$

La matriz $Q$ sería los vectores de la base $B_2$ expresados en canónica, colocados en columnas.

$$
Q=\left(\begin{matrix}
1&-1\\
1&1\\
\end{matrix}\right )
$$

Teniendo estos vectores en columna podemos calcular sin ningún problema $Q^{-1}$ que es la inversa de la matriz $Q$, que será

$$
Q^{-1}=\left(\begin{matrix}
1/2&1/2\\
-1/2&1/2\\
\end{matrix}\right )
$$

El producto de estas tres matrices $B=Q^{-1}AP$ nos va a dar la expresión matricial de la aplicación lineal $f$ en la base $B_1$ de partida y $B_2$ de llegada, que nos da:

$$
\left(\begin{matrix}
1&-2&-1/2\\
-1&2&-1/2\\
\end{matrix}\right )
$$

Fijese que en dimensiones da lo mismo porque tiene las mismas dimensiones que la matriz $A$ pero en este caso los vectores que responden al producto $B\cdot X$ tienen que venir expresados en la base $B_1$ y el resultado de multiplicar $B\cdot X$ nos da el vector del espacio $\mathbb{R}^2$ en la base $B_2$

Así sería una forma de hacerlo pero no hay que pensar que es la única porque aquí hay una parte que nos la podemos ahorrar que es la de la matriz $P$ por la matriz $A$ para acelerar el proceso, si hacemos $f$ de los tres vectores de la base $B_1$, que si calculamos las imágenes de los vectores de la base $B_1$ expresadas en la canónica de llegada que serían:
- $f(1,1,0)=(2,0)_c$
- $f(-1,1,2)=(-4,0)_c$
- $f(0,2,1)=(0,-1)_c$

Por así decir nos hemos saltado una parte porque hemos tomado los vectores de $\mathbb{R}^3_{B_1}$ nos hemos ido directamente al espacio en canónica $\mathbb{R}^2_{c}$ y ahora lo único que nos hace falta para conseguir el resultado en $\mathbb{R}^2_{B_2}$, por tanto la matriz $B$ es multiplicar por $Q^{-1}$.

$$
\begin{matrix}
& & \mathbb{R}^2_{c}\\
&\nearrow& \uparrow{Q^{-1}}\\
\mathbb{R}^3_{B_1} & \xrightarrow{B} & \mathbb{R}^2_{B_2} 
\end{matrix}
$$

Ya tenemos laa imagenes de esos tres vectores expresadas en canónica, nos hemos saltado el paso en el que transformamos la base de partida en canónica y el paso en que la canónica de partida se va a la canónica de llegada y ahora lo único que tenemos que hacer es transformar los tres vectores que están en la canónica de partida para transformarlos en la base $B_2$ de llegada. Para esto la matriz que hay que utilizar es la de cambio de base.

De modo que el vector $(2,0)_c$ se convertirá en (La matriz sacamos el factor comun $1/2$ y el resultado no sale en la base $B_2$ porque hemos multiplicado por la matriz de cambio de base):

$$
x_1=\frac{1}{2}\left(\begin{matrix}
1&1\\
-1&1\\
\end{matrix}\right )
\cdot
\left(\begin{matrix}
2\\
0\\
\end{matrix}\right )
=\left(\begin{matrix}
1\\
-1\\
\end{matrix}\right )_{B_2}
$$
El segundo vector sería la matriz de cambio de base que nos transforma canónica en $B_2$ multiplicado por el segundo vector

$$
x_1=\frac{1}{2}\left(\begin{matrix}
1&1\\
-1&1\\
\end{matrix}\right )
\cdot
\left(\begin{matrix}
-4\\
0\\
\end{matrix}\right )
=\left(\begin{matrix}
-2\\
2\\
\end{matrix}\right )_{B_2}
$$

Y por último 

$$
x_1=\frac{1}{2}\left(\begin{matrix}
1&1\\
-1&1\\
\end{matrix}\right )
\cdot
\left(\begin{matrix}
0\\
-1\\
\end{matrix}\right )
=\left(\begin{matrix}
-1/2\\
-1/2\\
\end{matrix}\right )_{B_2}
$$

Como detalle si colocamos estos tres vectores en columna nos da exactamente la matriz $B$ que se nos ha presentado anteriormente con el método rápido

$$
B=\left(\begin{matrix}
1&-2&-1/2\\
-1&2&-1/2\\
\end{matrix}\right )
$$

Si entendémos los conceptos nos podémos ahorrar pasos del algoritmo, podémos ir directamente de una base de partida a otra de llegada sin pasar por la canónica que hubieramos tenido que poner aquí intermedia.

Finalmente la fase final sería aquella que en lugar de darnos la base $B_1$ de la de aqui partida, nos dieran aquí directamente un vector concreto cualquiera en $B_1$ y nos pidieran quién sería su imagen en la base $B_2$ de llegada, $y$ en la base $B_2$ se obtiene el producto ahora ya de la matriz $B$, $y_{B_2}=B$ que transforma $\mathbb{R}^3_{B_1} \xrightarrow{B} \mathbb{R}^2_{B_2}$ por sus coordenadas en $\mathbb{R}^3_{B_1}$ que las representaremos como $x$, asi que nos queda $y_{B_2}=Bx_{B_1}$, si en algún momento nos encontramos con un ejercicio de este estilo, directamente para obtener las coordenadas de un vector concreto de cuál sería su imagen en la base $B_2$ de llegada, lo único que tenémos que hacer es este producto matricial $y_{B_2}=Bx_{B_1}$ y así obtendres el resultado.

## Determinante de un endomorfismo

**Determinante de un endomorfismo.** Sea $E$ un $\mathbb{K}$-espacio vectorial de dimensión finita $n$ y sea una aplicacion lineal definida en un espacio vectorial en si mismo $f:E\longrightarrow E$ es un endomorfismo. El determinante del endomorfismo $f$ no es otro que el determinante de la matriz $A$ 

$$\det(f) = \det(A)$$

donde $A$ es la matriz asociada a la aplicacion lineal $f$ con respecto a una base de $E$ cualquiera.

Para cualquier base que eligamos hay un endomorfismo y por tanto hay una aplicacion lineal asociada diferente, el determinante de $f$ coincide con el de la matriz $A$ en una base concentra, solo se elige una base y con esa construimos la matriz de la aplicacion lineal.

**Observación** Si $A'$ es la matriz asociada a $f$ con respecto a alguna otra base de $E$, tenemos que el utltimo resultado del colorario anterior nos decia que $A' = P^{-1}AP$ simplemente en un endomorfismo las matrices estan relacionadad atravez de sus matrices de cambio de base (cualquier matriz de la aplicacion lineal siempre es $P^{-1}AP$ donde $P$ es la matriz de cambio entre las bases elegidas) y se cumple que existira una matriz $P$ invertible de cambio de base tal que se cumple la formula, con ello.

Nos dice que el determinante de $A'$ es ese determinante $\det(P^{-1}AP)$ pero el determinante de un producto es producto de determinantes que lo habiamos demostrado asi que se separa en el determaniante de cada coeficiente y aqui vemos que el determinante de $P$ y el de $P^{-1}$ son inversos uno de otro ya que si una matriz es invertible estos desterminantes $P$ y $P^{-1}$ uno es el inverso de otro por tanto el producto nos dara $1$ y al final nos queda que es el igual al determinante de $A$

$$\det(A') = \det(P^{-1}AP) = \det(P^{-1})\det(A)\det(P) = \det(A)$$

Con lo cual, el determinante de un endomorfismo no depende de la base utilizada por eso al calcular el determinante de un endomorfismo con esta deficion podemos elegir cualquier base como la mas facil con los mas ceros pocibles.

