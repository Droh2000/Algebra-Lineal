---
title: "Espacios Vectoriales"
author: "yo"
date: "9/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Espacios vectoriales

**Espacio vectorial.** Un espacio vectorial sobre un cuerpo conmutativo $\mathbb{K}$ es un conjunto $E$ no vacío **(Debe tener elementos Afuerzas)** y cerrado con las siguientes operaciones definidas:

- Ley de composición interna
$$\forall\vec{x},\vec{y}\in E\Rightarrow \vec{x}+\vec{y}\in E$$, dados dos elementos del espacio $E$ para todos los elementos $\vec{x},\vec{y}$, sumando elemntos del espacio deben salir elementos de dicho espacio, **Sumar vectores salen vectores** 
- Ley de composición externa$$\forall\vec{x}\in E,\alpha\in\mathbb{K}\Rightarrow\alpha\vec{x}\in E$$, dado un elemento del espacio y un escalar del cuerpo $\mathbb{K}$ sobre el que esta montado el espacio, el producto del escalar por el elemnto del espacio debe pertenecer al espacio, **El producto escalar de un cuerpo por un vector nos da un vector**

que cumplen las siguientes condiciones

**Ojo: **Un vector es un elemnto que vive en un espacio vectorial (Sean Matrices, Vectores o Funciones)

**Condiciones de la ley de composición interna.**

- Propiedad conmutativa: $$\vec{x}+\vec{y} = \vec{y}+\vec{x}\quad \forall\vec{x},\vec{y}\in E$$
- Propiedad asociativa (Mover parentesis): $$\vec{x}+(\vec{y}+\vec{z}) = (\vec{x}+\vec{y})+\vec{z}\quad \forall\vec{x},\vec{y},\vec{z}\in E$$
- Elemento neutro de la suma: $\exists\vec{0}\in E:\ \vec{x}+\vec{0}= \vec{x}\quad \forall\vec{x}\in E$, El espacio vectorial debe contener un vector 0 Que sumado con cualquier vector da ese mismo vector dentro del espacio vectorial
- Existencia del opuesto: $\forall\vec{x}\in E,\ \exists-\vec{x}\in E:\ \vec{x}+(-\vec{x}) = (-\vec{x})+\vec{x} = \vec{0}$, existe otro vector perteneciente al espacio y asi sumandolo nos tiene que dar el vector 0 **Fijarse que este no es el numero 0. Es el Vector 0 por eso tiene una flecha Arriba**

**Condiciones de la ley de composición externa.**

- Propiedad asociativa: $$\alpha(\beta\vec{x}) = (\alpha\beta)\vec{x}\quad \forall\vec{x}\in E,\alpha,\beta\in\mathbb{K}$$, Multiplicar Esclares de diferetes formas por el vector nos dan el mismo resultado
- Elemento neutro del producto: $\exists1\in\mathbb{K}:\ 1\vec{x} = \vec{x}\quad \forall\vec{x}\in E$, tipicamente lo llamamos 1 pero puede ser cualquiera en distintos casos, Un numero del cuerpo multiplicado por cualquier vector nos tiene que dar ese vector
- Propiedad distributiva del producto respecto de la suma de vectores (Quitr parentecis):$$\alpha(\vec{x}+\vec{y}) = \alpha\vec{x}+\alpha\vec{y}\quad \forall\vec{x},\vec{y}\in E,\alpha\in\mathbb{K}$$
- Propiedad distributiva del producto respecto de la suma de escalares:$$(\alpha+\beta)\vec{x} = \alpha\vec{x}+\beta\vec{x}\quad \forall\vec{x}\in E,\alpha,\beta\in\mathbb{K}$$

**Vectores.** Nombre que reciben los elementos de $E$ que viva dento del espacio vectorial a pesar de no ser vectores en el sentido clasico de numeros en filas (Matrices y Funciones pueden formar esapcios vectorail)

**Escalares.** Nombre que reciben los elementos de $\mathbb{K}$ del cuerpo

En este caso un espaciovectorail estara formado por vectores y utilizaremos como operacion el producto por escalares Uelemntos del cuerpo)

**Observación.** En la definición anterior y en las propiedades aparecen dos sumas diferentes que denotamos del mismo modo por `+`. La suma de los elementos de $E$ (la suma de vectores) y la suma de los elementos de $\mathbb{K}$ (la suma de escalares). Del mismo modo, los elementos neutros de ambas sumas también los denotamos iguales, por 0 **(Esta el 0 del escalar y el 0 del espacio)**, aunque sean diferentes (uno es un vector y, el otro, es un escalar). Con las **Condiciones** que se explicaron arriba como en el caso del (Vector 0, Que es un vector **NO** un numero) vimos como serepresentaba al ser vector con una flecha arriba y si no tiene espor que es un escalar, ala hora de la practica no es necesario represantar asi. El contexto nos dirá en cada momento a qué suma y a qué elemento neutro nos estamos refiriendo **(Al del Cuerpo o Al del Espacio Vectorial)**, si hay confuncion poner una flecha todo lo que sea parte del espacio vectorial. Lo del cuerpo (Numeros) tipicamente seusaran letras Griegas como $\alpha$ o letras minusculas mientras que los de espacio vectorial se usaran como $x$ u otrasletras como en las ecuaciones.

Ocurre lo mismo con el producto. En caso de que pueda haber confusión, no se denotará ningún símbolo a la hora de referirnos a un producto de escalares, mientras que el producto de un escalar por un vector lo denotaremos por $\alpha\cdot \vec{x}$. Aunque no siempre seremos capaces de mantener esa notación, del mimso modo que denotaremos indistintamente como vectores $x$ o $\vec{x}$

## Ejemplos de espacios Vectoriales

Lo que hay que comprobar si es un espacio vectorial es verificando si si cumplen las traas propiedades (Cerrado, Suma de vectores nos da vectores, el producto por un elemento del cuerpo nos da un vector mas las otras)

**Ejemplo 1**

A continuación se muestran ejemplos de espacios vectoriales, (El mas sencillo es el de los vectores)

- $\mathbb{R}^n$ formado por los vectores de $n$ componentes $(x_1,x_2,\dots,x_n)$ los elemntos que viven en $\mathbb{R}^n$ estan represantos es en $x_n$ en el que cada $x$ seria un numero real, **Verificando las propiedades: ** es no vacio el conjunto porque podemos definir un vector que se nos de la gana y cabe dentro de este espacio vectorial y si multiplicamos un escalar por un vector de n componentes el resultado es un vector de n componentes, El Cuerpo $k$ sobre el que se monta la mayoria que mas usamos es el numeros reales
- El conjunto $P_n(\mathbb{K}) = \{a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0:\ a_i\in\mathbb{K},\ \forall0\le i\le n\}$, son los polinomios a coeficientes en K de grado menor o igual que $n$, **¿Existe un polinimos de este grado?**, Si el numero 0 cumple esta condicion (Si es mas facil en lugar de pensar en $n$ pensemos en un polinomio de grado menor o igual a 2, Vemos los diferentes connjuntos a formar), Si los **Sumamos** los polinomios se agrupan dependiendo del grado del monomio y si los **Multiplicamos** como solo se multiplica el coeficiente el exponente no se modifica
- El espacio $\mathcal{M}_2(\mathbb{K})$ de las matrices de orden 2 con coeficientes sobre $\mathbb{K}$
- El conjunto de las funciones continuas definidas sobre un cuerpo $\mathbb{K}$, estas funciones son las que podemos dibujar sin levantar el lapiz del papel, Ejemplo: la Funcion: $0$, totalmente horizontal es continua, La suma de funciones continua el resultado nos da una funcion continua, sila multiplicamos por un numero seguira ciendo una funcioon continua

Todo lo Anterior son Espacios Vectoriales:

**Ejemplo 2**

No son espacios vectoriales:

- El conjunto de matrices $\mathcal{M}_{m\times n}(\mathbb{Z})$
- El conjunto de polinomios de grado exactamente igual a 3 con coeficientes reales

**Ejercicio 1**

¿Por qué los conjuntos anteriores no son espacios vectoriales?

**Proposición.** Sea $E$ un $\mathbb{K}$-e.v.(espacio vectorial), entonces el neutro de la suma y el opuesto de un elemento cualquiera $x\in E$ son únicos.

**Demostración**

Para ver la unicidad del elemento neutro, supongamos que Existen dos vectores Neutros Diferentes $0_1$ y $0_2$ son dos neutros del espacio vectorial $E$. Entonces, $$0_1 = 0_1 + 0_2 = 0_2$$, Cumplen esto.

Con lo cual, $0_1 = 0_2$ tal y como queríamos ver. **El Neutro de Un Espacio Vectorial Es Unico**

Ahora, para ver la unicidad del elemento opuesto, supongamos que $x\in E$ tiene dos elementos opuestos: $y,z$. Entonces viendo propiedades del espacio vectorial, tendríamos $$y = y+0 = y + (x+z) = (y+x)+z = 0+z = z$$, Este Desmadre anterior nos dice que $y$ se puede poner como $y+0$ por que el $0$ es el Neutro de la suma, pero el $0$ se puede escribir como $(x+z)$ porque $z$ es el opuesto de $x$ (Recordemos que el opuesto es aquel tal que sumado con el original nos da el neutro) ya que esta suma $x+z$ nos da $0$, despues del $=$ podemos aplicar la propiedad asociativa y juntar $y+x$ y dejar la $z$ suelta pero esta suma de $y+x$ nos da cero porque $y$ tambein es un opesto de $x$

Con lo cual, $y = z$ tal y como queríamos demostrar si hubieran dos opestos de un vector $x$ estos dosopestos serian iguales. Por tanto EL **Neutro** y **Opuesto** son Unicos

**Proposición.** De la definición de $\mathbb{K}$-e.v. se deducen las siguientes propiedades, Todos los Elemntos despues del Igual llevarian flecha arriba por que son elementos del espacio vectorial entonses son Vectores:

- $0\cdot x = 0$, Nota: El 0 que multiplica es un escalar, mientras que el 0 de resultado es vecto
- $\lambda\cdot 0 = 0$, Aqui ahora el 0 es un vector y el lambda es un escalar
- Si $\lambda\cdot x = 0$, entonces uno de los dos $x = 0$ o $\lambda = 0$
- $(-\lambda)\cdot x = -(\lambda\cdot x) = \lambda\cdot (-x)$. En particular, $(-1)\cdot x = -x$, una consecuencia de esta propiedad el opesto de un elemento es multiplicar el propio vector por $-1$
- $\lambda\cdot (x-y) = \lambda\cdot x -\lambda \cdot y$
- $(\lambda-\mu)\cdot x = \lambda \cdot x-\mu\cdot x$

**Ejercicio 2.** Demostrar estas 6 propiedades formalmente.

## Subespacios vectoriales

**Subespacio vectorial.** Sea $F$ un subconjunto $F\subseteq E$ un subconjunto no vacío del espacio vectorial $E$ sobre un cuerpo $\mathbb{K}$. Diremos que el pequeño que es el que esta dentro de $F$ es un subespacio vectorial de $E$ si, y solo si, se verifica

- La suma de dos elementos de $F$ es otro elemento de $F$: $$\forall\vec{x},\vec{y}\in F\Rightarrow\vec{x}+\vec{y}\in F$$, Por tanto la suma de elemnentos del subespacio pertenece al propio subespacio
- El producto de un escalar por un elemento del subespacio $F$ es otro elemento de $F$: $$\forall\vec{x}\in F,\ \alpha\in\mathbb{K}\Rightarrow\alpha\vec{x}\in F$$

Por asi decir es un espacio vectorial dentro de un espacio vectorial, tiene que quedar todo cerrado dentro del subespacio al **sumar subvectoresdel subespacio** tienen que dar un subvector del subespacio, al **Multiplicar un vector** del subespacio por un escalar del cuerpo tiene que pertenecer a tambbien al subespacio
 
Cualquier espacio vectorial que se nos pase por la cabeza simpre tiene 2:  **Subespacios triviales.** Si $E$ es un $\mathbb{K}$-espacio vectorial, se verifica siempre que **Todo el conjunto es subespacio de si mismo** $E$ y degradado unicamente al conjunto $\{0\}$ que solo tiene el neutro son subespacios vectoriales de $E$. Estos se denominan subespacios vectoriales triviales de $E$ o impropios del espacio original. Estos son los dos subespacios vectoriales (el total y el cero).

De las diapositivas anteriores, se deduce fácilmente que,

**Proposición.** Si $F$ es un subespacio vectorial de $E$, entonces 

- $\vec{0}\in F$, Tiene ue contener al 0, Siempre cualquier subespacio vectorial tiene que tener al vector 0 del espacio si no lo tiene, **NO** es subespacio, no se considera un subespacio vectorial
- Si $\vec{x}\in F$, entonces $-\vec{x}\in F$, para cualquier elemntos del subespacio (para cualquier vector x de $F$ el -x tambien tiene que pertenecer al subespacio)

**Ejercicio 3.** Demostrar formalmente esta proposición.

Una gran utilidad de esta proposición es que si se comprueba que $\vec{0}\not\in F$, entonces este conjunto no puede ser nunca un espacio vectorial.

**Proposición.** Sea $E$ un $\mathbb{K}$-e.v. y $F$ un subconjunto no vacío, entonces son equivalentes las siguientes afirmaciones:

- $F$ es un subespacio vectorial (No vacio, cerrado para sumas y productos con escalares)
- $F$ es un $\mathbb{K}$-espacio vectorial con las mismas operaciones de $E$ restringidas a $F$
- $F$ verifica $ax+by\in F\quad\forall a,b\in\mathbb{K}$ y $\forall x,y\in F$, esta propiedad nos junta las dos propiedades del los subespacios junta el hecho de que la suma pertenesca a $F$ y  que el produco por un escalar pertenesca a $F$ y esta forma es la mas comoda de mesotraar ala hora de demostrr que pertenesca aun subespacio
- Cualquier combinación lineal de vectores de $F$ es un vector de $F$, es decir, $\sum a_ix_i\in F\quad\forall a_i\in\mathbb{K}$ y $\forall x_i\in F$, no solo vale para $ax+by$ si no que es para toda las $a_n$ $x_n$ simempre tiene que pertenecer a $F$ para culaquier conjunto de numeros

**Ejercicio 4.** Demostrar formalmente esta proposición.

**Proposición.** Sea $E$ un $\mathbb{K}$-espacio vectorial

- Si $(F_i)_{i\in I}$, Fijarse que la interseccion es arbitaria en la parte $i\in I$ $i$ perteneciente a una $I$ es una familia cualquiera de subespacios vectoriales de $E$ (Podria ser Finita, Numerable o Infinita). La interseccion de todos ellos los vectores que pertenecen ala vez a todos los subespacios forman un subespacio vectorial de $E$, entonces $\bigcap_{i\in I}F_i$ es un subespacio vectorial de $E$ contenido en todos los $F_i$ con $i\in I$, asi que si nos dan dos subespacios la interseccion (Los elemnotsque estan ala vez en un subespacio y en otro tambien es un subespacio vectorial), Es el subespacio vectorial mas **grande** metido dentro de todos los $f_i$  
- Si $F_1,\dots,F_n$ son subespacios vectoriales de $E$, entonces la suma de los subespacios $F_n$ que se define como sumar un vector de cada subespacio $x_n$, La suma solo es valida para conjuntos Finitos

$$\sum_{i = 1}^nF_i = F_i+\cdots+F_n = \{x_1+\cdots+x_n\ |\ x_i\in F_i,\ i = 1,\dots, n\}$$

es un subespacio vectorial de $E$ llamado **subespacio vectorial suma** que contiene todos los $F_i$ con $i = 1,\dots, n$, tiene la propiedad de ser el mas **pequeño** subespacio que contiene a todos los $F_i$

**Ejercicio 5.** Demostrar formalmente esta proposición.

Lo que nos dice la proposición anterior, en otras palabras, es que la intersección **infinita** (Arbitaria) de subespacios vectoriales es a su vez subespacio vectorial.

No obstante, la unión **No lo es** (finita o arbitraria) de subespacios vectoriales no es subespacio vectorial. Si nosotros unimos los vectores de un conjunto con los de otro podria ser que la union de esos dos conjuntos no sea un subespacio vectorial

Por su parte, una suma **finita** desde $1$ hasta $n$ de subespacios vectoriales sí es subespacio vectorial y sus elementos son de la forma descrita anteriormente. es decir un vector pertenece ala suma de $F_i$ si se puede escribir como lasuma de un Vector de cada una de las $F_n$

**Ejemplo 3**

En el $\mathbb{R}$ espacio vectorial $\mathbb{R}^2$, consideremos los subespacios vectoriales $F,G$ dados por los ejes de coordenadas cartesianas. Así pues, $F$ es ese subespacio vectorial de puntos donde $x$ es numero real, esto sobre el plano (Espacio vectorial R2) nos define el eje de la $x$, analizando la situacion donde si tomamos dos vectores sobre el eje de las $x$ y los sumamos siguen estando en ele eje de las $x$ y si cojemos un vector en el eje de las $x$ y lo multiplicamos por un escalar sigu estando en el eje de las $x$ por lo tanto ya sabemos que $F$ es un subespacio vectorial de $\mathbb{R}^2$, en el caso de $G$ seria el mismo rollo

$$F = \{(x,0)\ |\ x\in\mathbb{R}\}\qquad G = \{(0,y)\ |\ y\in\mathbb{R}\}$$

Entonces, es fácil ver cual es la interseccion de cada uno de sus ejes $F\cap G =\{(0,0)\}$ este es el unico punto que es ala vez sobre el eje de la $x$ y sobre el eje de l $y$ es este por lo tanto el subespacio interseccion es el subespacio trivial y que $F+G = \mathbb{R}^2$ en efecto si tomamos un elemnto de $F$ y lo sumamos con un elemnto de $G$ basicamente lo que sale es cualquier punto de $\mathbb{R}^2$, que son efectivamente subespacios vectoriales (de hecho son los impropios).

En cambio, si hacemos la unióin, tenemos $F\cup G = \{(x,y)\in\mathbb{R}^2\ |\ x=0\text{ o }y = 0\}$ serian los puntos que estan sobre el eje de las $x$ o sobre el eje de ls $y$, pero esto no es subespacio vectorial de $\mathbb{R}^2$, ya que tomando los elementos que estos ambos vectores pertenecen a cada uno de la union $(1,0), (0,1)\in F\cup G$ estos subvectores cumplen que estan en cadauno de los ejes,Sin emborga por ser subespacios deberia ocurrir que su suma deberia de pertenecer a ese conjunto sin embargo su suma nos da, $(1,1)\not\in F\cup G$ por esa la union de subespacios vectoriales no suele ser en general un subespacio vectorial

## Ejemplo de Espacios Vectoriales

Si $E$ es un $\mathbb{K}$-e.v. (Un espacio vectorial sobre un cuerpo $\mathbb{K}$). Siempre existen dos subespacios vectoriales de $E$: 
- El subespacio vectorial que solo tiene el vector {0} **Subespacio Trivial** 
- El subespacio vecotrial que son todos los vectores del espacio vectorial $E$ **Subespacio Total**

Los dos se llaman **subespacios impropios** porque son triviales, cualquier otro subespacio vecotrial que no sean estos dos sera **Subespacio Propio**. Por Ejemplo:

Dentro de $\mathbb{R}^3$ (El conjunto de vectores con 3 coordenadas). Si tomamos el conjunto $F = \{(x,y,z)\in\mathbb{R}^3\ x+y+z=0\}$, esto es un subespacio vectorial de $\mathbb{R}^3$, en general cualquier conjunto de soluciones de un sistema de $M$ ecuaciones lineales con $n$ incognitas que sea homogenio **(Que este igualado a 0 sobre un cuerpo $\mathbb{K}$)** sera el subespacio vectorial del espacio vectorial $\mathbb{K}^n$ donde la $n$ sera igual al numero de incognitas, Dentro de $\mathbb{R}^3$ seria un plano que pasa por el origen, con este plano si tomamos vectores que cumplan esta condicion, Ejemplo: Tomamos el vector $(x_1,y_1,z_1)\in F$ *Como estos pertenecen a F* Se cumple que $x_1+y_1+z_1=0$ y $(x_2,y_2,z_2)\in F$ igual para este vector $x_2+y_2+z_2=0$ por ser de $F$. Tomamos los escalares $\alpha,\beta \in \mathbb{K}$ cuando hacemos la combinacion lineal con estos: **Con la suma de vectores** $\alpha(x_1+y_1+z_1)+\beta(x_2,y_2,z_2) =$ y el resutado **¿Pertenece a F?** $(\alpha{x_1}+\beta{x_2}+\alpha{y_1}+\beta{y_2}+\alpha{z_1}+\beta{z_2}\in F$, En caso de pertenecer cumpliria la ecuacion igual a 0, Si lo ponemos como coordenadas: $(\alpha{x_1}+\beta{x_2})+(\alpha{y_1}+\beta{y_2})+(\alpha{z_1}+\beta{z_2})$ fijaros si lo reodenamos podemos juntar u ordenar asi: $\alpha(x_1+y_1+z_1)+\beta(x_2,y_2,z_2)$, por la propiedad inicial $x+y+z=0$, las partes quedan en $\alpha(x_1+y_1+z_1)=0+\beta(x_2,y_2,z_2)=0$ y $\alpha$ por 0 mas $\beta$ por 0, nos da 0 y como se cumple la ecuacion igual a cero conluimos que la combinacion lineal $\in F$ y por lo tanto se trta de un subespacio vectorial, **Si tenemos un sistema de ecuaciones homogeneo sobre un cuerpo K este siempre sera subespacio vectorial tiene que estar igualado a 0**

Por ejemplo: En $\mathbb{R}^2$ cualquier recta que pase por el origen sera un subespacio vectorial propio de este

Pero si nos dan esta ecuacion:

$$G = \{(x,y,z,t)\in\mathbb{R}^4:\ x+y=0,z=0,t=1\}$$, Nunca odre ser un subespacio vectorial porque no es un subsistema de ecuaciones homogeneo o en otras palabras porque no pasa por el origen, recordar una conidicion para que unconjunto sea subespacio vectorial es que el vector cero pertenesca al conjunto,en este caso el vector cero seria $\vec{0}=(0,0,0,0)$ por cada letra como en este caso $t=1$ por lo tanto el vector cero no pertenence.

Podemos elegir el espcio vectorial $\mathbb{R}^\mathbb{R}$ que basicamente es el conjunto de funciones definidas con dominio en $\mathbb{R}$ y variable real hacia numero reales, el conjunto de todas las funciones que existen serian un espacio vectorial, aqui lo que viven son funciones de una sola variable, dentro de este $\mathbb{R}^\mathbb{R} = \{F:\mathbb{R}-\mathbb{R}\}$, podemos tomar dos subespacios vectoriales:
- Tenemos el de las **funciones continuas** de $\mathbb{R}$ en $\mathbb{R}$, basicamente son las funciones que podemos dibujar sin usar un lapiz, el cero esuna funcion continua, la suma de continuas es continua y una continua por un escalar es continua
- Es el conjunto de **funciones Derivables** estas no tienen pinchitos, el valor absoluto no es deribavle por eso, con pinchitos nos referimos que al dibujar la funcion en el plano vemo que tiene una forma como letra $V$ con ese pico o pincho, la suma de funciones derivlables es deribale o el producto de una funcion deriblae por un escalar tambein es derivable

Mas ejemplo de Subespacios vectoriales como elegir sobre el conjunto de los polinomios hasta cierto grado en lo que su derivada es 0 o cumplan que la primera componente y la ultima sea la misma, la suma de los coeficientes de los polinimos de cero,**Todo lo que tenga ver con 0 esta bien**, pero si no involucra a este no es subespacio vectorial.

# Demostracion de los Ejercicios 3,4 y 5

**Ejercicio 3: **

Si $F$ es un subespacio vectorial siempre se cumple que el vector 0 pertenece a $F$ $\vec{0}\in F$ y si $\vec{x}\in F$ entonces el Opuesto $\vec{-x}\in F$ para cualquier valor X del subespacio $F$

Al ser $F$ un espacio vectorial una de las condiciones es que no sea vacio $F\not=0$ entonces tiene cualquier elemento $\vec{x}\in F$, La segunda condicion de subespacio decia que si: $\lambda\vec{x}\in F\ \forall\lambda\in\mathbb{K}$, esto nos dice que podemos tomar:
 $\lambda=0\Rightarrow0\cdot{\vec{x}}=\vec{0}\in F$, con esto se demuestra que el elemnto $0$ siempre pertenece al subespacio vectorial $F$, de otra forma si es: $\vec{x}\in F$ usando la misma propiedad podriamos escoger $\lambda=-1$ que tambien es del cuerpo y lo que pasa es que el vector $-1\cdot\vec{x}$ nos da el Opuesto de $x$, $\vec{-x}\in F$
 
**Ejercicio 4: **Habira que demostrar:

- 1.- $F$ es un subespacio vectorial
- 2.- $F$ es un $\mathbb{K}_{ev}$ con las misma operaciones Restringidas (Teniniendo en cuenta solo los elementos del connjunto $F$ con las mismas operaciones de $e$ restringidas a $F$)
- 3.- $ax+by\in F\ \forall_{xy}\in F, a,b \in \mathbb{K}$ sobre el que este construido el espacio vectorial
- 4.- Cualquier combinacion lineal $a_ix_i\in F\ \forall_{a_i}\in\mathbb{K}, x_i\in F$

Cuando en matematicas queremos demostrar tantas equivalencias lo mas normal es crear un circulo, Demostras que el 1 implica a 2 y el 2 a 3 y el 3 a 4 y el 4 al 1, Ahora tratamos todas estas implicaciones:

Que 1 implica a 2: Si $F$ es un subespacio vectorial las operaciones de $e$ definene las mismas operaciones de $F$, las sumas y productos son de $F$, como en el ejercicio 4 hemos garantizado que el 0 y los opuestos son de $F$

Que 2 implica a 3: Es trivial apartir de la definicion $\mathbb{K}$ espacio vectorial, en particular en un espacio vectorial se cumplia la ecuacion: $ax+by\in F$ solo que aqui se ha restringido al subespacio $F$, como en el punto 2 me aseguraba que $F$ es tambien un espacio vectorial, el punto 3 es tirvial es por la definion de que $F$ es a su vez un espacio vectorial y sabemos que en un espacio vectorial se verifica que $ax+by\in F$

Que 3 implique 4: Se puede realizar por induccion sobre $n$ en el numero de vectores de la combinacion lineal, para $n=2$ para suma de dos elemntos es la hipotesis numero 3 que ya la tenemos porque suponemos que es cierta, Si la suponemos sierta hasta $n-1$ esto significa que el $\sum{a_ix_i}$ desde $i=1$ hasta $n-1$ pertenece a $F$ para cualquier $a_i$ del cuerpo y $x_i$. y tambien demostrar que en lugar de escoger $n-1$, elegieramos $m$: $\sum_{i=1}^{m}a_ix_i$ que tambien es de $F$ y esta cosa se puede separr en dos: $(\sum_{i=1}^{n-1}a_ix_i)+a_mx_m$, fijese que el que sta encerrado en parentesis pertence a $F$ por hipotesis de induccion y $+a_mx_m$ tambien pertence a $F$ porque es un escalar por un elemnto de $F$, entonces la suma de dos elemntos de $F$ al final es de $F$ porque $F$ es un subespacio vectorial

Que la 4 implica 1: Si tomamos la propiedad 4 como la podemos aplicar a cualquier numero de elemntos, si la ponemos como: $\sum_{i=1}^{m}a_ix_i\in F\ \forall_{a_i}\in\mathbb{K}, x_i\in F$ tomamos esta propiedad y la aplicamos para $n=2$ (2 vectores) y tomar en el primer caso $x_1$ y $x_2$ dos elemntos cualesquiera y $\alpha_1$ y $\alpha_2$ los escalares igual a $1$ para obtener las $x+i$ cualquier $x$ mas cualquier $i$ pertenecerian a $F$ y en segundo lugar cualuier $\alpha$ multiplicado con cualquier $x$ con solo $n=1$, para demostrar que $F$ es un subespacio vectorial se tiene que $x+i$ pertenece a $F$ para todo $X$ e $i$ de $F$, tomamos $\alpha_1=1,\alpha_2=1$ e $x_i$ que es la propiedad 4 y por otro lado que $\lambda x$ siempre es de $F$ tomamos esa misma propiedad $n=1$ y tomamos un escalar y vector cualquiera obtener que la suma de vectores es de $F$ y el producto de un escalar por $F$ tambien y asi se completa la demostracion en circulo 

**Ejercicio 5: ** es la generalizacion de la interseccion y la suma de subespacios

- La interseccion arbitaria de cualquier numeros de subespacios es un espacio vectorial: $\displaystyle\bigcap_{i\in I}{F_i}$ de $F$ es un subespacio vectorial, si $F_1$ es una parte cualquiera de espacios vectoriales de $E$, la seccion de todos ellos es un subespacio vectorial de $E$ que ademas esta contenido dentro de todos los $F_i$ estes es un subespacio vectorial mas pequeño contenido dentro de todos los $F_i$ que forman parte de la interseccion y la segunda parte decia parecido con la suma $\displaystyle\sum_{i=1}^n F_i$ es un subespacio vectorial de $E$ que en este caso contienen a todos los $F_i$ por tanto dada una coleccion arbitaria la interseccion de todos los subespacios vectoriales es el subespacio vectorial mas grande contenido en el interior de $F_i$ y por el contrario la suma de una coleccion finita de subespacio es el subespacio vectorial mas pequeño que los continene todos ellos.

Para demostrarla iremos primero ala interseccion hay que demostrar que es un suubespacio, La Interseccion es arbitario (No es vacio) $\displaystyle\bigcap_{i\in I}{F_i}\not=0$ porque no es vacio ya que 0 es un elemnto que pertenece a todos los $F_i$: $\vec{0}\in F_i\ \forall_i\in I$, si cero pertenece a todos entonces cero pertenece ala Interseccion de los $F_i$ que no es vacio, Primera condicion de Subespacio cumplida, Segundo tomemos $a,b\in \mathbb{K}$ dos escalares del cuerpo K y tomemos $x,y$ que son dos vectores que pertenescan ala interseccion arbitaria de los espacios $F_i$: $x,y\in \displaystyle\bigcap_{i\in I}{F_i}$ entonces tanto $x$ como $Y$ pertenecen a todos los $F_i$ para todo $i$ de la coleccion arbitaria y como a su vez cada $F_i$ es un subespacio vectorial por separado de $E$ la combinacion lineal $ax+by \in F_i\ \forall_i\in I$ entoncese $ax+by\in \displaystyle\bigcap_{i\in I}{F_i}$

Con respecto ala Suma supongamos que tengamos: $Z,Z^{'}$ dos vectores de la suma de los subespacios $F_1+F_2+...+F_n$ esto quiere decir que existen vectores de cada uno de los subespacios tales que $Z$ se puede poner como suma de cada uno de ellos y $Z^{'}$ tambien por tanto existirian vectores $x_i$ de cada uno de los $F_i$ tales que $Z$ se puede escribir como $Z=x_1+x_2+...+x_n$ cada una de estas $x$ pertenece a su respectivo subespacio $F_n$, $n$ segun el numero y lo mismo existiran vectores $Y_i$ que pertencen a $F_i$ tal que el vector $Z^{'}=Y_1+Y_2+...+Y_n$ por tanto si hacemos una combinacion lineal de todos estos (Como ya sabes cual serian los valores de $Z y Z^{'}$): $aZ+bZ^{'}=(ax_1+by_1)+(ax_2+by_2)+...+(ax_n+by_n)$, el primer grupo $(ax_1+by_1)$ pertenece a $F_1$ por que este es un espacio vectorial y estamos haciendo combinaciones lineales de ese espacio y el otro gurpo pertenece a $F_2$ y el otro $F_n$ por tanto la suma pertenece a $F_1+F_2+...+F_n$, ademas cada $F_i$ esta contenido dentro del subespacio suma ya que si tomamos un elemento $X_i$ de algun $F_i$ cualquiera este tambien pertenece ala suma porque lo podemos escribir 0+0+0+... porque este pertenece a cada uno de su subespacios a $x_i=0+0+0+...+0+x_i+0+...+0$ en la cordenada e-nesima podriamos $x_i$ y luego seriamos llenando con 0 ya que por propia definicion los 0 pertenecen a cualquiera de los $F_i$ entonces cualquier vector de los $F_i$ se puede escribir como perteneciente al subespacio vectorial suma $F_1+F_2+...+F_i+...+F_n$ y esta es la demostracion de que la suma tambien es un subespacio

En un subespacio vectorial sum $F+G$, En ese nuevo subespacio la expresión de cada elemento como suma de un elemento de $F$ más un elemento de $G$ no tiene por qué ser única y, por lo general, no lo es. Podria ser que para obtener un elemnto de ese sub espacio suma se tendria de varias formas distintas

En este sentido, podemos dar las siguientes definiciones: (Clasificar los subespacio suma)

**Suma directa.** Sean $E$ un $\mathbb{K}$-e.v. y $F,G$ subespacios vectoriales de $E$. Entonces, si cada elemento del subespacio vectorial suma $F+G$ se escribe de manera única como suma de un elemento de $F$ más un elemento de $G$ (Solo se puede escriir de una unica forma), se dice que la suma de $F$ y $G$ es directa y se denota por $F\oplus G$ y tiene la particualaridad que cualuqier elemento de $F$ o $G$ se puede escribir de forma unica como un elemnto de $F$ mas un elemnto de $G$ 

**Complementarios en $E$.** Si además de tener $F\oplus G$ se verifica que $E$ es el total, el espacio donde viven $F$ y $G$: $E = F\oplus G$, se dice que $F$ y $G$ son complementarios en $E$.

**Proposición.** Sean $F$ y $G$ dos subespacios vectoriales de un $\mathbb{K}$-e.v. $E$. Entonces, la suma de $F$ y $G$ es directa (O lo que es lo mismo cad vector de $F$ mas $G$ se puede escribir de forma unica entonces el unico vector que vive ala vez dentro de $F$ y dentro $G$ es el vector 0) si, y solo si, $F\cap G = \{0\}$

**Ejercicio 6.** Demostrar formalmente esta proposición.

Ademas una consecuencia de la proposicion anterior

**Corolario.** Sean $E$ un $\mathbb{K}$-e.v. y $F,G$ subespacios vectoriales de $E$. Los subespacios $F,G$ serán complementarios si verifican

- $\forall x\in E$, $\exists y\in F,\ z\in G\ :\ x = y+z$, Para cualquier vector $x$ del espacio vectorial $E$ existen un $y$ de $F$ y un $z$ de $G$ de modo que $x$ se puede escribir como $y+z$ asi solo es reformular que $F$ es suma directa con $G$, cualquier vector de $E$ se puede escribir de forma unica como suma de un elemento de $y$ de $F$ y un elemento $z$ de $G$ 
- $F\cap G = \{0\}$, ademas la interseccion entre estos es el Vacio ya que sean complementarios hace que la unica ineterseccion sea el vacio 

Recordemos el `Ejemplo 3`: 

En el $\mathbb{R}$-e.v $\mathbb{R}^2$ (Este es un espacio vectorial) habíamos considerado, tenemos el subespacio $F$ (Un Subespacio es el conjunto de puntos que viven sobre el eje X es decir de la forma (X,0) donde X es un numero Real)

$$F = \{(x,0)\ |\ x\in\mathbb{R}\}\qquad G = \{(0,y)\ |\ y\in\mathbb{R}\}$$

y habíamos visto que $F+G = \mathbb{R}^2$ y que $F\cap G = \{0\}$. Aqui tenemos la propia definicion que $F$ es complementario a $G$ o que la suma directa de estos dos nos da $\mathbb{R}^2$, Volviendo al corolario cualquie punto de $\mathbb{R}^2$ se puede escribir como suma de un vector sobre el eje de las $x$ mas un vector sobre el eje de las $y$ y ese $G_x$ y $G_y$ tiene como interseccion el vector 0 punto (0,0)

Con lo cual, tenemos que son suma directa y complementarios $F\oplus G = \mathbb{R}^2$

El concepto de suma directa lo podemos generalizar a $n$ sumandos del siguiente modo:

**Suma directa.** Sean $E$ un $\mathbb{K}$-e.v. y $F_1,\dots,F_n$ subespacios vectoriales de $E$. Entonces, diremos que la suma de esos subespacios $F_1+\cdots +F_n$ es directa si cada elemento de $F_1+\cdots +F_n$ se escribe de manera única como suma de elementos de cada uno de ellos $F_1,\dots,F_n$. Se denota por $F_1\oplus\cdots\oplus F_n$. La suma de subespacios simplemente es tomar un vector de caada uno de los subespacios y sumarlos, y la suma directa ademas implica que ese vector que resulta de la suma se opuede obtener de forma unica como suma de un vector de cada uno de los subespacios

**Ejercicio 7**

Se puede demostrar de forma parecida al caso $n = 2$, que la suma $F_1+\cdots +F_n$ es directa si, y solo si, para todo $i = 2,\dots,n$ se tiene $$F_i\cap(F_1+\cdots+F_{i-1}) = \{0\}$$

En ocasiones disponemos de un subconjunto $S$ de $E$ que no es subespacio vectorial, pero estamos interesados en el más pequeño subespacio vectorial (con respecto a la inclusión) que contiene este subconjunto $S$.

Este subespacios siempre existe ya que solo debemos considerar la familia de todos los subespacios vectoriales de $E$ que contienen a $S$ y entonces sabemos que su intersección es otro subespacio vectorial que, evidentemente, contiene a $S$ y este será el más pequeño con la propiedad.

## Ejercicios 6 y 7

**Ejercicio 6:**

Si $F$,$G$ son subespacios vectoriales de $F$ entonces $F\oplus G$ si solo si $F\cap G=\{\vec{0}\}$, es el elemnto neutro (Cuando intersecamos estas dos el unico elemento que existe en los dos subespacios a la vez es el vector 0)

Para demostrarlo hay que demostrar dos implicaciones, de Izquierda a derecha y de derecha a Izquierda:

De izquierda a derecha: Supongamos que $F\oplus G$ y consideremos $x\in F\cap G$, si $x$ pertenece ala interseccion de los dos se puede escribir como la suma directa por un lado: $x=x+0$ ya que $x \in F$ y tambien se podria poner como: $x=0+x$ pero esta perteneceria a $x \in G$, ya que pertenece a la misma interseccion y ademas como son suma directa un elemneto cualquiera de estos dos se puede escribir como ya pusimos en la igualdad de la $x$, pero como la exprecion de cualuqier espacio vectorial es unica (solo se puede poner de un modo) entonces no podriamos tener que un objeto se pueda escribir como un elemento de $F$ mas $G$ ($x=x+0$) y otro como un elemento de $G$ mas el 0 de $F$ ($x=0+x$), La unica forma que hay de que esta igualdad que tenemos sea verdadera es que elemento de $x$ se el elemento $0$, ($x=0$), de donde el unico elemento que pertenece a $F\cap G$ es el 0, Aqui tenemos demostrado que si un elemento es de la interseccion enotonses tienen que ser el 0.

De derecha a Izquierda: Si $F\cap G=\{0\}$, si $F$ interseccion $G$ es el elemnto Neutro entonses la suma de $F,G$ es directa. Tomamos un elemento de la Suma: $z\in F+G$, (de momento no sabemos que es directa) si se pudiera escribir de dos formas diferentes significaria que la suma no seria directa, en cuyo caso: $z = x+y,x\in F,y\in G $ pero tambien se podria escribir como: $z = x^{'}+y^{'},x^{'}\in F,y^{'}\in G $, esto si NO fuera suma directa, viendo $z$ es igual a $z$, $x+y=x^{'}+y^{'}$ esto seria equivalente a decir que $x^{'}-x=y-y^{'}$, fijense que $x^{'}-x$ por un lado es de $F$ y $y-y^{'}$ son de $G$ asi podemos garantizar que este numero $\in F\cap G$ pero pro hipotesisi el unico numero que pertenece a esa interseccion es $\{0\}$ asi la conclucion es que $x^{'}-x=0$ que seria $x=x^{'}$, $y-y^{'}=0$ que seria $y=y^{'}$ por lo tanto $z=z^{'}$, $Z$ tiene directamente una unica representacion por lo que si un elemnto tendira dos representaciones esta seria la misma entonces la suma de $F+G$ es directa que es lo que queriamos demostrar

El **Ejercicio 7** se podria representar formalmente que si $n=2$ en lugar de dos subespacios tenemos $n$ en general, la suma $F_1+F_2+\cdots+F_n$ es directa si y solo si cada $F_i\cap (F_1\cap F_2\cdots\cap F_{i-1})={0}$ el $i-1$ se refiere a todos los anteriores, esto se puede por induccion, el caso base ya es el ejercicio 6 y la inductiva suponiendo que los $n-1$ son suma directa que pasaria cuando sumamos con el siguente, separar una suma de $n$ elemntos con el $n-1$ primeros con el ultimo.

En ocasiones disponesmos de un subconjunto de vectores $S$ de $E$ que no es subespacio vectorial (Cumplirarn todas las propiedades), pero estamos interesados en el mas pequeño en el mas pequeño subespacio vectorial (con respecto la inclusion) que contiene este subconjunto $S$ es decir que todos y cada uno de ese subconjuntos $S$ forman parte de ese subespacio vectorial.

Si nos encontramos una serie de vectores que no tendiran porque formar un subespacio pero que al buscarle la casa mas pequeña que es el subespacio mas pequeño donde vivien en cada uno de los vectores

Este subespacio siempre existe ya que solo debemos considerar la familia de todos los subespacios vectoriales de $E$ que contienen a $S$ y entonces sabemos que su interseccion es otro subespacio vectorial que, evidentemente, contienen a $S$ y este que es la interseccion sera el subespacio mas pequeño con la propiedad que lo contienen uno de cada ellos.

Por porpia construccion si tenemos un conjunto $S$ buscaremos todos los subespacios de $E$ que contienen a $S$ hacemos la interseccion de todos ellos y ese va a ser el sub espacio vactorial mas pequeño que contien el conjunto $S$ 

**Subespacio vectorial generado por $S$.** Subespacio vectorial más pequeño que existe y que contiene a todos los vetores del conjunto $S$. Lo denotamos por $\langle S\rangle$

Diremos también que $S$ es un conjunto o sistema generador de ese subespacio vectorial o que $S$ genera a $\langle S\rangle$.

En definitiva, hemos visto que el subespacio vectorial generado por $S$ es la interseccion de todos los $F$ subespacios que contienen a S.(Interseccion de todos los $F$ donde $F$ es un subespacio vectorial de que contiene a $S$, la interseccion de todos ellos sera ese subespacio vectorial mas pequeño que contiene a $S$)

$$\langle S\rangle = \bigcap_{\begin{matrix}S\subseteq F\\
F\text{ subespacio}\end{matrix}}F$$

Si tenemos que ir buscando todos los subespacios vectoriales de un espacio vectorial que contiene a un conjunto igual sera dificil porque podria haber infinitos, nesecitaremos de una caracterisacion mejor que no esta anterior.

De forma más general, definimos sistema generador como

**Sistema generador.** Dado un conjunto de vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\in E$(Estos serian los elemntos del conjunto $S$), se dice que forman un sistema generador del espacio vectorial $E$ si cualquier vector $\vec{u}\in E$ se puede expresar como una combinación lineal de ellos. Es decir

Ese conjunto para cualquier vector $u$ de $E$, existe una combinacion lineal de escalares $\alpha$ del cuerpo $\mathbb{K}$ tal que $u$ se puede escribir como es sumatoria, es decir nos regalan un vector de $E$ con esas $u$ seran un sistema generador de ese espacio vectorial  si podemos encontrar escalares tal que aplicando la sumatoria multiplicando escalar por su $u$ nos da ese vector $\vec{u}$

$$\forall\vec{u}\in E,\ \exists\alpha_1,\alpha_2,\dots,\alpha_n\in\mathbb{K}:\ \vec{u} = \sum_{i = 1}^n\alpha_i\vec{u}_i$$

**Proposición.** Sea $S$ un subconjunto cualquiera de un $\mathbb{K}$-e.v. $E$. Entonces

El subespacio vectorial generado por $S$ es las combinaciones lineales posibles de los elemntos del conjunto ($\alpha\cdot x$), donde n es cualquier numero entero positivo y los $x_i$ son elementos del subconjunto $S$ y los $\alpha_i$ son escalares, es decir que el generado por $S$ no es mas que buscar numeros que se nos de la gana y multiplicarlos por los $x_i$ respectivos y sumarlos 

$$\langle S \rangle = \{\alpha_1\cdot x_1+\cdots +\alpha_n\cdot x_n\ |\ n\in\mathbb{Z}^+;\ x_i\in S;\ \alpha_i\in\mathbb{K},\ i = 1,\dots,n\}$$

Es decir, $\langle S\rangle$ es el subespacio formado por todas las combinaciones lineales posibles de elementos de $S$ asi obtenemos el subespacio vectorial generado por $S$ 

**Ejercicio 8.** Demostrar formalmente esta proposición.

**Observación.** A partir de la caracterización de subespacio generado por un subconjunto, queda claro que si tenemos dos subconjuntos de $E$ de forma que $S\subseteq S'$($S$ esta contenido en $S^{'}$ es decir que la S prima tiene los mimos vectores que tenia S y unos pocos mas), entonces resulta que el generado por $S$ $\langle S\rangle\subseteq\langle S'\rangle$, esta contenido en el subespacio generado por S prima, De modo que cualquier vector que perteneciera a $S$ tambien pertenese al generado por $S'$ 

En el caso en que $S$ es finito(En el numero de vectores), $S = \{x_1,\dots,x_n\}$, entonces se puede escribir que el generado por $S$ es el generado por esos elemntos $x$ (son el subespacio generado por elemntos que viven dentro de $S$) y en este caso se puede escribir directamente que los elementos del subespacio generado por $x$, la diferencia es que aqui siempre tenemos $n$ sumas mientras que en la propocision de arriba el numero de sumas podria ser variable porque depende del enetero $n$, en el segundo caso si ya el conjunto generador es finito (tiene un numero finito de elementos) entonces se puede escribir directamente que el subespacio generado por $S$ sus elementos son de la forma de abajo  

$$\langle S\rangle = \langle x_1,\dots,x_n\rangle = \{\alpha_1\cdot x_1 +\cdots +\alpha_n\cdot x_n\ |\ \alpha_i\in\mathbb{K},\ i=1,\dots,n\}$$

**Ejemplo 4**

Los vectores $(1,0,0,\dots,0),\ (0,1,0,\dots,0),\ (0,0,1,\dots,0),\dots,(0,0,0,\dots,1)$, Estos son n vectores donde el primero tiene todos 0 salvo la primera pocicion, el segundo tiene todos 0 salvo la segunda posicion, El tercero tienen todos ceros salvo la tercera pocicion, etc. forman un sistema generador de $\mathbb{K}^n$. 

Por lo tanto, podemos decir que $\mathbb{K}^n$ está finitamente generado. Cualquier vector que se nos pase por la cabeza se puede escribir: un numero por cada uno de los vectores de arriba mas un numero por el otro vector y asi sucesivamente, cualuiqer vector se puede escribir como combinacion lineal de estos vectores de arriba

**Ejemplo 5**

Análogamente, los vectores $\{1,x,x^2,\dots,x^n\}$ forman un conjunto de generadores del $\mathbb{K}$-e.v. (de polinomios, sobre el cuerpo **K**, de grado menor o igual que **n**, porque cualquier polinomio de esos se puede escribir como, **Ejemplo en los vectores de arriba:** un numero por uno mas un numero por **x** mas un numero por **$x^2$**, mas un numero por **$x☻^n$**) $\mathbb{K}_n[x]$, que es por lo tanto los polinomios de grado menor o igual que n son finitamente generado.

En cambio, $\mathbb{K}[x]$ (Polinomios en general (No estan limitados en Grados) sobre un cuerpo **K**),**NO** es un $\mathbb{K}$-e.v. que no es finitamente generado. Si suponemos que $p_1(x),\dots,p_k(x)$ forma un conjunto finito de generadores de este espacio vectorial (Si con **K** polinomios nos vastara para general el conjunto de todos los polinomios habidos y por haber), Si consideramos **n** el maximo de los grados de estos polinomios $n = \max{(\deg(p_1),\dots,\deg(p_k))}$, todo polinomio de grado superior a $n$ (n+1) no podría ser expresado como combinación lineal de los $p_i(x)$ Porque al sumar polinomios de grado como mucho **n** nos sale un polinomio como mucho de grado **n** asi que el polinomio $x^n+1$ no se podria generar con esos $p_1(x),\dots,p_k(x)$, $i= 1,\dots,k$. Llegamos así a contradicción (Por ejemplo el polinomio $p^{n+27}$ si pertenecese a todos el conjuto de los polinomios habidos). Observemos pues que $\mathbb{K}[x]$ (El conjunto de todos los polinomiios en una variable construido sobre el cuerpo K) tiene un conjunto infinito (numerable) de generadores: $\{1,x,\dots,x^n,\dots\}$

**Ejemplo 6**

Dentro de $\mathbb{R}^3$ consideramos el subconjunto $F = \{(x,y,z)\ |\ 5x-y+3z = 0\}$. (Como el conjunto de puntos x,y,z tal que se cumpla la ecuacioon de al lado), esto es un Subespacio vectorial, basicamente si tomamos dos puntos de estaforma y lo sumamos sigue cumpliendo la ecuacion, si tomamos un punto de esta forma y multiplicamos por un escalar tambien sigue cumpliendo la ecuacion asi que $F$ es un subespacio 

Entonces, está claro que $F$ es un subespacio vectorial y que además, todo elemento de $F$ es de la forma $(x,5x+3z,z)$ (Lo que se ha hecho es de la formula como ya sabemos que lo puntos de $F$ cumplen esa ecuacion de arriba y despejamos la $y$ esta queda como $5x+3z$ con $x$ y $z$ libre, asi que) variando $x,z\in\mathbb{R}$ por eso se ponen los puntos de $F$, Si vamos variando los valres de $x$ y $z$ respectivamente nos van a salir todos los posibles punto de este subespacio. Así, todo elemento de $F$ se escribe de la forma $$u = x\cdot (1,5,0)+z\cdot (0,3,1)$$ (Lo que es lo mismo **Sacando x y z factorcomun** de dos vectores respectivamente cualquier elemnto de $F$ se podria esribir como asi arriba igualado a $u$, EN el primer vector en la tercera cmponente como no hay una $x$ se le one un $0$, asi para el segundo vector en su componente) Cuaquier punto que cumpla la ecuacion incial o sea que pertenesca al subespacio $F$ se puede escribir como un numero $x$ mutiplicado por el vector $(1,5,0)$ y mas otro numero $z$ multiplicado por el vector $(0,3,1)$ (Esta es la definicion de que los vectores son un sistema generador de $F$), por tanto, los vecotres $(1,5,0)$ y $(0,3,1)$ generan todo $F$ (Ese sistema generador si cualuqier punto se puede escribir como una combinacion lineal de esos vectores como lo dice en la igualacion a $u$)

**Ejercicio 9**

- Demostrar que $F$ es un subespacio vectorial de $E$
- Detallar por qué los elementos de $F$ tienen esa forma

**Proposición.** Sea $E$ un $\mathbb{K}$-e.v. y $S\subseteq E$ (Que S es un subconjunto de E). Si (u ya pertence al generado por S, o sea que es un elemnnento que se puede escribir como combinacion lineal de los elmentos de S) $u\in\langle S\rangle$, entonces se tiene $\langle S\cup \{u\}\rangle = \langle S\rangle$, (Dice que al generado por S y el generado por S añadiendole la u son el mismo subespacio vectorial), es decir que al espacio vectorial generado por un conjunto S le añadimos un nuevo vector que ya esta generado por los vectores de S el subespacio que sale es el mismo, No se amplia el subespacio

Lo que nos viene a decir esta proposición es que un mismo espacio o subespacio vectorial puede tener conjuntos de generadores diferentes. Importante si nos da mutiples sistemas generadores de un subespacio habran mas interesantes unos de otros, si nos diccen que un subespacio vectorial se puede generar con 5 vectores y otro se genera solo con 2 vectores, con cual nos quedariamos, con el que tiene menos elemntos seria el mejor

**Ejercicio 10.** Demostrar formalmente esta proposición.

## Ejercicio 8 y 10

**Ejercicio 8: **

Propiciones que tienen que ver con el subespacio vectorial geneado por un conjunto vectores, Este se puede escribir como las cominaciones lineales de $\alpha$, $$\langle S\rangle = \{\ \alpha_1 x_1 + \alpha_2 x_2+..+\alpha_n x_n \ |\ n\in \mathbb{Z}^+,\ x_i\in S,\ \alpha_i\in \mathbb{K}\ i=1...n \}$$, donde $n$ es un numero arbitario cualesquiera y cada elemnto de los $x_i$ son elemntos de $S$ y los $\alpha_i$ que serian escalares del cuerpo que estemos usando, Basicamente si este el conjunto formado por una serie de vectores haciendo todas las posibles combinaciones lineales de los elemntos de $S$ lo que sale es el sbespacio vectorial generado por $S$ vector.

En primer lugar el conjunto de combinaciones lineales de elementos de $S$ es un subespacio vectorial que contiene a $S$, es decir el conjunto generado por una serie de vectores $S$ esto es un subespacio vectorial que contiene a $S$ es decir el propio conjnto $S$ esta contenido dentro de este espacio vectorial: $$S\subseteq \langle S\rangle e.v$$

ya que la suma de las combinaciones lineales es una combinacion lineal y el producto de un escalar por una combinacion lineal tambien seria una combinacion lineal de los elemntos de $S$

Que contiene a $S$ ya lo tenemos, que es un espacio vectorial tambien, solo nos hace falta el subespacio vectorial mas pequeño que contiene a $S$, que no existe un subespacio vectorial mas pequeño que el generado por $S$ para esto supongamos que $F$ fuera un subespacio vectorial que contiene a $S$ mas pequeño que el generado por $S$ es decir: Si $F$ fuera un subespacio vectorial que contenga al conjunto $S$ y fuera mas pequeño que el subespacio vectorial generado por $S$: $$S\subseteq F \subseteq \langle S\rangle$$

ya sabemos que cualquier combinacion lineal de elementos de $S$ en particcular serian de $F$ porque este si es un subespacio vectorial, si tomamos una combinacion lineal de elemnentos de $S$ esa combinacion lineal tambien seria de $F$, Pero culaquier combinacion lineal de elementos de $S$ es la definicion de generado por $\langle S\rangle$ ya que ver como este esta definido en un inici por combinaciones lineales de elmentos de $S$ asi que demostrariamos que el generado por $\langle S\rangle$ esta contenido dentro del subespacio vectorial de $F$.

Por tanto al final $F$ y $S$ al final son exactamente el mismo, los dos subespacios vectoriales son iguales, en particular apartir de esta carcterisacion del subespacio vectorial generado por un subconjunto es muy facil demostrar que en el caso de conjunto si tenemos: $$S\subseteq S'$$ entonces el $\langle S\rangle\subseteq\langle S'\rangle$ , el generado de s es un subespacio vectorial del genrado por S prima.

Esta demostracion nos permite decir que el generado por un conjunto de vectores $S$ es el subespacio vectorial mas pequeño que contnene ese conjunto de vectores.

Si tomamos un sobconjunto de R3 que sea subespacio vetorial tal que se cumple esa ecuacion, $F=\{(x,y,z)\in\mathbb{R}\ |\ 2x-y+z=0\}$, podriamos demostrar que es un subespacio vectorial porque es una ecuacion lineal homogenea. De aqui se puede escribir despejando $y$: $y=2x+z$ o lo que es lo mismo cualquier de la forma $(x,y,z)$ cumple que su coordenada $y$ es dos veces la primera mas la tercer o sea que se puede escribir como: $(x,y,z) = (x,2x+z,z)$ Vease que extrayendo la combinacion lineal de vectores  se podria escribir como el vetor $(x,2x,0)$ osea un vector que solo tubiera $x$ y un vector que solo tuviera $Z$ que seria $(0,z,z)$ o lo que es lo mismo: $x(1,2,0)+z(0,1,1)$, asi que podiramos decir que $F$ es el subespacio vectorial generado por los vetores: $F=\langle(1,2,0),(0,1,1)\rangle$, esta tecnica es la base de trabajar con espacios vectoriales, ser capases de buscar sistemas generadores de un espacio vectorial

**Ejercicio 10: **

Esta decia que si $E$ es un $\mathbb{K}-e.v$ y $S\subseteq E$, si el vector $u\in\langle S\rangle$, Entonces el Generado por S y le añadimos el vector $u$, es lo mismo que el generado por $S$; $\langle S\cup \{u\}\rangle=\langle S\rangle$, Aqui lo que tenemos que demostrar es una doble inclusion, hay que demostrar que $S$ union $u$ el geenreado por ellos esta contenido dentro del generado por S y hay que demsotrar tambien lo contrario que el generado por S esta dentro del anterior.

Una de estas es Trivial, Antes se habia dicho que $\langle S\rangle\subseteq\langle S'\rangle$ si el subconjunto $S$ esta contenido dentro de $S'$ entonces si lo aplicamos aqui, de las dos $S\subseteq S\cup\{u\}$ porque el segundo connjunto tienen un elmento mas que el primero, asi que aplicando aquel colorario que vimos: $\langle S\rangle\subseteq \langle S\cup\{u\} \rangle$, es trivial de demostrar por quel colorario, asi que de las dos implicaciones la que va hacia la izquierda es la facil y la que va a la derecha no llevara mas tiempo asi que tomando un elmento: $x\in \langle S\cup\{u\} \rangle$, en este conjunto el elemento $x$ que tomamos se puede escribir como una combinacion lineal; $x=\alpha\vec{u}+\alpha_1x_1+\alpha_2x_2+...+\alpha_nx_n$ aqui se esta supioniendo que el conjunto $S$ continene a los vectores $x$, entonces seria una combinacion lineal de estos elemntos con los $x_i\in S\ \alpha_i,\ \alpha\in\mathbb{K}$ y el vector $u$ que ya conocemos, Como tenemos que por hipotesis que $u\in \langle S\rangle$ enotnces $u\in S$, $u$ se puede escribir como la forma de cierta combinacion lineal de los elemntos de $s$ es decir: $u=\beta_1y_1+\beta_2y_2+...+\beta_ky_k$ donde en este caso $y_i\in S$ y $\beta_i\in\mathbb{K}$, antes se habia dicho que era finitimente generado y no necesarimente lo que si es asi es la ocmbinacion lineal entonces el primero tiene $n$ elemntos y el segundo $k$ elemntos en el conjunto $s$ asi juntando lo primero con lo segundo en las $x$ la $u$ que tenemos la podemos remplazar por la igualdad que es $u$ que es la ultima expresion desarrollada y obtener que el vector $x$ es igual a: $$x=\alpha(\beta_1y_1+\beta_2y_2+...+\beta_ky_k)+\alpha_1x_1+\alpha_2x_2+...+\alpha_nx_n$$

Se podira escribir asi Tambien: $x=(\alpha\beta_1)y_1+(\alpha\beta_2)y_2+...+(\alpha\beta_k)y_k+\alpha_1x_1+\alpha_2x_2+...+\alpha_nx_n$, vijaros pues que escrito de este modo $x$ es combinacion lineal de elemntos de $S$ poque las $y$ y $x$ son elemntos de $S$, asi que tenemos un elentos que es cominacion lineal de elemntos de $s$, en otras palabras $x\in \langle S\rangle$,Asi comprobamos que si un vector ya es combinacion lineal de otros el generado por ese conjunto mas el vector nuevo es igual que  solo tomar los vectores sin contar que el que añadimos al final

# Dependencia e Independencia Lineal de vectores

## Combinaciones lineales

Recordemos la definición de Combinación Lineal (CL)

**Combinación lineal.** Dados $p$ vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_p\in\mathbb{K}^n$ (Que pertenescan al mismo espacio $\mathbb{K}^n$) y los escalares $\alpha_1,\alpha_2,\dots,\alpha_p\in\mathbb{K}$, una combinación lineal de esos $p$ vectores es un vector (Tambien de $\mathbb{K}^n$) dado por una expresión de la forma

$$\alpha_1\vec{u}_1+\alpha_2\vec{u}_2+\cdots+\alpha_p\vec{u}_p\in\mathbb{K}^n$$
Una comobinacion lineal simplemente es una suma de Productos ya que en si en este caso en particular es escalar por vector y se van sumando para **obtener un vector de la misma dimencion**

**Ejemplo 7**

Expresar el vector $(2,-4)$ como combinación lineal de los vectores $(1,1)$ y $(-2,0)$, En otras palabras: Cuanto tengo que alargar el vector $(1,1)$ y el vector $(-2,0)$ para conseguir el vector $(2,-4)$

Necesitamos (Estamos buscando dos escalares) $\alpha,\beta\in\mathbb{R}$ tales que (Que tiene que valer $\alpha$ y que tiene que valer $\beta$ para que esta ecuacion vectorial sea cierta)

$$(2,-4) = \alpha(1,1)+\beta(-2,0)$$

Con lo cual, se trata de resolver el sistema coordenada a coordenada, Escribiendo el sistema de ecuaciones para cada una de las componentes como $\alpha -2\beta = 2$ para la componente $x$ y $\alpha= -4$ para la componente $y$ fijensen que este sistema con dos ecuaciones y dos incognitas tiene una solucion trivial porque el sistema ya esta escalonado   $$\left\{\begin{matrix}
\alpha &-&2\beta &=& 2\\
&& \alpha&=& -4\end{matrix}\right.$$

Así pues, ya tenemos que $\alpha = -4$. Con lo cual, $$2\beta = \alpha-2 = -6\Rightarrow\beta = -3$$

Entonces, la combinación lineal que buscábamos es

$$(2,-4) = (-4)(1,1)+(-3)(-2,0)$$
Por lo que el el vector $(2,-4)$ es combinacion lineal de los vectores $(1,1)$ y $(-2,0)$

## Dependencia lineal

**Dependencia lineal.** (Linealmente dependiente: Uno vector se puede escribir como cominacion lineal del resto de vecctores) Dados los vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_p\in\mathbb{K}^n$, diremos que son linealmente dependientes (LD) si la ecuación vectorial de la combinacion lineal de esos vectores donde las incognitas son $\alpha_i$

$$\sum_{i = 1}^p\alpha_i\vec{u}_i = \vec{0}$$

Estos vectores son linealmente dependientes si esta ecuacion tiene infinitas soluciones y por tanto los escalares $\alpha_i\in\mathbb{K}$ pueden tomar (ininitos) valores no nulos, Lo que estamos buscando para que sea linealmente dependiente es que esta ecuacion vectorial tenga infinitas soluciones o que los $\alpha$

Dados los vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_p\in\mathbb{K}^n$, diremos que son linealmente dependientes (LD) si alguno de ellos se puede expresar como una combinación lineal del resto:

Es decir que si algun $u_i$ de por ahi en el medio se puede expresar como una cominacion lineal hasta el $u_p$ sin contar el i-esimo, si algun $u_i$ se puede escribir como este sumatorio

$$\exists1\le i\le p:\ \sum_{k\ne i}\alpha_k\vec{u}_k = \vec{u}_i$$
Esta cuacion es consecuencia de la ecuacion de mas arriba $$\sum_{i = 1}^p\alpha_i\vec{u}_i = \vec{0}$$ ya que si este conjunto de vectores es linealmente dependiente se puede escribir esta ecuacion con algun $\alpha_i$ no cero, al no ser cero lo que podemos hacer es dividir toda la ecuacion por $\alpha_i$ y despejar el $\alpha_i$ respectivo, este quedara despejado en funcion del resto porque se puede despejar porque su coeficiente no es cero y como lo podemos despejar (Que es lo que dice la segunda definicion) algunos de los $u_i$ (no necesariamente todos) se puede expresar como combinacion lineal de los restantes asi como en el **Ejercicio 7** en el que los vectores son linealmente dependiente porque el primero se puede escribir como combinacion lineal del resto y es presisamente la segunda definicion dada de los vectores linealmente dependientes.**Esto nos permite saber si exite una solucion que permite despejar un vector en funcion de otro** 

## Independencia lineal

**Independencia lineal.** Dados los vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_p\in\mathbb{K}^n$, diremos que son linealmente independientes (LI) si la ecuación vectorial, La misma de antes nos dice que si la combinacion lineal de estos vectores donde $\alpha$ son las incognitas

$$\sum_{i = 1}^p\alpha_i\vec{u}_i = \vec{0}$$

tiene como única solución la solución trivial (donde los $\alpha_p$ son $0$). Es decir, $\alpha_i = 0\ \forall i=1,2,\dots,p$, La diferencia es que si los vectores son **linealmente dependientes**, algunos de los escalares ($\alpha$) de la ecuacion vectorial pueden no ser cero de modo que se puden despejar algunos de los vectores en funcion del resto Pero si los vectores son **Linealmente Independientes** de forma inmediata, todos y cada uno de los $\alpha_i$ deben ser exactamente igual a $0$

En este sentido si los vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_p\in\mathbb{K}^n$ son linealmente dependientes entonces no sera posible nunca expresar esta combinacion lineal como esta que tenemos aqui, Nunca podremos despejar algunos de los $u_i$ en funcion de todos los demas, nunca podremos escribir $u_i$ igual al sumatorio para $k$ diferente de $i$ porque de forma inmediata la solucion al sistema anterior es de puros $0$ y no se puede despejar ningun $u_i$ porque pasariamos a dividir un $0$, entonces otra forma de caracterizar a los vectores linealmente independientes es que nunca es posible expresar uno como combinacion lineal de los demas por eso son linealmente independientes

Dados los vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_p\in\mathbb{K}^n$, diremos que son linealmente independientes (LI) si no es posible expresarlos como una combinación lineal del resto.

$$\not\exists1\le i\le p:\ \sum_{k\ne i }\alpha_k\vec{u}_k  = \vec{u}_i$$

## Dependencia e Independencia Lineal de vectores

En el caso de que tengamos un conjunto $S\ne\emptyset$ (No Vacio), $S\subseteq E$ (y subconjunto del espacio vectorial $E$) finito o no, diremos que $S$ como conjunto es linealmente independiente si cualquier subconjunto finito de $S$ lo es. Es decir que aunque el conjunto sea infinito nosotros estudiamos por subconjunto finitos de $S$ y sus elemntos. Si ese conjunto tiene cualquier suconjunto finito linealmente independiente por extencion se dice que $S$ es un conjunto linealmente independiente.

Es decir, si cualquier combinación lineal de un número finito de elementos de $S$ es igual a 0, implica que todos los escalares deben ser 0. Entonces el conjunto $S$ es un conjunto lineal mente independiente. Esto es por si queremos pensar en conjuntos infinitos como ocurria con los vectores que generaban el conjunto $k$ de $x$ es decil los polinmos de cualquier grado sobre el cuerpo $\mathbb{K}$ si vamos estudiando y todos y cada uno de ellos es sus subconjuntos finitos son todos linealmente independiente el grande por extencion tambien lo es.

De forma análoga, diremos que $S$ es linealmente dependiente si existen un número finito de elementos de $S$ y una combinación lineal suya igual a 0 donde no todos los escalares son 0, es decir que existen un conjunto finito de elementos de $S$ tal que alguno de ellos es combinacion lineal del resto

## Dependencia e Independencia Lineal de vectores

Se ah dicho que los vectores linealmente independientes con la ecuacion vectorial tienen como unica solucion la trivial y que son vectores dependientes en el caso en que la solucion de la ecuacion vectorial tenga alguna solucion ademas de la de puros ceros.

**Proposición.** Sea $E$ un $\mathbb{K}$-espacio vectorial, entonces los vectores $x_1,x_2,\dots,x_n\in E$ son linealmente dependientes si, y solo si, uno de ellos es combinación lineal del resto.

Se da como definicion alternativa de la dependencia lineal, esta proposicion ya que en muchos casos, es asi. Si los conjuntos o los vectores son linealmente
dependientes osea que uno es combinacion lineal del resto.

Se dijo que si el conjunto es linealmente dependiente tiene como solucion que se puede dividir toda la ecuacion por $\alpha_i$ y despejar explicitamente ese $u_i$ en funcion del resto.

**Observación.** Como habréis notado, nosotros habíamos dado como definición alternativa de dependencia lineal esta proposición. En muchos casos es así y en otros muchos se toma como propiedad. Por eso aquí hemos incluído ambas opciones.

Tenemos que demostrar que son dependientes. La combinacion lineal de uno se puede expresar en funcion del resto y que si uno se puede expresar como combinacion lineal del resto el sistema y hasta el $\alpha_n u_i$ tiene alguna solucion diferente de la trivial.

Ambas definiciones son validas: cuando queremos demostrar que un conjunto es linealmente dependiente podriamos usar la del sistema de ecuaciones que es mas facil en el sentido de que es mas mecanico. Uno se pone a escribir la ecuacion vectorial y ver que en efecto o todos los $\alpha_i$ son ceros o algunos no son cero, en cuyo caso son independientes o dependientes respectivamente pero la definicion alternativa nos dice que uno es combinacion lineal del resto, si nos fijamos bien como se componen los vectores y tambien vale para ver la dependencia o independencia lineal.

**Ejercicio 11.** Demostrar formalmente esta proposición.

**Proposición.** Sea $E$ un $\mathbb{K}$-espacio vectorial y $S\subseteq E$ un conjunto linealmente independiente. Si $u\not\in \langle S\rangle$, entonces $S\cup\{u\}$ es linealmente independiente

En otras palabras si tenemos un conjunto que es linealmente independiente y conseguimos un vector que no esta generado o que no pertenezca al subespacio generado por $S$, no se puede escribir como una combinacion lineal de los vectores de $S$, entonces $S\cup \{u\}$ sigue siendo linealmente independiente si uno se puede escribir como combinacion lineal de $S$, añadiendo $u$ al conjunto $S$ que teniamos sigue siendo linealmente independiente

Es que si nos dan dos vectores linealmente independientes y nos dan un tercero que no es combinacion lineal de esos dos primero, los tres vectores tambien son linealmente independientes. De estos tres vectores independientes nos dan otro nuevo vector que no es combinacion lineal de esos tres vectores que ya teniamos, ahora los cuatro vectores tambien son linealmente independientes. Y esta es la idea de ir haciendo crecer el conjunto de vectores linealmente independientes a base de encontrar vectores que no sean combinacion lineal de ellos. Esto esto va a ser una tecnica llamada buscar una base de un espacio vectorial.

**Ejercicio 12.** Demostrar formalmente esta proposición.

## Ejemplos de Depndencia e Independencia Lineal

Lo primero es que si solo tenemos un vector $x\in E  \leftrightarrow x\not=0$ de un espacio vectorial cualquiera
siempre es linealmente independiente siempre y cuando se cumpla que x no sea el vector cero el vector cero (no es independientes de nadien, es dependiente de todo el mundo) pero en general con solo un vector, este siempre sera linealmente independiente.

Dicho de otro modo el vector cero que pertenece al espacio vectorial $0\in E\ l.d$ es el unico vector de un espacio que por si solo es linealmente dependiente ($l.d$), cualquier
otro es linealmente independientes si solo tenemos un vector.
Tambien se puede ver que los vectores por ejemplo en R2: $\mathbb{R}^2=(1,0)(0,1)l$, estos dos vectores son linealmente independientes simplemente con la combinacion lineal y la igualamos a cero: $\alpha(1,0)+\beta(0,1)=(0,0)$

Fijensen que trivialmente nos sale la ecuacion: $\alpha=0$ y $\beta=0$ que es precisamente la definicion de que un conjunto de vectores sea linealmente independiente osea que cualquier combinacion lineal igualada a cero da como unico resultado la solucion.

Ejercicio pero ahora conciderando:
$$\mathbb{K}^m= (1,0,...,0),(0,1,0,...,0)...(0,0,...,1)$$
Al final siempre seran vectores linealmente independientes, demostrando planteando un sistema de ecuaciones lineales que tendran $n$ ecuaciones
y $n$ incognitas con la unica solucion la cero.

Si nos dan un conjunto de vectores explicito (Nos dan estos Vectores): $\{(1,0,1),(0,1,1),(-1,1,2)\}$,
¿Estos tres vectores son linealmente independientes o son linealmente dependientes?.
Realmente es bastante mecanico porque lo unico que hacemos es plantear una combinacion lineal de ellos, igualada a cero y resolver.

Eso significa que si planteamos una combinacion lineal nos quedara: $\alpha(1,0,1),\beta(0,1,1),\gamma(-1,1,2)=(0,0,0)$ de aqui extraeriamos tres ecuaciones con tres incognitas que podriamos operar: Multiplicar cada uno de los escalares por los vectores, sumar los vectores e igualar a cero pero en si, las tres ecuaciones que salen serian: 

-1) $\alpha - \gamma = 0$
-2) $\beta + \gamma = 0$
-3) $\alpha + \beta + 2\gamma=0$

Este sistema lo podriamos demostrar por **Gauss**, despejar directamente, pero es bastante
sencillo demostrar que es un sistema compatible determinado. Sacando los valores:

- $\alpha=\gamma$
- $\beta=-\gamma$
- $\gamma-\gamma+2\gamma=0$,$2\gamma=0$,$\gamma=0$, Esta sustituyendo la ecuacion 1 y 2 en la ecuacion 3, nos queda. Por lo tanto sustituyendo $\gamma=0$, nos queda que $\beta$ y $\alpha$ son igual a $0$.

La solucion seria una solucion trivial, El sistema es compatible determinado porque $\alpha=\beta=\gamma=0$ y por tanto los tres vectores son en efecto linealmente independientes. Asi es como se demuestra en general **la dependencia o independencia lineal de vectores**: se plantea una ecuacion y se resuelve el sistema de ecuaciones lineales que y con el resultado lo sacamos.

**Otro Ejemplo: ** Es que si dentro del conjunto de los polinomios consideramos el
conjunto: $S=\{1,x,x^2,x^3,...,x^n,...\}$ es decir los monomimos desde grado $0$ hasta el infinito. Estos vectores o este conjunto siempre es linealmente independiente ya que cualquier combinacion lineal de un numero finito de monomios de este estilo casi siempre se podran escribir como un polinomio de cierto grado $m$ igualado a cero.
Asi que si plantearamos una combinacion lineal nos quedaria: $$\alpha_0 1+\alpha_1 x+\alpha_2 x^2+...+\alpha_m x^m=0$$

Entonces la unica forma de que este polinomio, sea igual al polinomio 0, es que automaticamente todos y cada uno de los coeficientes que acompañan a cada monomio sean exactamente todos iguales a cero.

Si hablamos de matrices de orden $m$ por $n$, que teniendo un espacio vectorial sobre $K$ en el que podriamos considerar conjuntos de submatrices que cumplieran ciertas Caracteristicas es decir:

Si mi espacio vectorial $E$ es el conjunto de todas las matrices de orden m por
n. **(m filas n columna sobre un cuerpo k)**: $E=M_{mxn}(\mathbb{K})$ 

Para cada elemento fila columna $i\in \{1,2,...,m\}$ y $j\in \{1,2,...,n\}$, se nos podria ocurrir crear un conjunto de matrices que seria la matriz de la forma:
$$e_{ij}\in M_{mxn}(\mathbb{K}$$
Tal que tuviera todas las entradas igual a cero excepto la que tiene en la fila $i$ y columna $j$ que valdria: $1$. Entonces: $e_{ij}$ seria la matriz que tendria ceros por todos lados y un 1 unica y exclusivamente la fila $i$ y columna $j$ entonces por **ejemplo** quien seria la matriz: $e_{11} = \begin{pmatrix} 1 & 0 & ... & 0\\ 0 & ... & ... & ... \\... &  ... & ... &. .. & \\ 0 & ... & ... & 0\end{pmatrix}$ y $e_{12}= \begin{pmatrix} 0 & 1 & ... & 0\\ 0 & ... & ... & 0 \\... &  ... & ... &. .. & \\ 0 & ... & ... & 0\end{pmatrix}$

Bueno pues resulta que estos $mxn$ elementos son linealmente independientes.
el conjunto formado por los: $e_{ij}\in M_{mxn}(\mathbb{K}$, variando $i$ y $i$ con
los indices que se nos han dicho, desde 1 hasta m y desde 1 hasta n respectivamente son linealmente independientes.

Todas esas matrices si nos planteamos una combinacion lineal y las igualamos a cero nos daran que los escalares tienen que ser si o si todos ellos iguales a cero.

## Ejercicio 11 y 12

Los resultados que tienen que ver con la dependencia o independencia lineal de vectores dentro de un espacio vectorial.

El primero es que si tenemos los vectores $x_1,x_2,...,x_n\in E$ de un espacio vectorial $E$ que son linealmente dependientes,Si y solo si, uno de ellos es combinacion lineal del resto (al menos uno de ellos). Fijaros hay que demostrar un si y solo si.

Empecemos por demostrar de izquierda a derecha. Osea que si son dependientes al menos uno es combinacion lineal del resto. Si $x_1,x_2,...,x_n \in E$ son linealmente dependientes (son vectores linealmente dependientes del espacio vectorial $E$). Entonces existe una combinacion lineal de ellos igual a cero.,

Existe un $\alpha_1 x_1+ \alpha_2 x_2 + ... \alpha_n x_n = 0$ y en esta combinacion lineal no todos los escalares son cero.

Por comodidad Vamos a suponer que es el primero, si no fuera el primero podriamos permutar, si fuera el octavo, cambiamos el octavo por el primero y el primero por el octavo y daria igual, asi que vamos a suponer que el primero de los escalares es el que no es cero en la combinacion lineal igualada a cero. $\alpha_1 \not= 0$.

Entonces si $\alpha_1$ no es cero, podriamos despejar $\alpha_1 x_1 = -\alpha_2 x_2- \alpha_3 x_3 + ... - \alpha_n x_n$ Y como no es cero podriamos dividir toda la expresion por $\alpha_1$ o lo que es lo mismo multiplicar por el inverso de $\alpha_1$ ya que no es cero y tiene inverso por tratarse de un cuerpo, asi que $\alpha_1$ se podria escribir como: $-\alpha_1^{-1} \alpha_2 x_2 - \alpha_1^{-1} \alpha_3 x_3 .... - \alpha_1^{-1} \alpha_n x_m$.

Entonces esta seria una forma de despejar $x_1$ o en otras palabras es como decir que
$x_1$ es combinacion lineal de los otros vectores, asi que son combinacion lineal de $x_2$ es combinacion lineal de $x_3$,$x_4$ hasta $x_m$.

Hemos demostrado de izquierda a derecha que en el caso de dependencia lineal al
menos uno de ellos se puede escribir como combinacion lineal del resto.

Evidentemente si en lugar de ser primero el $\alpha_1$ fuera algun otro $\alpha_i$ se
podria demostrar del mismo modo o con el truco en el que cambiamos el papel de $\alpha_1$ por el del $\alpha_i$ que no fuera $0$ en la combinacion lineal para la vuelta se puede llevar a cabo tambien, si $x_1$ es combinacion lineal de los otros es decir si $x_1 = \alpha_2 x_2 + \alpha_3 x_3 + \alpha__n x_n$ esta es nuestra hipotesis uno de ellos es combinacion lineal del resto, fijaros que podemos pasarlo todo a restar y obtener que $x_1 - \alpha_2 x_2 - \alpha_3 x_3 ........ - \alpha_m x_n = 0$.

Tenemos una dependencia lineal porque tenemos una combinacion lineal donde la hemos igualado a cero y tiene por solucion precisamente que no todos ellos son cero en particular fijaros como el primero, es como si tuviera un $\alpha_1 = 1$ y tendriamos una combinacion lineal de los vectores $x_1,x_2,x_n = 0$ con NO todos los $\alpha_i = 0$.

En otras palabras el primer $\alpha$ no es cero y por tanto concluimos que los vectores son linealmente dependientes. El resultado seria analogo si cualquier $x_i$ de por enmedio fuera el que es combinacion lineal del resto,aqui hemos tomado el primero por comodidad.

Esto significa que ahora podemos aplicar cosas como al tener una serie de vectores.
$(1,1,2),(1,1,0),(0,0,2)$ son linealmente dependientes en $\mathbb{R}^3$ porque si tomamos el primer vector le resto el segundo y le resto el tercero da el vector cero cero cero: $(1,1,2)-(1,1,0)-(0,0,2)=(0,0,0)$, Asi que podemos afirmar que son linealmente dependientes porque tenemos una combinacion lineal de ellos igualada a $(0,0,0)$ sin que estos coeficientes respectivos sea cero. (Al menos alguno de ellos) o tambien porque el vector $(1,1,2)$ es combinacion lineal del $(1,1,0)+(0,0,2)$ ahora podriamos jugar a que un conjunto es linealmente dependiente bien porque uno es combinacion lineal del resto o bien por la condicion de dependencia lineal cuando tenemos una combinacion lineal de ellos, algunos de esos coeficientes no es cero como pasa precisamente en este ejemplo.

La segunda proposicion a demostrar es consecuencia de lo anterior Y es que si $E$ es un $K$ espacio vectorial y $S\conj E$ (subconjunto de ese espacio vectorial) un conjunto de vectores que es linealmente independiente entonces si $u$ es un vector que no pertenece al generado por $S$ ($u\not\in generadoS$) (ya sabemos que es un espacio vectorial). Entonces podriamos afirmar que (Union)$S\cap\{u\}$ (el nuevo vector $u$) es un conjunto tambien linealmente independiente.

Si añadimos a un conjunto linealmente independiente un vector que no es combinacion lineal suya automaticamente los vectores antiguos por asi decir junto con el nuevo siguen siendo linealmente independiente. Entonces queremos demostrar que son linealmente independientes vamos a expresar una combinacion lineal suya e igualarla 0. Por tanto: $\alpha U + \alpha_1 x_1 + \alpha_2 x_2 + ... +\alpha_n x_n = 0$ tenemos una combinacion lineal. Para ver que es linealmente independiente vamos a tener que sacar que todos los $\alpha_i$ y el $\alpha$ que acompaña a la $U$ son 0. Asi que $x_i\in S$ no se ah dicho pero los $x_i$ son los elementos del conjunto $S$ evidentemente linealmente independientes por hipotesis.

Supongamos que $\alpha \not= 0$ esto significaria que existiria $\alpha^-1\in\mathbb{K}$ y podriamos multiplicar toda la ecuacion por ese escalar, de modo que quedaria despejado ya: $U = - \alpha^-1 \alpha_1 x_1 - \alpha_^-1\alpha_2 x_2 ... -\alpha^1 \alpha_m x_n$. entonces en este caso habriamos expresado $u$ como combinacion linea de los $x_i$ pero como los $x$ son todos de $S$. Este vector $U$ perteneceria al espacio vectorial generado por $generadoSgenerado$. y esto es una contradiccion por el hecho de que $U$ no era de $S$ asi que no puede pertenecer a $S$. y esto nos lleva a que $\alpha$ no puede ser $0$ asi que $\alpha$ no sea cero tambien es una contradiccion. O sea que Alpha tiene que ser 0. Si $\alpha=0$ la combinacion lineal que tenia expresada originalmente ahora se convierte simplemente en $\alpha_1 x_1 + \alpha_2 x_2 +...+ \alpha_n x_m = 0$. Pero claro tengo una combinacion lineal igualada a cero y por hipotesis los $x_i$ son linealmente independientes. Esto lleva a que todos los escalares (todos los $\alpha_i$) tienen que ser $0$, todos los $\alpha_i$ son $0$, $\alpha$ se a demostrado que es cero, por tanto una combinacion lineal de todos esos vectores igualada a cero da cero. Asi que $\alpha = \alpha_1 = \alpha_2 = ... = \alpha_n = 0$. Por tanto el conjunto $S$ ahora que se le ah añadido el vector $U$ es linealmente independiente.


# Bases de un espacio vectorial

Es la forma en que podemos describir los elementos de que forma parte de un espacio vectorial

**Base de $E$.** Un conjunto de vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\in E$ son una base de $E$ si 

- $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n$ es un sistema generador de $E$ cualquier de vector de $E$ se tiene que escribir como combinacion lineal de esa ecuacion
- $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n$ son linealmente independientes

Asi que una base de $E$ son un conjunto de vectores que viven en $E$ que son sistema generador, culauiqer vector de $E$ se puede escribir como cpminacion lineal de los $u_i$ y es linealmente independientes

**Teorema.** Sean $E$ un $\mathbb{K}$-e.v. y $B\subseteq E$. Entonces, $B$ es una base de $E$ si, y solo si todo vector $\vec{u}\in E$ se puede expresar como una combinación lineal de $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\in B$ de manera única, Aqui esta la clave si $B$ es un base la combinacion lineal que nos da cada uno de los vectores del esppacio vectorial $E$ es unica, solo se puede escribir de una forma

$$\forall\vec{u}\in E,\ \exists!\alpha_1,\alpha_2,\dots,\alpha_n\in\mathbb{K}:\ \vec{u} = \sum_{i = 1}^n\alpha_i\vec{u}_i$$, Esta expreccion se lee, que para todo vector $U$ que pertenescael espacio vectorial de $E$  existe una unica vez esa exclamacion, la $E$ al reves que exsite con la exclamacion es que existe una unica combinacion lineal de $\alpha$, esto es consecuencia de las dos condiciones que tiene que cumppir la base de que es un sistema gerenador (esto de sstema generador es que cualquier vector se puede escribir como combinacion lineal) y que sea linealmente independiente es que ninguno de los vectores sobran si se quitara uno de ellos pues ya no tendriamos un sistema generador y como conecuencioa la combinacion lineal que nos dan cada uno de los vectores del espacio esunica

**Ejercicio 13.** Demostrar formalmente este Teorema.

**Proposición.** Sea $E$ un $\mathbb{K}$-e.v. finitamente generado y sea $S=\{u_1,\dots,u_n\}$ un conjunto de generadores. Entonces $E$ tiene una base finita $B$ de forma que $B\subseteq S$, Podria ser que en ese conjunto genereadores sobraran algunos de ellos (Es un conjunto generador: cualquier vector de $E$ se puede escribir como combinacion lineal de $u_i$ pero a lo mejor alguno subra) y es que $B$ es una base y tambien es un subconjunto de $S$, podriamos extraer un subconjunto finito de todos los $u_i$ que tenemos y esos $B$ van a ser una base del espacio

asi que si tenemos un sistema generador y vemos que sobran vectores o lo que es lo mismo algunos son combinacion lineal de otros simplemente lo quitamos y asi sucesivamente hasta que el sistema generador sea una base o lo que es lo mismo que todos y cada uno de los vectores sea linealmente indpendiente

**Ejercicio 14.** Demostrar formalmente esta Proposición.

**Teorema de Steinitz.**  Sea $E$ un $\mathbb{K}$-e.v., sea $B = \{u_1,\dots,u_n\}$ una base de $E$ y sean $v_1,\dots,v_m$ vectores linealmente independientes. Entonces, se pueden sustituir $m$ vectores cualesquiera de la base $B$ por los $v_1,\dots,v_m$ obteniendo así una nueva base. En particular, se tiene que necesariamente que el numero de vectores $m\le n$ que el numero de vectores de la base. Si nos dan una base $B = \{u_1,\dots,u_n\}$ y encontramos $m$ vetores que son linealmente independientes nosotros podemos elegir que si alguno de la base no nos gusta lo sacamos y colocamos unos de los $v_1,\dots,v_m$ y los que nos queda una base, esto de legir cuales quiera, podemos cambiar cualquieras $m$ vectores de la base por otros linealmente independientes y el resultado seguira siendo una base y eseu la base se puede remplazar

**Ejercicio 15.** Demostrar formalmente este Teorema.

**Teorema.** Sea $E$ un $\mathbb{K}$-e.v. Si $E$ tiene una base finita, digamos $B = \{u_1,\dots,u_n\}$, entonces todas las bases de $E$ son finitas y tienen exactamente $n$ elementos. Esta base tiene $n$ elemntos todas la bases de $E$ volveran a ser finitas con esos elemnto, todas las bases de un mismo espacio vectorial tendran el mismo numero de elemntos

**Ejercicio 16.** Demostrar formalmente este Teorema.

Como hemos visto hasta ahora, un espacio vectorial tiene infinitas bases por tanto no puedes encontrar la BASE, lo que si tienen todas son el mismo numero de elemntos.En particula si hay tantas bases habra alguna que sea tan buena, que nos permita hacer las coasas rapidas o hay una que tiene características especiales. Esta no es otra que la **base canónica.**, esta es la que tiene una representacion mas sencilla de los vectores del espacio vectorial

**Ejemplo 8. Base canónica**

- En $\mathbb{R}^2$, la base canónica es $\{\vec{e}_1,\vec{e}_2\}$ donde $$\vec{e}_1 = (1,0)\qquad \vec{e}_2 = (0,1)$$, En otras palabras cuales marcan el eje de las X $(e_1)$ y cual el eje de las Y$(e_2)$
- En $\mathbb{R}^3$, la base canónica es $\{\vec{e}_1,\vec{e}_2,\vec{e}_3\}$ donde $$\vec{e}_1 = (1,0,0)\qquad \vec{e}_2 = (0,1,0)\qquad \vec{e}_3= (0,0,1)$$, aqui serian los ejes (x,y,z), en la que iria su respecitva $e_i$
- En $\mathbb{R}^n$, la base canónica es $\{\vec{e}_1,\vec{e}_2,\dots,\vec{e}_n\}$ donde $e_i$ tendria un $1$ en la posicion i-esima $\vec{e}_i = (0,\dots,0,1,0,\dots,0)\quad \forall i = 1,2,\dots,n$. Es decir, $e_i$ es el que tiene puros ceros en todas las coordenadas, todas las componentes del vector son 0 a excepción de la $i-$ésima que vale 1.

En particular todas las bases del palno sea canonica o no, tendra dos vectores, todas las bases del espacio de $\mathbb{R}^3$ van a tener tres vectores se o no canonica, todas las bases de $\mathbb{R}^n$ tendra $n$ vectores, esto ya nos da que el numero de cuerpo son el numero de vectores que tiene, Cada Base tienen el mismo numero de vectores para cada para un espacio vectorial fijo.

## Ejemplo de Base

Ya vimos en un ejemplo anterior que el conjunto de vectores es: $\vec{e}_1 = (1,0,0,\dots,0)$, $\vec{e}_2 = (0,1,0,\dots,0)$ hasta $\vec{e}_n = (0,0,0,\dots,1)$ que seria el que tendria $1$ unicamente en la ultima componente que eran vectores linealmente independientes y en particular forman una base. De hecho la llamamos la base canonica de $\mathbb{R}^n$ y forman una base precisamente porque si nosotros tomamos un punto cualquiera de $R^n$ cualquiera por ejemplo $\alpha_1,\alpha_2,\cdots,\alpha_n\in \mathbb{R}^n$

Este se puede escribir muy comodamente, como: $\alpha_1,\alpha_2,\cdots,\alpha_n = $ (se podria escribir como multiplicar $\alpha$ por cada uno de su vector correspondiente):
$\alpha_1(1,0,0,\dots,0)+\alpha_2(0,1,0,\dots,0)+\cdots+\alpha_n(0,0,0,\dots,1)$
o en otras palabras los $\alpha_i$ se podrian expresar como una combinacion lineal de $\sum_{i=1}^{m}\ \alpha_i \vec{e_i}$ o sea cualquier vector de $\mathbb{R}^n$ se puede escribir omo combinacion lineal de los vectores de la base canonica de $\mathbb{R}$.

Tambien el conjunto aquel de matrices, cuando vimos el ejemplo $M_{mxn}\mathbb{K}$ en el que habiamos construido el conjunto $e_{ij}$ que basicamente eran las matrices que tenian un 1 en fila $i$ y columna $j$. Donde $i$ era un numero entre $1,\cdots, m$ y $j$ era un numero entre $1,\cdots,n$ es la matriz de orden m por n que tiene un 1 en fila $i$ columna $j$ y $m$ por $n$ matrices.

Tambien es una base que se llama la base canonica de las matrices m por n, que cualquier matriz ejemplo si A es una matriz $A=(a_{ij})$ entonces se puede expresar como el sumatorio doble $\sum_{i=1}^m\sum_{j=1}^n$ como el producto de las coordenadas de cada matriz todas las entradas $a_{ij}$ multiplicada cada una de ellas por la base $e_{ij}$ De forma analoga el conjunto de los polinomios hasta grado n en la variable X y dentro de el estudiamos el conjunto: $\mathbb{K}_n[x] \{1,x,x^2,\cdots,x^n\}$ esta es la base canonica de los polinomios hasta grado n.

Mas general en lugar de coger los polinomios hasta cierto grado, cogemos todos los polinomios en una variable de cualquier grado pues el conjunto $\mathbb{K}_n[x] \{1,x,x^2,\cdots,x^4,\cdots,x^n\}$ forman tambien la base canonica de los polinomios en una variable en este caso es una base infinita, tiene infinitos numeros de elementos. En su momento vimos el ejemplo: $F=\{(x,y,z)\in\mathbb{R}^3\}$ que cumplia la ecuacion. $2x-y+z=0$ y habiamos hecho una serie de pasos para llegar a que F estaba generado: $F=\langle (1,2,0),(0,1,1)\rangle$ no son solo un sistema generador, sino que son una base, puesto que generan todo el espacio y ademqs estos dos vectores son linealmente independientes. Lo podemoss ver como **Ejercicio**: si la combinacion lineal $\alpha(1,2,0)+\beta(0,1,1) = \vec0$ y vemos que en efecto esta combinacion lineal igualada a cero resulta que tanto Alpha como beta tienen que ser cero.

Mas general, si nos dan un $\mathbb{K}$ espacio vectorial $E$ cualquiera. Es evidente que si el conjunto $S$ es linealmente independiente entonces es una base del subespacio vectorial generado por $S$.

Es decir, si encontramos a $S$ como un conjunto de vectores linealmente independiente automaticamente $S$ es una base del subespacio vectorial generado por $\langle S\rangle$ entonces,esto es aplicacion directa en el ejemplo $F=\langle (1,2,0),(0,1,1)\rangle$ aqui tenemos dos vectores que son linealmente independientes, son una base del espacio vectorial que generan. Este ejemplo se va a utilizar cuando dado un subespacio vectorial quedremos encontrar una base que en si seran tantos vectores linealmente independientes como nos hagan falta para que generen dicho subespacio vectorial.

## Ejercicios 13,14,15,16

Vamos a demostrar 4 teoreamas de las bases de un espacio vectorial

**Primer teorema** $B\subseteq E$ es una base de un espacio vectorial entonces todo vector
$x\in E$ se puede escribir de una **unica forma** como combinacion lineal de los elementos de la base: $\exists!$.
Es decir que para todo $x$ de $E$ existen unos unicos escalares: $\alpha_1,\alpha_2,\cdots ,\alpha_n\in \mathbb{k}$ tal que $x$ es la combinacion lineal de la sumatoria $\sum_{i=1}^n\ \alpha x_i$ los vectores son $x_i$ de la base y $\alpha_i$ son del cuerpo y los $x_1,x_2,\cdots ,x_n\in E$ son algunos de los elementos de la base, no tendriamos porque tenerlos todos, ya que la base podria ser infinita

pero aqui seleccionamos una serie de escalares y una serie de vectores de la base de modo que el vector $x$ se pueden escribir como combinacion lineal de los vectores de la base.
Estos escalares, los que multiplican a $x_i$ se llaman las coordenadas del vector $x$ en la base $B$.

Para demostrar: si $B$ es una base de $E$ entonces todo vector $x$ se podria escribir como
combinacion lineal de elementos de $B$ por definicion de base, lo que nos falta ver es que la representacion es unica, es decir todo lo que hay que demostrar es, $\ \exists!$ nos dice que la combinacion lineal es unica.

Supongamos que $x$ tuviera dos representaciones en esa base, es decir que $x$
se pudiera escribir como una combinacion lineal: $x = \alpha_1x_1 + \alpha_2x_2 +\cdots +\alpha_k x_k$ pero que tambien tuviera una representacion totalmente diferente.
$=\beta_1y_1+\beta_2y_2+\cdots+\beta_my_m$, unos detalles es que los $x$ y $y$ no tienen porque ser los mismos, si que es verdad que los $x$ y $y$ son vectores de la base pero podrian ser diferentes e incluso el numero de vectores podria ser diferente.

Fijaros que aqui tenemos una representacion con $n$ vectores y anteriormente la tenemos con $m$ vectores, podemos suponer que en ambos casos ademas aparecen los mismos vectores de $B$.Si suponemos que algunos podrian ser con coeficientes cero, entonces en lugar de tener una representacion como la que tenemos aqui (Cambiamos a $_k$ para poder seguir usando la $n$).

En lugar de suponer que los vectores son
diferentes: $x = \alpha_1x_1 + \alpha_2x_2 +\cdots +\alpha_k x_k = \beta_1y_1+\beta_2y_2+\cdots+\beta_my_m$ Podriamos suponer que a la izquierda y a la derecha del igual, aparecen los mismos vectores
añadiendo los coeficientes $0$ que hicieran falta, es decir si aqui tenemos un vector que no esta aqui entonces lo ponemos con coeficiente cero delante o viceversa, $0x$. Una forma de pensar de esta combinacion lineal es que $x$ se puede poner como $\alpha_1u_1+\cdots+\alpha_nu_n=\beta_1u_1+\cdots+\beta_nu_n$
La ventaja es que hemos reescrito la combinacion lineal de modo que ahora los vectores de la combinacion lineal son los mismos de izquierda y derecha del igual y por tanto
en este caso no nos preocupa que la $x$ no esta en las $y$ o la $y$ no esta en la $x$ simplemente los vectores son todos ellos y hemos agregado tantos ceros como hiciera falta, en el caso que un vector estuvieran en uno de los dos lados del igual y en el otro no. Por tanto esta combinacion lineal de vectores lo pasamos todo a un lado del igual, igualamos a cero y sacamos factor comun de vectores.
Podriamos reescribir como: $(\alpha_1-\beta_1)\vec{u_1} + (\alpha_2-\beta_2)\vec{u_2} + \cdots+ (\alpha_n-\beta_n)\vec{u_n} = 0$, ahora tenemos una combinacion lineal de vectores igualada a cero pero los $u_i$ son elementos de una base (en una base en particular los elementos son linealmente independientes). si tenemos una combinacion lineal de ellos igualada a cero en efecto resulta que todos los escalares deberian ser cero, asi que $\alpha_1 - \beta_1 = 0,\ \alpha_2-\beta_2=0,\ \cdots ,\alpha_n-\beta_n = 0$ ya
que los $u_i$ son elementos de una base, en particular son linealmente independientes.
De aqui ya sacamos directamente que $\alpha_1=\beta_1,\ \alpha_2 = \beta_2,\ \alpha_n = \beta_n$ o lo que es lo mismo, las dos expresiones, la de la izquierda y la de la derecha del igual son absolutamente iguales osea que las representaciones coinciden. 

Este teorema es un si y solo si, es decir ahora hemos demostrado que si tengo una base cada vector tiene una representacion unica.
Pero pasa lo contrario si cada vector tiene una representacion unica en elementos de ese conjunto $B$ entonces es una base.

vamos a hacer de la otra forma Y es que si, todo vector $x$ se escribe de una unica forma como combinacion lineal de elementos de $B$ se puede considerar una de esas combinaciones.
Podemos considerar que $x = \alpha_1x_1 + \alpha_2x_2 +\cdots+ \alpha_n x_n$ Y tomando esta combinacion lineal supongamos que la igualamos a cero.

Entonces nos sobra la $X$ que hemos puesto por aqui, simplemente tomamos una combinacion lineal de elementos del conjunto $B$ y la igualamos a cero para todos los $b_i$ entonces el vector cero $\vec0$ sabemos que se puede escribir como: $0x_1 + 0x_2 +\cdots + 0x_n$. Pero la hipotesis es que cada vector se puede escribir de una unica forma.
(La hipotesis es esta exclamacion $\exists!$ que tenemos aqui)

Entonces el hecho de que el $0$ se pueda escribir $0x_1 + 0x_2 +\cdots + 0x_n$  pero que la representacion sea unica nos fuerza a que esta combinacion lineal $\alpha_1 = \alpha_2 = \alpha_n$ porque todos ellos deben ser iguales solo existe una representacion para cada vector, finalmente por hipotesis los $\alpha_i=0$ y por tanto tenemos una base del espacio vectorial porque los vectores son linealmente independientes y tenemos un sistema generador porque cualquier vector se puede escribir como combinacion lineal de los elementos de ese conjunto que es una base.

**Segundo Teorema: ** si $E$ es un espacio vectorial infinitamente generado, es decir con un numero finito de elementos finitamente generado y $S = \{ u_1,u_2,..,u_m \}$, es decir que es un conjunto generador del espacio vectorial $S$. entonces $E$ tiene una base finita y ademas la propia base esta contenida en $S$: $B\subseteq S$ de los elementos de $S$ siempre se puede extraer una base finita.

Haciendo la demostracion, si los $u_i$=$u_1,u_2,...,u_n$ son linealmente independientes, entonces ya forman una base y hemos acabado. Lo son porque ya son un sistema generador que ya lo tenemos (el de $E$). Pero si no fuera linealmente independiente supongamos que uno de ellos $u_i$ fuera combinacion lineal del resto entonces por una proposicion anterior el espacio vectorial generado $\langle u_1,u_2,...,u_n \rangle$ seria el mismo espacio
vectorial que el generado $\langle u_1,...,u_{i-1},u_{i+1},...,u_n \rangle$ entonces lo que se ah hecho ha sido quitar el $u_i$ que es el espacio vectorial de todos los $n$ o quitar uno, seria el mismo. Si este nuevo conjunto ya es linealmente independiente ya tenemos una base si no quitamos otro y si no, quitamos otro y si no, quitamos otro, repitiendo este razonamiento las veces que haga falta. Esta claro que llegara un momento en que tendremos un subconjunto $B\subseteq S$ que llegara a ser un sistema generador y linealmente independiente. Entonces en un momento dado, acabaremos con un conjunto de la forma por ejemplo: $\langle u_1,u_2,..,u_k \rangle$ iremos reduciendo hasta que ese conjunto sera linealmente independiente y seguira siendo un sistema generador. Asi es como llegamos a construir una base o bien podria pasar que el espacio vectorial $E$ directamente solo fuera $\langle u_j \rangle$., Podria ser que se quedara reducido a un solo elemento.
Entonces claro este $\langle u_j \rangle$ tendra que ser diferente de cero, $u_j\not=0$, si fuera cero tendriamos el espacio vectorial trivial y evidentemente se supone que no tendira que ser el espacio vectorial trivial porque, si no,no hay que demostrar nada, ya no hay nada que hacer, no hay base, entonces nos quedaria al menos un vector y con ese vector, un vector siempre es linealmente independiente de si mismo y por tanto seria una base.

**Teorema 3: ** llamado Teorema de Steinitz, nos afirma que $B = \{u_1,u_2,...,u_n\}$
es una base de $E$ y por su lado tenemos una coleccion de vectores $v_1,v_2,...,v_m$ que son linealmente independientes. Entonces podremos sustituir $M$ vectores de la base $E$ (sacar $m$ vectores fuera y meterlos $m$ en $v_1,v_2,...,v_m$ y lo que resulta siempre es una base, como consecuencia de esto si tengo $m$ vectores linealmente independientes por lo tanto $m$ es menor e igual que $n$ (que es el tamaño de la base).
Para demostrarlo se puede se puede hacer de forma recursiva sustituyendo los elementos adecuados de la base $u_i$.

En primer lugar vamos a introducir el primero, $v_1$, tomamos este elemento que se supone que es un elemento del espacio vectorial $v_1$ se podra escribir como combinacion lineal de los elementos de la base es decir como: $v_1=\sum_{i=1}^{n}\ \alpha_iu_i$
Dado que $v_1$ forma parte de un conjunto linealmente independiente, $v_1$ esta claro que no puede ser el vector cero si no, no seria linealmente independiente que es la hipotesis del teorema. Entonces como no puede ser cero existe al menos un escalar.
Existe al menos algun $\exists \alpha_i \not= 0$.

Si reordenamos la base $u_i$ (ya sabemos que una base no tiene orden, simplemente tiene una serie de vectores) podemos suponer sin perder generalidad que este escalar que es diferente de cero en lugar de ser un $u_i$ cualquiera seria el primero que es el $\alpha_1$ en cuyo caso si multiplico toda la expresion $\vec{0} \not= v_1 = \sum_{i=1}^{n}\ \alpha_iu_i$ por el inverso de $\alpha_1$ (porque si $\alpha_1$ no es cero tiene inverso) entonces se podria escribir ya desarrollado:
$$\alpha_1^{-1}v_1=u_1(\alpha_1^{-1}\alpha_2)u_2+...+(\alpha_1^{-1}\alpha_n)u_n$$
o lo que es lo mismo, si despejamos $u_1$ nos quedaria: $u_1=\alpha_1^{-1}v_1-(\alpha_1^{-1}\alpha_2)u_2-...-(\alpha_1^{-1}\alpha_n)u_n$.
En otras palabras de esta expresion juntamente con la expresion de $v_1$ que teniamos
como combinacion lineal de los $u_i$, tenemos que el espacio vectorial generado $\langle u_1,u_2,...,u_n\rangle$ coincide con el espacio vectorial generado $\langle v_1,v_2,...,v_n\rangle$, Se puede reemplazar porque como $u$ se puede escribir como combinaci??n lineal de todos los demas mas $v_1$ el espacio vectorial generado por todos los $u$ o quitar el $u_1$ y meter el $v_1$ es el mismo espacio vectorial.

Asi pues $v_1,u_2,u_3,...,u_n$ serian una base y solo faltaria ver que son linealmente independientes, pero es que si escribimos una combinacion lineal de los nuevos:
$\beta_1v_1+\beta_2u_2+...+\beta_nu_n=0$, El hecho de que $v$ se pueda escribir como combinacion lineal de todos los $u_i$ nos quedaria: $\beta_1(\sum{\alpha_iu_i})+\beta_2u_2+...+\beta_nu_n = 0$ Si expando y modifico todo este sumatorio quitamos el parentesis y agrupo los vectores nos quedaria: $$\beta_1\alpha_1u_1+(\beta_1\alpha_2+\beta_2)u_2+...+(\beta_1\alpha_n+\beta_n)u_n=0$$
Esto seria la expresion desarrollada de esta combinacion lineal, como los $u_i$ forman una base todos estos escalares que tenemos por aqui, tienen que ser cero, es decir todo lo que contenga $\alpha$ y $\beta$ en esta ultima expresion. Pero claro como todos tienen que
ser ceros si empezamos por el primero $\beta_1\alpha_1=0$ fijensen que $\alpha_1$ no puede ser $0$ por que fue lo que pusimos al inicio como $\alpha_1\not=0$ entonces como $\alpha_1$ no es 0, Esto implica que $\beta_1=0$.

Si repetimos al segundo elemento nos sale que $\beta_2$ tambien serea $0$, $\beta_3=0$, $\beta_n=0$, entonces todos los $\beta_i$ son ceros o en otras palabras son linealmente independientes asi que son un sistema generador (generan el mismo espacio vectorial, son linealmente independientes: quitar $u_1$ y poner $v_1$ sigue siendo una base del mismo espacio vectorial).

Una vez que ya hemos introducido el primero de los vectores, supongamos que ya hemos introducido $K$-vectores de las $v$ es decir que ahora tenemos el conjunto $v_1,v_2,...,v_k$ y veamos como se podria introducir el siguiente $u_{k+1}$ porque a partir del conjunto que forma una base tendriamos $,u_{k+2},u_{k+3},...,u_{k+n}$ una vez que hemos medido el primero, supongamos que hemos metido hasta el $k-esimo$ veamos que se puede quitar el $u_{k+1}$ y meter en su lugar el $v_{k+1}$. Esto realmente seria volver a reescribir la demostracion, simplemente como $v_{k+1}$ es un elemento del espacio vectorial $E$, el $v_{k+1}$ se escribiria como cierta combinacion lineal, solo que en este caso la primera parte seria $\sum_{i=0}^{\mathbb{K}}\alpha_iv_i+\sum_{i=\mathbb{K}+1}^{n}\alpha_iu_i$, entonces se separo el sumatorio en las dos partes. En la que se llevan los vectores $v$ y en la que se llevan los vectores $u$ y evidentemente haciendo el mismo argumento que
en el paso anterior se puede sustituir el $v_{k+1}$ por cualquier vector de la base en esta combinacion lineal que tenga coeficientes no nulos y tendriamos finalmente que es un sistema generador.

solo falta ver que algunos de los $\alpha_i$ que tenemos arriba, no es
cero, pero es que si todos fueran cero, evidentemente entrariamos en contradiccion de que los $v_i$ son linealmente independientes, porque si todos estos que tenemos en la parte de la sumatoria $\sum_{i=\mathbb{K}+1}^{n}\alpha_iu_i$ fueran cero, $\alpha_i=0$
al final lo que tendriamos es una combinacion lineal igualada a cero con algun $\alpha_i$ de la parte de la sumatoria $\sum_{i=0}^{\mathbb{K}}\alpha_iv_i$ que serian diferente de cero, entonces esto nos diria que el conjunto $v_1,v_2,...,v_{k+1}$ es ligado o lo que es lo mismo es linealmente dependiente, pero esto es una contradiccion ya que por hipotesis eran linealmente independientes

**Teorema 4: ** Si $E$ tiene una base finita, entonces todas las bases de $E$ son finitas y tienen el mismo numero de elementos, en otras palabras si $B=\{u_1,...,u_n\}$ es base de $E$ todas las bases tienen siempre $n$ elementos (siempre todas las bases de $E$ son finitas y todas tienen el mismo numero de elementos).
Y la demostracion: Si $S$ es otra base $S=\{v_j,j\in J\}$ no necesariamente
finito, pero si es una base de $E$, cualquier subconjunto finito de $S$ a lo mejor no seria generador pero cualquier subconjunto finito de $S$ seria linealmente independiente.

Aplicando el teorema de Steinitz tenemos una base y un subconjunto linealmente independiente. tenemos una consecuencia de que el numero de elementos de ese conjunto linealmente independiente $m$ tendra que ser menor igual que $n$ (el numero de elementos de la base) y por tanto directamente $S$ tiene que ser finito, que es la primera parte que tiene un numero finito de elementos y por tanto $S$ se puede escribir $S=\{v_1,v_2,...v_m\}$. Pero a un razonamiento similar aplicado a $B$ nos demostraria que si, tenemos que poder generar con esa base, cualquier elemento de la otra porque a su vez $S$ tambien es base. Aplicando el mismo razonamiento al reves.
Ademas llegaria a la conclusion de que $n$ tiene que ser menor o igual que $m$ tomando el papel de $B$ y de $S$ girado y llegariamos a lo contrario, uniendo estas dos desigualdades, $m$ mas pequeño $n$ y $n$ mas pequeño que $m$, llegariamos a la conclusion de que en efecto $m=n$ o lo que es lo mismo todas las bases de un mismo espacio vectorial tienen siempre el mismo numero de elementos.

Como también hemos visto que el número de elementos de una base siempre es el mismo dado un espacio vectorial $E$ es único, tiene sentido definir el concepto de **dimensión de un espacio vectorial $E$**.

**$E$ de dimensión finita.** Sea $E\ne\{0\}$ (NO NUlo) un $\mathbb{K}$-e.v.. Diremos que $E$ es de dimensión finita si existe $n\in\mathbb{Z}^+$ y una base de $E$ (y, por tanto, todas) formada por $n$ vectores. En este caso $n$ se llama la dimencion del espacio $E$

**Dimensión de $E$.** Es el número $n$ de vectores que conforman cualquiera de sus bases. Lo denotamos $\dim(E)$

**Observación.** Si $E = \{0\}$,(Es que el espacio vectorial 0, tendra tamaño 0) por definicion no tendrá base, pero diremos que es de dimensión finita (Si su base tiene un numero finito de elemntos), tiene exactamente $n$ elemntos (n vectores) y en ese caso el numero de elemntos que conforma cualquiera de sus bases se le llama la Dimencion del espacio vectorial y con $\dim(E) = 0$, el $0$ si que es un espacio vectorial y ademas esta contenido dentro de cualquier otro espacio, es un subespacio trivial

Asi que llamaremos dimencion del espacio vectorial $E$ al numero de vectores que conforme cualuqier de sus bases

Si el numero de vectores no es finito sera que:

**$E$ de dimensión infinita.** Si $E$ no tiene ninguna base finita, como podria ser el caso de los polinomios con coeficientes a $K$ polinomios de cualquier grado, no exsitia ningun conjunto finito que fuera generador de ese espacio vectorial. En este caso diremos que $E$ es un espacio vectorial de dimencion infinita, lo denotaremos como $\dim(E) = +\infty$

**Ejemplo 9**

- $\dim(\mathbb{K}^n) = n$, del espacio vectorial formado por vectores de $n$ coordenadas, ya vimos que una base del espacio vectorial es la base canonica
- $\dim(\mathbb{K}_n[x]) = n+1$, Es decir los polinosmios de grado menor o igual que $n$ en variable $x$ es $n+1$ la dimencion y esto es porque una base del espacio vectorial del de polinomios es $1,x,x^2,x^2,...,x^n$ si contamos esta base que seria la base canonica de los polinomios es exactamente de $n+1$ elemntos, uno para cada polinomio, de la exprecion de arriba, Seria los polinomios de grado menor o igual que $n$ sobre la variable $x$ es de dimencion $n+1$
- $\dim(\mathbb{K}[x]) = +\infty$, porque no tiene ninguna base finita, asi que la dimencion de los polinomios en una variavle es infinita como espacio vectorial
- $\dim(\mathcal{M}_{m\times n}(\mathbb{K})) = m\times n$, una base de estas matrices, seria las $mxn$ matrices que tienen $0$ en todos lados menos en un pocicion que tienen un uno, por ejemplo las matrices $2x2$ tienen dimencion $4$ (que serian todas las pocibles combinaciones de puros $0$ y con un solo $1$), esas serian las 4 matrices que generarian el espacio vectorial de las matrices cuadradas de orden 2, ademas que serian linealmente independiente

**¡Ojo!** Puede que a veces nos haga falta escribir la dimensión indicando sobre que cuerpo estamos trabajando.

Entonces, lo que haremos será escribir $\dim_{\mathbb{K}}(E)$, aveces se habla de la dimencion de un espacio vectorial sobre un cuerpo

**Ejemplo 10** en particular la dimecion de $\mathbb{R}$ como espacio vectorial

- $\dim_{\mathbb{Q}}(\mathbb{R}) = +\infty$, Se dice como $Q$ espacio vectorial fabricado sobre los numeros enteros es de dimencion infinita porque se nececitan infinitos numeros racionales para generar a los reales, asi que la dimencion de $R$ como $Q$ espacio vectorial tiene dimecion infinita
- $\dim_{\mathbb{R}}(\mathbb{R}) = 1$, La dimencion de $R$ como $R$ espacio vecctorial solo se nececitan un numero para multiplicarlo o sumar por escalares y obtener todos los demas 
- $\dim_{\mathbb{R}}(\mathbb{C}) = 2$, La dimencion de loa complejos como $R$ espacio vecotrial es $2$,ya que $\{1,i\}$ es una base de (los numeros complejos utilizando numeros reales, es 1 para conseguir los reales y la unidad imaginaria $i$ para conseguir los complejos puros)$\mathbb{C}$ como $\mathbb{R}-$e.v.

**Proposición.** Sea $E$ un $\mathbb{K}$-e.v. de dimensión finita y sea $B = \{u_1,\dots,u_n\}$ una base de $E$

1. Si los vecctores cualesquiera $v_1,\dots,v_n$ son L.I.(linealmente Independietes), entonces son base de $E$, Tenemos $n$ vectores L.I en un espacio de dimencion $n$ porque su base tiene $n$ elementos, entonces son tambien una base  
2. Si $v_1,\dots,v_n$ generan todo $E$, entonces son base de $E$, Si tenemos los mismo numeros de vectores que la base y generan todo el espacio tambien son una base, si sabemos que tenemos un espacio vectorial de dimencion $n$, es un espacio vectorial sobre $K$, con $n$ elemntos en su base, con encontrar $n$ vectores linealmente independiente tenemos una base, con encontrar $n$ vectores que generen todo el espacio tenemos una base
3. La dimensión de $E$, coincide con el máximo número de vectores LI, si el espacio es de dimencion $n$ no podremos encontrar $n+1$ vectores L.I (Por ejemplo en $R^2$ nunca vamos a encontrar $3$ vectores L.I) y con el mínimo número de generadores, cualquier sistema generador tendra como minimo el numero de elemntos que la dimencion del espacio vectorial (Por ejemplo en $R^2$ como minimo se nececitan $2$ vectores para tener un sistema generador)
4. Todo conjunto de vectores LI de $E$ se puede completar hasta una base de $E$, nos dan un conjunto de vectores L.I que todabia no son base (no tenemos $n$), podemos añadir vectores que sean independientes de los anteriores, hasta completar y conformar una base del espacio vectorial $E$
5. Si $F$ es un sub-e.v. de $E$, entonces $F$ también es de dimensión finita y $\dim(F)\le\dim(E)$. Además, $\dim(F) = \dim(E)$ si, y solo si $F = E$, es decir la dimencion de un subespacio siempre es menor o igual que la dimencion del espacio y solo es igual en caso que el subespacio sea el propio espacio vecctorial

**Ejercicio 17.** Demostrar formalmente esta Proposición.

**Corolario.** Sea $E$ un $\mathbb{K}$-e.v. Entonces $E$ es de dimensión infinita si, y solo si, podemos encontrar conjuntos de vectores LI de cardinal finito tan grandes como queramos, si podemos ir rellenando con mas y mas vectores L.I hasta tener un conjunto infinito  al final el espacio vectorial sera de dimencion infinita

**Ejercicio 18.** Demostrar formalmente este Corolario.

**Corolario.** Si $E$ es un $\mathbb{K}$-e.v. y $E = \langle u_1,\dots,u_n\rangle$ entonces $E$ es de dimensión finita y $\dim(E)\le n$. Es decir, todo $\mathbb{K}$-espacio vectorial finitamente generado es de dimensión finita menor o igual al número de generadores podria ser que en esos generadores alguno sobrara y fuera dependiente por eso la dimencion del espacio siempre es menor o igual que el numero de generadores

**Ejemplo 11**

Sea $F$ el subespacio vectorial de $\mathbb{R}^3$ dado por el conjunto de puntos $$F = \{(x,y,z)\in\mathbb{R}^3\ |\ x-y+z = 0\}$$, esto seria un plano dentro de $\mathbb{R}^3$

Todos los vectores de $F$ se pueden escribir como $(x,x+z,z)$ (Hemos Despejado la $y$) variando $x,z$ en $\mathbb{R}$ y, por tanto, nos dan todos los vectores del plano esto significa que cualuqier punto de $F$ se podra escribir como $x$ veces el vector que multiplica y $z$ vesces el vector que multiplica 

$$(x,x+z,z) = x\cdot(1,1,0)+z\cdot(0,1,1)$$

Es evidente que $u_1 = (1,1,0)$ y $u_2 = (0,1,1)$ van a ser un sistema generador de $F$. También estos dos vectores por si solos son LI (No se pueden generar entre si para obtener el otro) viendo que $u_1$ tiene un cero en la tercera coordenada y $u_2$ tiene un cero en la primero entonces por que multipliquemos el vector $u_1$ por el numero que se nos de la gana nunca vamos a conseguir un $1$ en la tercera coordenada lo mismo para $u_2$, asi que multiplicando un vector por un escalar nunca vamos a conseguir el otro por eso son Linealmente Indepententes. Por lo tanto, forman una base de $F$.

Veamos ahora como completarla (Podriamos amplear) hasta una base de $\mathbb{R}^3$ (Que es el espacio grande donde vive $F$), este $\mathbb{R}^3$ tiene dimencion $3$ y aqui tenemos dos vectores:

Siguiendo el Teorema de Steinitz podemos seguir ampleando hasta conseguir el mismo numero de vectores que la dimencion, lo que haremos será ir introduciendo sucesivamente $u_1,u_2$ a una base conocida, como por ejemplo la unica que conocemos es la base canónica $e_1 = (1,0,0),\ e_2 = (0,1,0),\ e_3 = (0,0,1)$. (Este teorema nos decia que podemos quitar un vector de estos e introducir uno que sea linealmente independiente)

- Como que el vector se puede escribir $u_1 = (1,1,0) = 1\cdot e_1+1\cdot e_2$, podemos sustituir por ejemplo $e_2$ (de la base canonica) por $u_1$ obteniendo así una nueva base que seria $e_1,u_1,e_3$, como total el $u_1$ ya es combinacion lineal de $e_1$ y $e_2$, nos cargamos a $e_2$ de la base canonica y metemos este $u_1$

Para introducir $u_2$, primero hay que detectar de quien es combinacion lineal porque vamos a usar la nueva base: $e_1,u_1,e_3$, lo escribimos como combinación lineal de la nueva base $$u_2 = (0,1,1) = -(1,0,0)+(1,1,0)+(0,0,1) = (-1)\cdot e_1+1\cdot u_1+1\cdot e_3$$

Por tanto $u_2$ es combinacion lineal de los 3, podemos sustituir cualquiera de los restantes ya que todos los escalares son distintos a 0. Así, según el Teorema de Steinitz, si nos cargamos a un $e$, $u_1,u_2,e_3$ es una base de $\mathbb{R}^3$ que evidentemente completa a la de $F$, pero si no cargamos a $e_3$ y haber dicho que $e_1,u_1,u_2$ es tambine una base de $\mathbb{R}^3$, Hemos tomado una base de un subespacio dos vecotres que eran linealmente independientes y que generaban ese subespacio $F$ y los hemos ido complentando con los vectores de la base canonica hasta que hemos conseguido la base del espacio vectorial completo es decir de $\mathbb{R}^3$

## Ejercicio 17 y 18

Vamos a demostrar las propciones de arriba.

- 1.- Si los vecctores cualesquiera $v_1,\dots,v_n$ son L.I. Aplicando el teorema de Steinitz podemos sustituir todos los $u_i$ por los $v_i$. Si los podemos sustituir todos entonces es que son una base. los independientes los cambiamos por otros de los que ya son una base y el conjunto que queda tambien es una base. Asi que es trivial a partir del teorema de Steinitz.
- 2.- Si $v_1,\dots,v_n$ generan todo $E$, entonces son base de $E$, el espacio vectorial $E$ en este es el espacio generado por los $v_i$ que es el que colocamos entre diamantes.
Entonces $v_1,\dots,v_n$ es un sistema generador de $E$ por un resultado anterior que hemos demostrado, $E$ tendra una base contenida dentro de los $v_1,\dots,v_n$. Eso significa que un subconjunto de ellos sera una base, pero claro la dimension de $E$ ya es $n$ porque una base de del espacio vectorial tiene $n$ elementos, son los $v_1,\dots,v_n$, asi que como la dimencion del espacio vectorial es $n$ tienen que ser todos los $v_1,\dots,v_n$ ser una base, ya que de lo contrario habremos encontrado una base mas pequeña lo cual es imposible porque en un espacio vectorial todas las bases tienen la misma dimension (el mismo numero de elementos).
- 3.- La dimensión de $E$, coincide con el máximo número de vectores LI y con el mínimo número de generadores. La primera parte que coincide con el numero maximo de vectores linealmente independientes es trivial por el teorema de Steinitz ya que podriamos reemplazar y no podriamos hacer crecer a la base. Y en el segundo caso tambien
es cierto, con el minimo numero de generadores, porque si hubiera uno mas este seria linealmente dependiente de los anteriores y habria una contradiccion con que la base siempre tiene el mismo numero de elementos asi que por propia construccion y por el teorema de Steinitz, se verifica que la dimension coincide con el maximo numero de los linealmente independientes y con el minimo numero de vectores generadores (es el optimo por asi decir, no podemos hacerlo crecer o decrecer mas porque violaria la regla de independencia o la regla de sistema generador).
- 4.- Todo conjunto de vectores LI de $E$ se puede completar hasta una base de $E$. Es decir que si tenemos $i_1,i_2,...,i_k$ que son vectores del espacio vectorial $E$ linealmente independientes, podriamos buscar $n-k$ vectores adicionales $z_1,z_2,...,z_{n-k}$ de modo que la base de juntarlos $i_1$ hasta $i_k$ y los $Z_1$ hasta $z_{n-k}$ formarian una base, esto es como decir el enunciado del teorema de Steinitz, que si tenemos algo que es independiente lo podemos ir haciendo crecer hasta tener una base de $n$ vectores.
- 5.- Si $F$ es un sub-e.v. de $E$, entonces $F$ también es de dimensión finita y $\dim(F)\le\dim(E)$. Además, $\dim(F) = \dim(E)$ si, y solo si $F = E$. En este caso aplicando el apartado 3 de esta misma proposicion como todos los vectores de $F$ tambien son de $E$., No se puede tener mas de $n$ vectores linealmente independientes, por tanto como tope $F$ podria tener tantos vectores linealmente independientes como tiene $E$ y esto se traduce en que la dimension de $F$ tendria que ser menor o igual que la de $n$.
Lo segundo es que si la dimension de $F$ es igual a $n$, $F$ tendra una base de $n$ elementos llamemoslos $v_1,v_2,...,v_n$ que tambien seran una base de $F$ porque uno es subconjunto de otro y ademas los dos son de la misma dimension, entonces $E$ es igual a $F$, si y solo si, coinciden en dimensiones, porque si coincidieran en dimensiones la base de uno seria a su vez la base de otro.

**El Corolario** Sea $E$ un $\mathbb{K}$-e.v. Entonces $E$ es de dimensión infinita si, y solo si, podemos encontrar conjuntos de vectores LI de cardinal finito tan grandes como queramos. Si la dimension de $E$ es infinita esta claro que podremos encontrar conjuntos de $n$ vectores linealmente independientes para cualquier $n$ que sea. $n=7$ podriamos encontrar $7$ vectores linealmente independientes, $n=25$ podriamos encontrar 25 vectores linealmente independientes.

Por tanto si $n$ es de dimension infinita podriamos encontrar tantos conjuntos de vectores independientes como quedramos y lo contrario. Si la dimension de $E$ fuera un numero finito, aplicando la proposicion anterior no podremos encontrar un conjunto de $n+1$ vectores linealmente independientes, lo cual llega en contradiccion con el enunciado de que podemos encontrar un conjuntos de vectores tan grande como queramos asi que la unica forma es que, el espacio vectorial sea de dimension infinita demostrando asi este si y solo si.

**El Otro Corolario:** Es trivial porque si $E$ esta generado por esos vectores, cualquier vector se puede escribir como combinacion lineal de ellos. Lo unico que hay que hacer es quitar de en medio a aquellos que sean dependientes y lo que me resultara sera una base porque tendremos un numero igual o menor que los vectores que generan el espacio vectorial.
-----------------------------------------------------------------------------------------

Este teorema nos relaciona la dimencion de $F$ y $G$ (de dos subespacios del espacio vectorial $E$) con la dimencion de la suma y la dimencion de la interseccion

**Teorema. Fórmula de Grassmann.** Sea $E$ un $\mathbb{K}$-e.v. de dimensión finita y sean $F$ y $G$ subespacios vectoriales de $E$. Entonces se verfica $$\dim(F+G)+\dim(F\cap G) = \dim(F)+\dim(G)$$ Nos dice que la dimencion del subespacio vectorial, suma de $F$ y $G$ mas la dimencion del espacio vectorial interseccion $F$ y $G$, es igual a la dimencion de $F$ mas la dimencion de $G$, (La dimencion es el numero de elemntos de las bases respectivas)

**Ejercicio 19.** Demostrar formalmente este Teorema.

**Observación.** Notemos que si tenemos una suma directa, $F\oplus G$, tenemos que $F\cap G = \{0\}$ (El vector 0), lo que equivale a decir $\dim(F\cap G) = 0$ (Porque el vector 0 no tiene base pero si tiene dimencion que es 0). Por tanto, por el `Teorema` anterior, tenemos $$\dim(F\oplus G) = \dim(F) + \dim(G)$$ Aqui se ve porque la suma directa es **Importante** ya que la suma de subespacios (que tienen su complejidad) se convierte en suma de dimenciones, asi que es la mejor suma que existe en terminos de subespacios vectoriales porque la dimencion del suespacio suma directa es la suma  de dimenciones de ambos subespacios

**Corolario.** Sean $E$ un $\mathbb{K}$-e.v. de dimensión finita y $F,G$ sub-e.v. de $E$. Entonces las siguientes afirmaciones son equivalentes:

- $F$ y $G$ son complementarios ($E = F\oplus G$, que $E$ se puede escribir como esta suma directa)
- $F\cap G = \{0\}$ (Solo el vector $0$ vive a la vez dentro de $F$ y de $G$) y $\dim(E)= \dim(F)+\dim(G)$

**Corolario.** Sea $E$ un $\mathbb{K}$-espacio vectorial de dimensión finita, entonces todo subespacio vectorial $F$ admite al menos un complementario. a nosotros nos dan un espacio vectorial $E$ dentro buscamos un subespacio vectorial $F$ y siempre podremos encontrar un complementario tal que aplicando suma directa con $F$ da el espacio vectorial total ($E$).

**Ejercicio 20.** Demostrar formalmente este Corolario.

## Ejercicios 19 y 20.

La demostracion de la **formula de Grassmann: ** Nos dice que, sea $E$ un $\mathbb{K}$-e.v. de dimensión finita y sean $F$ y $G$ subespacios vectoriales de $E$. Entonces se verfica $$\dim(F+G)+\dim(F\cap G) = \dim(F)+\dim(G)$$.

Es decir que dados dos subespacios vectoriales cualesquiera la suma de sus dimensiones es igual a la dimension del espacio vectorial suma mas la dimension del espacio vectorial interseccion.(siempre y cuando $E$ sea de dimension finita). Como $E$ es de dimension finita digamos que $dim(e)=n$ para comodidad nuestra, entonces todos sus subespacios vectoriales lo son. Asi que vamos a llamar $m$ la dimension del espacio vectorial interseccion ($dim(F\cap G)$), $r$ la dimension del espacio vectorial F ($dim(F)$) y $s$ la dimension del espacio vectorial G ($dim(G)$).

En este caso si tomamos una base del espacio vectorial interseccion, pues esta seria de la forma: (nos lo inventamos) $u_1,u_2,...,u_m$ (Porque se ha dicho que es de dimension $m$), esto seria una base del subespacio vectorial $F\cap G$.
Como consecuencia del teorema de Steinitz sabemos que podemos completar esta base hasta tener una base de F. De modo que nos quedaria que el conjunto $u_1,u_2,...,u_m$ y a partir de aqui considerar un $u_{m+1}$ hasta $u_r$.
$$u_1,u_2,...,u_m,u_{m+1},...,u_r$$

Obtendriamos de eso una base de $F$. El teorema de Steinitz afirma que podemos extender una base de ese subespacio o un conjunto linealmente independiente hasta que sea una base del espacio grande.

por otro lado podemos hacer lo mismo de extender el conjunto $u_1,u_2,...,u_m$ con otros nuevos vectores $v_i$, todo junto quedaria: $$u_1,u_2,...,u_m,v_{m+1},...,v_s$$

Para obtener en este caso una base del espacio vectorial $G$. De este modo, como todo vector del espacio vectorial suma ($W$): $W\in F+G$.

$W$ sera una combinacion lineal de los vectores:
$$W=\{u_1,...,u_m,u_{m+1},...,u_r,v_{m+1},...,v_s\}$$
Si vemos que todos estos vectores son linealmente independientes habremos acabado ya que seran una base de $F+G$ porque cualquier vector se podra escribir como combinacion lineal suya (formaran un sistema generador del espacio vectorial suma). Y aparte obtendremos que la dimension del espacio vectorial $dim(F+G)$ seran los $m$ vectores que teniamos originalmente,mas los vectores $m+(r-m)$ que son los que se añadio de $F$ y $s-m$ vectores:
(Operando ya todo Junto nos quedaria)
$$dim(F+G)=m+(r-m)+s-m=r+s-m$$

Asi nos quedara demostrada la formula de Grassmann, asi que todo pasa para demostrar que los vectores $u_i$ y $v_i$ son linealmente independientes.

Y para demostrar que son linealmente independientes, tomemos una combinacion lineal de todos ellos e igualemos a cero. La combinacion Lineal:
$$\sum_{i=0}^{r}\alpha_iu_i+\sum_{j=m+1}^{s}\beta_jv_j=0$$

Y si con esto sacamos que si todos los $\alpha$ y los $\beta$ son $0$ habremos terminado, ya que tenemos si pasamos a restar un sumatorio al otro:
$$\sum_{i=0}^{r}\alpha_iu_i=-\sum_{j=m+1}^{s}\beta_jv_j$$

Entonces fijensen que este vector $\sum_{i=0}^{r}\alpha_iu_i$ es una base del espacio vectorial $F$ pero tambien es un vector de $G$ porque $-\sum_{j=m+1}^{s}\beta_jv_j$ es un elemento generado por vectores que forman parte de una base de $G$. Asi que esta combinacion lineal es a la vez de $F$ y es a la vez de $G$:
$$\sum_{i=0}^{r}\alpha_iu_i=-\sum_{j=m+1}^{s}\beta_jv_j\ \in F\cap G$$
Es a la vez de los dos subespacios osea que pertenece al espacio vectorial interseccion.
Por tanto se podria escribir la segunda parte.
(Por comodidad se ah quitado al menos, si un elemento pertenece, su opuesto tambien).
$$\sum_{j=m+1}^{s}\beta_jv_j=\sum_{k=1}^{m}\delta_ku_k$$
Se podria escribir como esa combinacion lineal, es $\delta_ku_k$ porque como pertenece a la interseccion lo expreso como combinacion lineal de la base de la interseccion O tambien podriamos escribir que el sumatorio:

$$\sum_{k=1}^{m}\delta_ku_k-\sum_{j=m+1}^{s}\beta_jv_j=0$$

Se a vuelto a pasar al otro lado del igual, pero ahora todos los vectores que aparecen en esta combinacion lineal son una base de G y por eso son linealmente independientes ya que aqui tenemos los $u_k$ hasta $m$ y lo $u_j$ desde el $m+1$ hasta el $s$ y son en efecto la base del subespacio vectorial $G$ y si solo tenemos una combinacion lineal de elementos de una base igualada a cero, como los elementos o la base son linealmente independientes todos los escalares que aparecen en esta combinacion lineal deberian ser ceros.

Asi que esto implicaria que $\delta_i=0$ para $i=1,...,m$ pero tambien $\beta_j=0$ para $j=m+1,...,s$ esto significa que cuando tomemos la expresion que teniamos anteriormente 
$$\sum_{i=0}^{r}\alpha_iu_i=-\sum_{j=m+1}^{s}\beta_jv_j\ \in F\cap G$$

El lado de la derecha del igual: $$-\sum_{j=m+1}^{s}\beta_jv_j$$

todos los $\beta_i=0$ asi que nos quedaria: $\sum_{i=0}^{r}\alpha_iu_i=0$ pero ahora podemos repetir el argumento, todos los $u_i$ que aparecen en esta zona son una base de $F$ y si son una base de $F$ son linealmente independientes y si son linealmente independientes y tenemos una combinacion lineal suya igual a cero, todos los escalares tambien son nulos. asi que en efecto los $\alpha_i=0$ para los $i=1,...,r$ o en otras palabras como todos los $i$ desde $1$ hasta $r$ son ceros y los $\beta_j$ tambien son ceros. Llegamos a la conclusion que queriamos demostrar.

Que la combinacion lineal de el elemento que habiamos elegido de $\sum_{i=0}^{r}\alpha_iu_i+\sum_{j=m+1}^{s}\beta_jv_j=0$ que simplemente una combinacion lineal de todos los vectores $u_1,...,u_m,u_{m+1},...,u_r,v_{m+1},...,v_s$ igualada a cero da como resultado que todos los escalares son igual a cero, eso significa que los $u_i$ desde $i=1,...,r$ con los $u_j$ desde $j=m+1,...,S$. Son un conjunto linealmente independiente o que en otras palabras se verifica que la dimension de $dim (F+G)$ es la dimension de $F$ mas la dimension de $G$ menos la dimension de la interseccion.
Porque el numero de elementos del que forma parte de esta base de $F+G$ tiene exactamente $r+s-m$ (donde $r=dim(f)$,$m=dim(F\cap G)$,$s=dim(G)$) los elementos fijensen que si la suma es directa., La interseccion es el espacio vectorial trivial (el cero) y que por tanto la dimension de la suma directa de $F+G$ seria la dimension de $F$ mas la dimension de $G$ esta $dim(F\cap G)=0$ en el caso de la suma
directa, que en consecuencia se podria demostrar que son equivalentes decir que $F$ y $G$ son complementarios y decir que $F$ interseccion $G$ es cero y que la dimension de $E$ es igual a la dimension de $F$ mas dimension de $G$, simplemente es aplicacion directa de la formula de Grassmann en este caso y como consecuencia

Segun un corolario que podriamos demostrar es que si $E\in \mathbb{K}e.v$ de dimension
finita cualquier subespacio vectorial de $F$ siempre admite al menos un complementario y esto es aplicacion directa de la formula de Grassmann porque si consideramos que $dim(E)=n$ y que la dimension de cualquier subespacio vectorial suyo $dim(F)=m$ entonces esta claro que $m$ siempre sera menor igual que $n$ porque $F$ es de su espacio vectorial.
Si llamamos a $u_1,u_2,...,u_m$ son una base de $F$, el teorema de Steinitz nos permite completar esta base de $F$ hasta obtener una base del espacio vectorial $E$.
Por tanto yo podria tomar $u_1,u_2,...,u_m$ y complementar con nuevos vectores $u_{m+1},u_{m+2},...,u_n$ entonces los nuevos que se añadieron para completar una base del espacio vectorial grande. Forman un espacio vectorial, si tomamos el espacio vectorial generado $G=\langle  u_{m+1},u_{m+2},...,u_n\rangle$ entonces este conjunto es un espacio vectorial pero no uno cualquiera sino que es el complementario o un complementario de $F$ o sea que la suma directa $F\oplus G$  es todo el espacio vectorial E.

# Construcción de espacios vectoriales

¿Cómo construimos nuevos espacios vectoriales a partir de otros conocidos? (Polinomios, Vectores, Funciones y matrices)

Esto es lo que veremos a lo largo de este apartado

## Espacio vectorial producto

**Espacio vectorial producto.** Sean $E,F$ dos $\mathbb{K}$-espacios vectoriales (en primera no importa la dimencion que sea lo que importa es que sean sobre el mismo cuerpo). Definimos sobre el conjunto producto cartesiano $E\times F$ las siguientes operaciones: (son pares de coordenadas con un elemnto de $E$ y uno de $F$)

$$(u,v)+(u',v') = (u+u',v+v')$$, esta es la operacion suma de modo que la tupla donde $u$ es un elemnto de $E$ y $v$ uno de $F$, esto es como sumar vectores pero ahora son elementos de espacios vectoriales diferentes, es decir para definir la suma de estos vectores se define atravez de las sumas de el espacio vectorial $E$ y $F$, cuando vemos el $+$ en la segunda igualacion, este $u+u'$ es el de $E$ por tanto estamos sumando objetos de $E$, podria ser que esas $u$ sean vectores y al mismo tiempo en la suma de $v+v'$ de $F$ estamos sumando funciones o matrices que son cosas mas generales
$$\alpha\cdot(u,v) = (\alpha\cdot u,\alpha\cdot v)$$, este es el producto de un escalar por la tupla $(u,v)$ que se puede hacer porque el producto de un escalar tanto en $E$ como en $F$ esta definido

donde $u,u'\in E$, $v,v'\in F$ y $\alpha\in\mathbb{K}$. Es inmediato ver que con estas operaciones, el conjunto (El producto carteciano con la suma y el producto)$(E\times F,+,\cdot)$ es un $\mathbb{K}$-e.v. Es llamado **espacio vectorial producto** o **espacio vectorial suma directa de $E$ y $F$**. 

Lo denotaremos habitualmente por $E\oplus F$ y como es diferente el espacio vectoriales producto de la suma directa

**Observación.** El elemento neutro del $\mathbb{K}$-e.v. será $(0,0)$, donde el primer $0$ es el elemento neutro de $E$, mientras que el segundo, es el elemento neutro de $F$

De forma análoga, el opuesto de cualquier elemento $(u,v)\in E\oplus F$ será $(-u,-v)\in E\oplus F$, tambien porque $-u$ pertenecen a $E$ y $-v$ pertenece a $F$, de forma natural todas las propiedades que eran validas para el espacio vectorial $E$ y $F$ tambien lo es para el producto carteciano o suma directa de $E$ con $F$

**¡Atención!** Anteriormente hemos hablado de suma directa de **sub**espacios vectoriales y ahora de suma directa de **espacios** vectoriales. Ambos se denotan del mismo modo, $\oplus$.

Así pues, dados dos subespacios vectoriales $F,G$ de un $\mathbb{K}$-e.v (que sean subespacio del mismo espacio vectorial $E$), tenemos que $F,G$ pueden ser considerados también como $\mathbb{K}$-e.v. y, por tanto $F\oplus G$ denota a la vez dos objetos inicialmente diferentes:

- Suma directa de objetos subespacios vectoriales de $E$: ($F$ y $G$ subespacio espacio vectorial de $E$) se define como los elemntos del espacio grande tal que se pued escribir como suma de un elemnto de $F$ y otro elemnto de $G$ (es lo que esta en medio de la tansparencia)
$$F\oplus G = \{z\in E\ |\ z = x+y\ \text{ para ciertos }x\in F,\ y\in G\}$$
- Suma directa como $\mathbb{K}$-espacios vectoriales: (se denota que $F$ y $G$ el producto carteciano o la suma directa o el espacio vetorial producto) estos no son como subespacio de uno mas grande si no como ellos mismos espacios vectoriales, se define como las tuplas (los pares) $(x,y)$ de elemntos tales que la primera coordenada es de $F$ y la segunda es de $G$, **PERO** ni $x$ ni $y$ tienen por que ser numeros , podrian ser vectores,matrices,funciones
$$F\oplus G = \{(x,y)\ |\ x\in F,\ y\in G\}$$

Lo indicado anteriormente no lleva a ninguna confusión porque, como veremos más adelante, los dos conjuntos corresponden a $\mathbb{K}$-espacios vectoriales isomorfos, es decir, (tanto la definicion de arriba como la de abajo) estas dos sumas directas van a ser iguales. el isomorfimo basicamente es identificables desde el punto de vista de la estructura de espacio vectorial que manejamos, es decir que tanto nos va a sumar elementos en la suma directa de subespacios como sumarlos en la suma directa o espacio vectoriales o multiplicar por escalares, escencialmente el resultado y la estructura de espacio vectorial que manejamos es la misma

La definición de espacio producto o espacio suma directa se puede generalizar a $n$ sumandos:

**Espacio vectorial producto.** (Formado a apartir de $n$ espacios vectoriales cualesquiera) Sean $E_1,\dots,E_n$ $\mathbb{K}$-e.v. cualesquiera. Definimos el $\mathbb{K}$-espacio vectorial producto o suma directa de $E_1,\dots,E_n$ como $$E_1\oplus\cdots\oplus E_n = \{(\vec{u}_1,\dots,\vec{u}_n)\ |\ \vec{u}_i\in E_i,\ \text{ para }i= 1,\dots,n\}$$. como la suma directa hasta $E_n$ serian las tuplas de $n$ coordenadas, donde la primera coordenada es un vector del primer espacio vectorial, la segunda del segundo espacio vectorial, el tercer vector del tercer espacio vectorial y asi sucecivamente hasta la n-esima que seria un vector del ultimo espacio vectorial

Con las operaciones suma y producto por escalares definidas componente a componente:
(La suma de estos dos vectores pero de ese mismo espacio producto se sumara de componente a componente)
$$(\vec{u}_1,\dots,\vec{u}_n)+(\vec{v}_1,\dots,\vec{v}_n) = (\vec{u}_1+\vec{v}_1,\dots,\vec{u}_n+\vec{v}_n)$$
$$\alpha\cdot(\vec{u}_1,\dots,\vec{u}_n) = (\alpha\cdot \vec{u}_1,\dots,\alpha\cdot\vec{u}_n)$$, y el producto por escalar es del mismo modo, un escalar por toda una tupla,sera el escalar por el primer vector como sea que este definida en ese espacio vectorial, el escalar por el segundo elemnto, el escalar por el tercero y asi sucecivamente, hasta el escalar por el ultimo elemento, los $u_n$ son elementos respectivos de cada uno de los espacios vectoriales 

donde $\vec{u}_i,\vec{v}_i\in E_i,\ \forall i = 1,\dots, n$ y $\alpha\in\mathbb{K}$ los escalares $\alpha$ pertenecen al cuerpo $K$ que comparten todos y cada uno de los espacios vectoriales, todos estan construidos sobre el mismo cuerpo $K$

**Ejemplo 12**

De la definición anterior, deducimos que el $\mathbb{K}$-espacio vectorial $\mathbb{K}^n$ lo podemos ver como la siguiente suma directa $$\mathbb{K}^n = \mathbb{K}\oplus\cdots\oplus\mathbb{K}$$, es verdad como $\mathbb{R}^2$ es lasuma directa de $\mathbb{R}$ con $\mathbb{R}$ porque se escriben pares de coordenadas entre parentesis dos numeros reales que eso es la suma directa de $\mathbb{R}$ con $\mathbb{R}$ con esta defincion, Lo elevado que esta $\mathbb{R}$ son las veses que se esta sumando directamente de espacios vectoriales asi que en general es en $n$ veses

**Proposición.** Sean $E,F,E_1,\dots,E_n$ $\mathbb{K}$-e.v. (Todos construidos sobre el mismo cuerpo $K$)

- Si $E,F$ son de dimensión finita, entonces $E\oplus F$ también lo es y $$\dim(E\oplus F) = \dim(E)+\dim(F)$$, seimpre y cuando ambos sean de dimencion finita
- Si todos los $E_i$ son de dimensión finita y si llamamos $\dim(E_i) = n_i\ \forall i=1,\dots,n$, entonces la suma directa $E_1\oplus\cdots\oplus E_n$ también es de dimensión finita y se cumple que la dimencion de las sumas directas hasta $E_n$  es la suma de las dimenciones es decir hasta la diemncion de $E_n$ que es igual al sumatorio de los $n_i$ que son la dimencion de cada espacio vectorial

$$\dim(E_1\oplus\cdots\oplus E_n)=\dim(E_1)+\cdots+\dim(E_n) = \sum_{i = 1}^n n_i$$

Esta operacion de la suma directa se lleva bien con las dimenciones pues la dimencion de la suma directa es la suma de numeros reales de dimencion

**Ejercicio 21.** Demostrar formalmente esta Proposición.

Vamos a demostrar la proposición acerca del espacio vectorial producto. Teníamos dos apartados:

el primero es que si $E$ y $F$ son dos espacios vectoriales de dimensión finita, entonces $E\oplus F$ también lo es y además $dim(E\oplus F)=dim(E)+dim(F)$

La segunda es la generalización es decir si tenemos $E_i$ una familia finita de espacios vectoriales, todos ellos de dimensión finita, entonces la suma directa de todos ellos también lo es y la dimensión del espacio vectorial suma directa es la suma de las dimensiones de  cada subespacio vectorial

Como cumplen que son finitos

Demostracion de la **primera parte**, si llamamos a cada espacio vectorial $n=dim(E)$ y $m=dim(F)$ de aquí podemos extraer dos bases: $u_1,u_2,...,u_n$ que es base de $E$ y $v_1,v_2,...,v_m$ que es base de $F$. Queremos demostrar que la dimensión 
$dim(E\oplus F)=n+m$, para lo cual bastará ver que los vectores de la forma $(u_1,0),(u_2,0),...,(u_n,0)$ con $(v_1,0),(v_2,0),...,(v_m,0)$ son linealmente independientes y que forman un sistema generador del subespacio vectorial $E\oplus F$

En otras palabras queremos demostrar que este conjunto es una base de la suma directa de $E\oplus F$ veamos primero que son linealmente independientes, para demostrar que los elementos son lineealmente independientes hay que considerar una combinación lineal de estos elementos e igualarla a cero. Por tanto si hacemos una combinación lineal (con los primeros vectores $u_i$) y sumandolo con una combinación lineal de la segunda parte ($v_i$)
$$\sum_{i=1}^{n}\alpha_i(u_i,0)+\sum_{j=1}^{0}\beta_j(0,v_j)=(0,0)$$

Igual al vector cero que es el que no tiene ningun componente en la primera parte del espacio vectorial $E$ y que tiene componente cero también en el espacio vectorial $F$, quieremos ver qué tanto los $\alpha_i$ como los $\beta_i$ son cero.

A partir de esto esta combinación lineal se puede escribir como el vector suma

$$\big(\sum_{i=1}^{n}\alpha_i(u_i,0),\sum_{j=1}^{0}\beta_j(0,v_j)\big)=(0,0)$$

En este caso tambien lo que es lo mismo se podría escribir el sistema de ecuaciones lineal, cuando estan igaladasa a cero es el vector 0

$$\sum_{i=1}^{n}\alpha_iu_i=0,\sum_{j=1}^{0}\beta_jv_j=0$$
Los $u_i$ y $v_j$ son linealmente independientes porque son respectivamente una base de $E$ y una base de $F$, Así que dada esta combinación lineal todos $\alpha_i=0$ y todos los $\beta_j=0$, entonces el conjunto formado por los $(u_i,0)$ y los $(0,v_j)$ son linealmente independientes porque la combinación lineal resulta en todos los escalares iguales a 0.

Falta demostrar que generan todo el espacio vectorial suma directa. En este caso son un sistema generador: cualquier elemento de $E$ suma directa $F$, si tomamos un $w$ de la suma directa con $F$: $w\in E\oplus F$, Este $w$ Va a ser de la forma: $\vec{w}=(u,v)$ será una parte de $E$ y una parte de $F$ y evidentemente $u\in E$ y $v\in F$, por ser los $u_i$ una base de $E$ y los $v_j$ una base de $F$ existirán una serie de escalares llamémoslos $\alpha_i$ y $\beta_j$ respectivamente del cuerpo $\mathbb{K}$: $\exists\alpha_i,\beta_j\in\mathbb{K}$ tal que el $u_i$ y $v_j$ puede escribir como combinación lineal:
$$\vec{u}=\sum_{i=1}^{n}\alpha_iu_i,\ \vec{v}=\sum_{j=1}^{m}\beta_jv_j$$
Por tanto nuestro vector $w$ que habíamos tomado del espacio vectorial suma y que es $\vec{w}=(\vec{u},\vec{v})$ se podrá escribir más detalladamente como:
$$\vec{w}=(\vec{u},\vec{v})=\left(\sum_{i=1}^{n}\alpha_iu_i,\sum_{j=1}^{m}\beta_jv_j\right)$$
pero a su vez si lo desplegamos en suma de dos vectores:
$$\left(\sum_{i=1}^{n}\alpha_iu_i,0\right)+\left(0,\sum_{j=1}^{m}\beta_jv_j\right)$$

pero a su vez por linealidad esto sería el vector sumatorio:
$$\sum_{i=1}^{n}\alpha_i(u_i,0),\ \sum_{j=1}^{m}\beta_j(0,v_j)$$

Así que en conclusión hemos tomado un vector que pertenecía a la suma directa de $E$ con $F$ y lo se ah podido expresar como combinación lineal de los vectores $(u_i,0)$ y $(0,v_j)$, En efecto el espacio vectorial suma directa $E\oplus F$ es un espacio vectorial que está generado cualquier elemento se puede escribir como combinación lineal: $E\oplus F=\langle\{(u_i,0),(0,v_j)\} \rangle$, Entonces son un sistema generador y son un sistema linealmente independiente por tanto es una base y se cumple que una base del espacio vectorial suma directa $E\oplus F$ tiene el mismo número de elementos que, los que tuviera una base de $E$ más los que tuviera una base de $F$.

En otras palabras se cumple la fórmula que habíamos dicho, que es $dim(E\oplus F)=dim(E)+dim(F)$.

De todas las operaaciones aritmeticas, para dividir un espacio vectorial con otra es lo que se llama el espacio vectorail cosiente se tiene que hablar de **relaciones de equivalencia** o tambien llamado la **relacion modulo F**

## Espacio vectorial cociente

**Relación módulo $F$.** Sean $E$ un $\mathbb{K}$-espacio vectorial (Podemos pensar en $\mathbb{R}^n$, polinomios hasta cierto grado,Matrices) y $F$ un subespacio vectorial de $E$ cualquiera. Definimos sobre $E$ la siguiente relación llamada relación módulo $F$.

Definimos que dos elementos del espacio vectorial $E$, $x$ e $y$ se relacionan a travez de $F$ y esto se indica asi: $x\sim_{F}y$, si y solo si la resta de $x-y$ pertenece a $F$

$$x\sim_{F}y \Leftrightarrow x-y\in F$$
Por decir que de todos los vectores que vivirian en el espacio buscamos aquellos pares de vectores tales que cuando los restamos pertenecen a $F$, fijensen que ni $x$ o $y$ tienen porque pertencer a $F$ simplemente estos pertenecen a un espacio vectorial generico $E$ y tienen la propiedad que se relacionan (son semejantes o parecidos) si su resta cae dentro de $F$

**La IDEA es **que buscamos vectores tales que restados se agrupen dentro de $F$, $x$ e $y$ podrian no pertencer a $F$ pero cuando los restamos si que pertenecen a $F$ a ese subespacio vectorial

La relación definida anteriormente sobre $E$ es siempre una relación de equivalencia cualquiera que sea el subespacio vectorial $F$ ya que (que sea una relacion de equivalencia significa que cumple tres propiedades)

- **Reflexiva**: Todo elemento se relaciona consigo mismo es decir que todo $x$ $\forall x\in E$, tenemos que se relaciona atravez de $x\sim_F x$ (con el propio $x$), estos es trivial porque $x$ se relaciona con $x$ a travez de $F$ ya que $x-x = 0\in F$ por ser $F$ subespacio vectorial, Porque esta resta nos da el vector cero y este pertence a cualquier subespacio vectorial por propia definicion 
- **Simétrica**: (Fijensen en la tilde) $\forall x,y\in E$, si $x\sim_F y$, tambien es $y\sim_F x$, fijaros que si $x$ se relaciona con $y$ atravez de la relacion de pertenecer a $F$ tenemos que $x-y\in F$ pero como esta resta pertence a $F$ como $F$ es un subespacio vectorial y por tanto tiene su opuesto que también pertenece a $F$ que si simplificamos el opueso es $y-x\in F$ y esto Es decir lo mismo que tenemos $y\sim_F x$ ($y$ se relaciona con $x$ atravez de la relacion de pertenecia del subespacio $F$), si un elemento se relaciona con el otro, el otro tambien se relaciona con el primero es una relacion simetrica
- **Trasitiva**: Si tenemos elementos del espacio vectorial $x,y,z\in E$ tales que $x\sim_F y$ ($x$ se relaciona con $y$) y $y\sim_F z$ ($y$ se relaciona con $z$), entonces ($x$ se relaciona con $z$) y esto es trivial porque el hecho que la $x$ se relacione con $y$ significa que las siguentes restas pertenescan a $F$, $x-y,y-z\in F$. Por tanto si estas restas son de $F$, su suma también es de $F$, es decir, $$(x-y)+(y-z) = x-z\in F\Leftrightarrow x\sim_F z$$, despues de simplificar y tachar la $y$ nos queda que $x-z$ pertenece a $F$ quiere ecir que $x$ se relaciona en $F$ con $z$

Despues de analizar estas tres podemos decir que la tilde es una **relacion de equivalencia** y de esta se puede considerar seimpre el conjunto cociente

De esta manera podemos considerar el **conjunto cociente**, denotado como $E/F$ formado por todas las clases de equivalencia módulo $F$ quiere decir todas las clases que son escencialmente diferentes

**Clase de equivalencia módulo $F$**. Dado $x\in E$, para calcular su clase de equivalencia módulo $F$ la denotamos por $[x]_F$ (corchete sub $F$ del elemnto $x$) y viene dada por todos los elementos del espacio vectorial $E$ que se relacionan con dicho $x$, es decir que se trata de buscar todos los elemntos $y$ que pertenencen a espacio vectorial de partida que es el $E$ tal que la resta de $y-x$ da un elemnto $z$ que pertenece a $F$.

$$[x]_F = \{y\in E\ |\ y\sim_F x\} = \{y\in E\ |\ y-x = z\in F\}$$ $$=\{y\in E\ |\ y = x+z,\ z\in F \} = \{x+z\ |\ z\in F\} = x+F$$

Que $y-x$ sea un elemento $z$ que pertenece a $F$ es como decir que $y$ se puede escribir $x+z$, tal que $z$ es un elemento de $F$ o en otras palabras que esos elemntos son de la forma $x+z$ con $z$ un elemnto del subespacio $F$ por eso muchas veces la clases de equivalencia modulo $F$ se suelen escribir como $x+F$ donde $x$ es el propio elemento, Aqui **estamos sumando un elemento con todo un espacio vectorial  **, pero $x+F$ es una forma de indicar que a un elemnto $x$ del espacio vectorial $E$ le sumamos cualquier vector del subespacio vectorial $F$, asi que las clases de equivalencia de cualquier elemento del espacio vectorial $E$ se pueden escribir como diho elemento mas cualquier otro vector del subespacio $F$ con el que elaboramos la relacion de equivalencia

**Observación.** La clase del $0$ coincide con el propio subespacio $F$, con lo que acabamos de decir la clase del $0$ seria $0+z$ tal que $z$ sea un elemnto del subespacio $F$, pero $0+z=z$ entonces $z$ es un elemnto de $F$, asi que la clase del $0$ es el subespacio vectorial $F$

$$[0]_F = \{0+z\ |\ z\in F\} = F$$

De hecho, más generalmente tenemos: La clase de $x$ en la relacion de equivalencia coincide con la clase del $0$ si y solo si $x$ se relaciona con $0$ a travez de a relacion de equivalencia pero que $x$ se relaciona con $0$, significa que $x-0$ pertenece a $F$, y como esto pertenece a $F$ es lo mismo que decir que $x$ pertenece a $F$ asi que en estos casos resulta que la clase de equivalencia de $x$ donde $x$ es ya un vector de $F$ coincide con todo el subespacio vectorial $F$ asi que la clase de un elemento $x$ que pertenesca al subespacio $F$ conincide con el propio subespacio vectorial $F$

$$[x]_F = [0]_F\Leftrightarrow x\sim_F0\Leftrightarrow x\in F\ \text{y en estos casos }[x]_F = F$$

Estas clases de equivalencia se denominan **variedades lineales** 

**Variedad lineal.** Es la suma de un vector y un subespacio vectorial, ejemplo un elemento $x$ mas el subespacio vectorial $F$

Como hemos visto, las variedades lineales son una exprecion de la forma vector mas subespacio y solamente son subespacios vectoriales en si misma cuando $x\in F$, o equivalentemente, cuando la variedad contiene el elemento $0\in E$, coincidiendo en estos casos con el propio subespacio $F$.

Es decir que las Variedades Lineales son una especi de pseudosubespacio vectorial porque no son subespacio vectoriales en si solo lo podrian ser si la variedad lineal contiene un elemnto $x$ del subespacio $F$ o lo que es lo mismo si contienen el $0$, son menos que un subespacio vectorial pero aun asi es interesante estudiar las variedades lineales

Dentro del conjunto cociente (que basicamente esta formado por todas las clases de equibalencias, cualquier clase corchete $x$ gracias a la relacion de equivalencia de $F$) $E/F = \{[x]_F\ |\ x\in E\}$ podemos definir las siguientes operaciones dentro de las clases de equivalencia, a través de sus representantes:

$$[u]_F +[v]_F = [u+v]_F$$
sumar la clase de equivalencia del elemnento $u$ mas la clase de equivlencia del elemento $v$ es lo mismo que definir la clase de equivalencia del elemento $u+v$ en $F$, fijensen que $u$ y $v$ son vectoes del espacio vectorial $E$ y portanto la suma que tenemos $[u+v]$ es la suma vectorial de $E$, asi que sumar clases se define como sumar los representantes de las clases

$$\alpha\cdot[u]_F = [\alpha\cdot u]_F$$

Multiplicar $\alpha$ por una clase de equivalencia es la clase de equivalencia de $\alpha$ por el vector $u$

Estas anteriores operaciones podrian estar mal definidas, dentro de una clase de equivalencia podria haber muchos elementos, cualquiera de los elemntos que viven dentro de una clase de equivalencia mas cualquiera de los otros elementos  que viven en otra clase de equivalencia podriamos verlo que sumamos dos elemntos cualquiera de una clase de equivalencia,les hacemos la suma en $E$ y ¿Al aplicar la clase esto simepre nos va a dar la misma?, **SI** ya que las operaciones estan bien definidas

Veamos que las operaciones anteriores están bien definidas. En otras palabras, comprobemos que la suma y el producto por un escalar no dependen del representante elegido:

- **SUMA**: Supongamos que (La clase de cierto elemento $x$del espacio vecctorial $E$ es igual a la clase de $x$ prima(siendo otro elemnto de $E$), cunado decimos clase nos referimos a la misma relacion de equivalencia) $[x]_F = [x']_F$ y que $[y]_F = [y']_F$, en otras palabras $x$ y $x'$ pertenecen a la misma clase de equivalencia e $y$ y $y'$ igualmente, son dos represntantes de la clase de equivalencia. Queremos ver que si sumamos a través de los representantes $x,y$ o $x',y'$, el resultado es el mismo. Como era de esperar:

Que la clase de $x$ sea la misma que la clase de $x'$ con la relacion modulo $F$ quiere decir que $x$ se relaciona con $x'$ y esto quiere decir que $x-x'$ pertenece a $F$

$$[x]_F = [x']_F\Leftrightarrow x\sim_F x'\Leftrightarrow x-x'\in F$$
Del mismo modo que la clase de $y$ sea igual a la clase de $y'$, significa que $y$ es equivalente a $y'$ o que en otras palabras $y-y'$ pertence a $F$

$$[y]_F = [y']_F\Leftrightarrow y\sim_F y'\Leftrightarrow y-y'\in F$$ 

Ests ultimas restas son vectores de $F$ y consecuentemente sumando $(x-x')+(y-y')$ obtenemos que es un vector de $F$, la cual si movemos parentesisi, esta operacion $x+y-(x'+y')$ significa que $(x+y)$ se relaciona con $(x'+y')$, recordemos que $x$ se relaciona con $y$ si y solo si la diferencia es un vector de $F$, aqui tenemos la diferencia de dos vectores $x+y-(x'+y')$, que la diferencia pertenesca a $F$ significa que $(x+y)$ se relaciona $(x'+y')$ con la relacion de equivalencia modulo $F$ o en otras palabras que la clase $[x+y]$ modulo $F$ es la misma que la clase $[x'+y']$, entonces para la suma esta bien definida no depende del represante que eligamos, cuendo tenemos una clase de equivalencia si sumamos a travez de cualquiera de sus elementos la operacion esta bien definida

$$x+y-(x'+y')\in F\Leftrightarrow (x+y)\sim_F(x'+y')\Leftrightarrow [x+y]_F = [x'+y']_F$$

- **PRODUCTO por escalares**: De forma similar, si $\alpha\in\mathbb{K}$ y $[x]_F = [x']_F$, tenemos que $x-x'\in F$ y, entonces, si tenemos $\alpha\cdot(x-x')$ tambien pertenecera a $F$ porque como $F$ es un subespacio vectorial si $x-x'$ es un vector de $F$ si multiplico por $\alpha$ un escalar del cuerpo tambien seguira siendo de $F$, y sacando mas claro el producto que $\alpha\cdot x-\alpha\cdot x'$ la resta de estos dos vectores sea de $F$ significa que la clase de equivalencia del primero $[\alpha\cdot x]_F$ es igual a la clase de equivalencia del segundo $[\alpha\cdot x']_F$

$$\alpha\cdot(x-x')\in F\Leftrightarrow \alpha\cdot x-\alpha\cdot x'\in F\Leftrightarrow [\alpha\cdot x]_F = [\alpha\cdot x']_F$$

Con todo lo visto hasta el momento, es fácil ver que el conjunto cociente (donde viven todas las clases de equivalencia de la relacion modulo $F$) $E/F$ junto con estas operaciones de suma y producto por un escalar son un $\mathbb{K}$-espacio vectorial:

**Espacio vectorial cociente.** Sea $E$ un $\mathbb{K}$-e.v. y $F$ un sub-e.v. de $E$ cualquiera. Definimos el espacio vectorial cociente de $E$ por $F$ (con las operaciones de suma y producto de clases, como se han visto, y estan aqui abajo) al $\mathbb{K}$-espacio vectorial dado por $(E/F,+,\cdot)$ con las operaciones

$$[u]_F +[v]_F = [u+v]_F$$
$$\alpha\cdot[u]_F = [\alpha\cdot u]_F$$

donde $\alpha\in\mathbb{K}$, $u,v\in E$

Dentro de espacio vectorial salen unos resultados muy interesantes

**Proposición.** Sea $E$ un $\mathbb{K}$-e.v. de dimensión finita, $\dim(E) = n$ y sea $F$ un subespacio vectorial. Entonces, ($E$ cociente $F$) $E/F$ es también de dimensión finita y la dimenciond del espacio cociente es la dimencion del grande menos la dimencion del pequeño, es decir la dimencion del espacio vectorial cociente es la diferencia de dimenciones del espacio grande y el subespacio con el que hacemos cociente

$$\dim(E/F) = \dim(E)-\dim(F)$$

**Ejercicio 22.** Demostrar formalmente esta Proposición.

## Ejemplo de Espacio Vectorial Cociente

El Ejemplo de dividir o hacer el cociente de dos espacios vectoriales, recuerda que el espacio vectorial cociente, se monta de un espacio vectorial sobre un subespacio de $E$ cualquiera, de modo que si tenemos $F$ un subespacio vectorial de $E$ se define el espacio vectorial cociente $E/F$ como el conjunto de las clases de equivalencia de $X$ con la relación de equivalencia tilde: $[x]\sim$, tal que $x\in E$ y la relación tilde es una relación de equivalencia de modo que dos vectores se relacionan, $x$ se relaciona con $y$ si y sólo si la resta, $x$ menos $y$, pertenece a $F$: $x\sim y \Leftrightarrow x-y\in F$.
Esta cosa de aquí es bastante abstracta.

Empecemos por el $\mathbb{R}$ espacio vectorial que todos conocéis.

$\mathbb{R}^2$ (el plano) que es $_{\mathbb{R}-e.v}$ y sea $F$ el subespacio vectorial generado por un vector $F$ sobre $\mathbb{R}^2$ donde $F$ es un vector no nulo: $F=\langle v \rangle\ v\in\mathbb{R}^2$, esto si dibujamos el generado por un vector (Simplemente sumar ese vector o combinaciones lineales suyas) nos saldría una recta de modo que sobre el plano $\mathbb{R}^2$ **(Imagine un plano carteciano, con el vector $v$ en el centro apuntado su flecha al lado I)**. Si tomamos $v$ un vector cualquiera, el espacio vectorial $F$ es ni más ni menos que la recta definida por el vector $v$ esto sería con el subespacio vectorial $F$ **cruzando todo el plano en la direccion del vector $v$ (Arriba y abajo) en diagonal**, entonces quien sería el espacio vectorial cociente de $\mathbb{R}^2$ (módulo o equivalencia): $\mathbb{R}^2/F$. Serían los dos vectores que estarían en la misma clase de equivalencia, si su resta pertenece a F. 

Dibujo **Imaginario** para aclarar más: 

Tenemoas un plano carteciano $\mathbb{R}^2$ y sobre este vamos a tener un vector $v$ **Inciando en el centro y alaragnadoce haci: '/' (Con la fecha apuntado arriba) en la parte I del plano** evidentemente el subespacio vectorial $F$ es el generado por $v$, así que hay que (alargar) hacer combinaciones lineales de ese vector, hasta obtener el espacio vectorial $F$ que sería **El que esta igual que el vector $v$, asi: '/' en diagonal pero alargado, partiendo todo el plano en la mitad** es decir el subespacio vectorial $F$ de $\mathbb{R}^2$ sería la recta definida por el vector director $v$. Sin embargo ahora las clases de equivalencia o de qué formas sería el cociente de $\mathbb{R}^2$ módulo $F$: $\mathbb{R}^2/F$. Porque habría que buscar parejas de vectores tales que al retarlos su resta perteneca a $F$.

Así que habría que buscar dos vectores cualesquiera.**(Se dibuja en el plano en diagonal asi '/' (Con la fecha apuntado arriba) pero mas parado que el vector $v$ )** habría que buscar otro tal que al rescatarlo con este que pusimos nos dé la misma dirección que $v$, Así que habría que buscar otro **Se dibuja otro vector entre el que pusimos arriba y el vector $v$ solo que este es mas alargado** tal que cuando lo restamos la dirección que da es paralela a $V$ (misma dirección).  Es la felcha de colo rojo de amero arriba

<img src="images/planoDireccionV.png" alt="Plano R2"/>

Así que los dos vectores de color negro pertenecerían básicamente a la misma clase de equivalencia. En otras palabras nos quedaran rectas paralelas a $F$.

Podemos elegir aquí una recta que sea paralela a $F$ y cualquier dos puntos de aquí **(Son esas flechas rojas apuntando al parelo de abajo)** cuando se hiciera la resta, el vector que nos quedaría siempre sería paralelo a $F$. **Es la felcha roja que para por la linea paralela**

<img src="images/paraleladeR2.png" alt="Plano Paraleleo R2"/>

Así que las clases de equivalencia para este caso serían rectas paralelas a $F$.

Vamos a demostrarlo.

Si tomamos la clase de equivalencia de un punto o un vector $[(a,b)]$ cualquiera de $\mathbb{R}^2$ está claro que la clase de equivalencia lo que se solía utilizar es esta expresión $x\sim y \Leftrightarrow x-y\in F$ y es que dos elementos se relacionan o pertenecen a la misma clase, si su diferencia pertenece a $F$ es decir que la clase de equivalencia de un punto $[(a,b)]$ se podría despejar de esa expresion y decir que es:
$$[(a,b)]=(a,b)+F$$
sería la suma de un vector más el espacio vectorial $F$. Esto seria el punto o el vector $(a,b)$ más $\lambda$ veces la dirección $v$: $\{ (a,b)+\lambda\vec{v},\lambda\in\mathbb{R}$, y precisamente $v$ es el vector que define el subespacio vectorial $F$. Así que con un $\lambda$ cualquiera nosotros podríamos decir que una clase de equivalencia de un vector $(a,b)$ es la recta que pasa por $(a,b)$ y tienen la misma dirección que la recta o el espacio vectorial $F$, (tiene la dirección de $v$), cuando pasa por $(0,0)$ tendríamos el propio subespacio vectorial $F$, la clase seria: $[(0,0)]=(0,0)+F$ o lo que es lo mismo el propio subespacio vectorial $F$. O si perteneciera a la clase de algún elemento que ya fuera de F, cualquier elemento que ya es de $F$ más $F$ también daría $F$ así que cualquier vector sobre la recta de F su clase de equivalencia sería el propio subespacio vectorial $F$. Si no el resultado es el conjunto cociente o el espacio vectorial cociente de $\mathbb{R}^2$ módulo $F$ es el conjunto de todas las rectas paralelas a $F$ con la misma dirección del vector $v$. Así que para cada recta paralela sería una posible clase de equivalencia. Por ejemplo:

<img src="images/puntopunto2.png" alt="Plano Paraleleo R2"/>

este punto marcado por coordenadas sería el $(4,0)$ y esta última recta que se marca aquí en discontinuó podría ser la clase del punto $[(4,0)]$ fijensen que con un solo vector de $\mathbb{R}^2$ obtenemos toda una recta.

Evidentemente en esto de los subespacios o los espacios vectoriales cocientes es bastante complicado para llegar a dominarlos. Hay resultados muy interesantes como por ejemplo los casos extremos para cualquier $\mathbb{k}$ espacio vectorial

Si elegimos para hacer el cociente el subespacio trivial cero $F=\{0\}$. Pues resulta que $E/F$ es lo mismo que hacer $E/\{0\}$ o lo que es lo mismo es que estaría formado por los puntos o los vectores del espacio vectorial $E$ más el subespacio cero con cualquier elemento $x$ que pertenesca al espacio vectorial $E$: $\{x+\{0\},x\in E\}$ esto se resume como simplemente el conjunto de puntos $x$: $\{\{x\},x\in E\}$ Y evidentemente esto es ni más ni menos que $E$, es decir son isomorfos. 

Hacer cociente por el vector cero nos quedamos igual no quitamos nada, por su lado si $F$ fuera el espacio vectorial tota, $F=E$ ,el otro espacio vectorial impropio entonces el cociente sería: $E/E=\{x+E,\ x\in E\}$ y esto es la clase $\{E\}$ es decir todo $E$ es el único elemento de una clase o lo que es lo mismo es el $\mathbb{K}$ espacio vectorial trivial porque sólo tiene un elemento en este caso el propio espacio vectorial.

En particular el resultado que hemos enunciado acerca de los espacios vectoriales cocientes. Nos permite hacer una afirmación muy fuerte que dice que si tenemos un subespacio vectorial generado por una base. Por ejemplo si $F$ es el subespacio vectorial de $\mathbb{R}^3$ definido en este caso por $F$ igual al generado por los vectores:
$F=\langle(1,0,0),(0,1,1)\rangle$ Evidentemente se puede definir el espacio vectorial cociente que sería $\mathbb{R}^3/F$ que sería un plano según la proposición que hemos demostrado en teoría: Si queremos obtener una base del cociente, lo único que tenemos que hacer es completar la base de $F$ hasta tener una base de $\mathbb{R}^3$, En este caso si a los dos vectores de $F$ le añadimos el vector $(0,0,1)$ lo que nos sale es el conjunto de vectores $(1,0,0),(0,1,1),(0,0,1)$ son tres vectores que son linealmente independientes y como son tres en un espacio vectorial de dimensión tres, forman una base, así que como se completo una base de $E$ con el vector $(0,0,1)$ hasta tener una base del espacio sobre el cual hacíamos cociente, resultan dos cosas:

lo primero es que $dim(\mathbb{R}^3/F)=1$ porque la base de este espacio vectorial sería la clase $[(0,0,1)]$ o lo que es lo mismo el conjunto más el subespacio $F$: $(0,0,1)+F$, si lo detallamos las clases serían conjuntos y dentro el punto que elegimos (el vector) $(0,0,1)$ más una combinación lineal de elementos de $F$, así que sería: $\{(0,0,1)+\alpha(1,0,0)+\beta(0,1,1)$ siendo $\alpha$ y $\beta$ numeros reales.

## EJercicio 22

Proposicion que tiene que ver con espacios vectoriales cocientes.

Nos dice que si $E$ se es un espacio vectorial de dimensión finita $dim(E)=n$ y $F$ es un subespacio vectorial de $E$, entonces se cumple que el cociente $E/F$ también es finito y que la dimensión del espacio vectorial cociente $dim(E/F)= dim(E)-dim(F)$

Cuando hacemos cociente por un subespacio vectorial es como si el espacio grande le quitáramos la dimensión del espacio sobre el cual hacemos el cociente, **Vamos a demostrarlo** y para ello nos basaremos en que primero hemos visto el caso del espacio trivial, es decir si tomamos el subespacio trivial que es $dim(F)=0$ entonces resulta que trivialmente: $dim(E/F)=dim(E)$, Porque el cociente de $E$ entre $F$, cuando $F$ es el subespacio trivial, es el propio espacio. Así que vamos a suponer que hacemos cociente no por un subespacio trivial sino que la dimensión de $F$ es un número llamémosle $r$: $dim(F)=r>0$ y supongamos $0<r<m$, fijaros que se pone **menos o estricto que m** porque en el caso precisamente en que la dimensión de $F$ fuera $m$ la fórmula también se verificaría trivialmente, porque hacer cociente de un espacio entre sí mismo da como resultado el subespacio trivial, así que tanto si $dim(F)=0$ como $dim(F)=m$ se verifica trivialmente la fórmula.

Si es así, $F$ es un subespacio vectorial de dimensión $r$, consideremos: $\{u_1,u_2,...,u_r\}$ qu es una base de $F$, como es un subespacio de dimensión $r$ tendrá $r$ vectores que formará una base y supongamos que la completamos con el teorema de Steinitz hasta conseguir una base del espacio vectorial $E$.

De modo que esta base es completada, del $u_1,...,u_r$ son los de $F$. Pero ahora la completamos $u_{r+1},...,u_m$ para obtener una base del espacio vectorial $E$.

Resulta que los $u_i\in F$ son los $i=1,...,r$, para esos $u_i$ que pertenecen a $F$, Hay que fijarse que la clase del vector de la base $[u_i]$ será precisamente igual a la clase de $[0]$ ( ya sabes que si un vector es del subespacio, en este caso pertenecen los dos a la clase de cero) mientras que los $i=r+1,...,n$ tienen la particularidad que no pertenecen a $F$, estos $u_i\not\in F$ porque no lo generan (no son base), por tanto la clase de $[u_i]$ no puede ser la clase del $[0]$.

Veamos que estas últimas las que van $i=r+1,...,n$ forman una base del cociente, entonces el conjunto formado por las clases $\{[u_{r+1}],...,[u_n]\}$ son una base de $E/F$ asi habrá quedado demostrado porque tendremos $n-r$ vectores formando una base del cociente que precisamente es la dimensión de $E_n$ menos la dimensión de $F_r$. El objetivo es demostrar que forman una base y para eso habrá que demostrar la independencia lineal y que generan el espacio cociente.

Así habremos demostrado que las dimensiones $n-r$ (la dimensión del cociente $n$ menos $r$) que es $dim(E)-dim(F)$ que es lo que queremos demostrar.

Vamos a demostrar que se cumplen las dos propiedades para ser una base,

**Primero** hay que demostrar que son linealmente independientes:

Hay que considerar una combinación lineal de estos elementos igualado a cero, entonces consideremos esta combinación lineal: 
$$\sum_{i=r+1}^{n}\alpha_i\cdot[u_i]=[0]$$
Esta igualada a la clase del cero porque recuerda que estamos dentro del cociente por lo tanto todos son clases y las clases son lineales, por tanto la clase de una combinación lineal es la combinación lineal de las clases y viceversa así que esto se podría escribir como:

$$\left[\sum_{i=r+1}^{n}\alpha_i\cdot u_i\right]=[0]$$

Pero si toda esa clase del sumatorio es igual a la clase del cero, significa que la combinación lineal $\sum_{i=r+1}^{n}\alpha_i\cdot u_i\in F$

Recuerda que la clase del cero es la clase de cualquier vector que es de $F$ eso significa que la combinación lineal tiene que ser un elemento de F. Por tanto esto significa que esa combinación lineal debería ser igual a una combinación lineal de una base que es la base de $F$

$$\sum_{i=r+1}^{n}\alpha_i\cdot u_i=\sum_{j=1}^{r}\alpha_j\cdot u_j$$

Pero si ahora lo escribimos todo a la derecha de un igual y lo ponemos igual a cero no queda:
$$\sum_{j=1}^{r}\alpha_j\cdot u_j-\sum_{i=r+1}^{n}\alpha_i\cdot u_i=0$$

Ahora tenemos una combinación lineal de la base de $E$ igualada a cero, por tanto como los $\{u_i,...,u_n\}$ son base de $E$ entonces son linealmente independientes y si son linealmente independientes tenemos una combinación lineal de estos igualada a cero, por tanto todos los escalares desde el $\alpha_1,...,\alpha_r$ tienen que ser $0$ pero también desde el $\alpha_{r+1},...,\alpha_n$ deben ser cero. para eso tomamos sólo la última parte tenemos que la combinación lineal de cualquiera de los elementos de la base del cociente  igualada a cero da como resultado la combinación lineal nula, por tanto concluimos que los vectores clase de $[u_{r+1}],[u_{r+2}],...,[u_n]$ son linealmente independientes.

Para demostrar la **Segunda Parte** lo único que tenemos que demostrar es que generan todo el cociente.

Así que vamos a demostrar que es un sistema generador para lo cual vamos a tomar un elemento del cociente que es $[u]\in E/F$ y ya sabemos que si la clase de $[u]$ es un elemento del cociente eso es porque hemos tomado un elemento $u$ del espacio vectorial $E$ y lo mandamos a la clase que corresponda.

Pero por ser un elemento del espacio vectorial $E$ al que luego le hacemos cociente para obtener la clase, $u$ se podra escribir como combinación lineal de los elementos de la base de $E$ y esta base eran los $u_i$ desde el $1$ hasta el $n$. Por tanto $u$ se podría escribir como una combinación lineal:
$$u=\sum_{i=1}^{n}\alpha_i\cdot u_i$$

pero si $u$ se escribe como combinación lineal de estos fijensen que la clase de $[u]$, será la clase de esta combinación lineal:
$$[u]=\left[\sum_{i=1}^{n}\alpha_i\cdot u_i\right]$$

Pero como la clase de una combinación lineal es la combinación lineal de las clases se podra sacar el sumatorio fuera de la clase, sacar el escalar fuera de la clase y dejar:
$$[u]=\sum_{i=1}^{n}\alpha_i[u_i]$$

Fijensen que estamos haciendo clase de $u_i$ desde $1$ hasta $n$ y recuerda que desde $i=1,...,r$ los $u_i$ son una base de $F$, $u_i\in F$ ,por tanto en esos casos desde el primero hasta el R-esimo. La clase $[u_i]=[0]$, entonces este sumatorio no tiene $n$ elementos, Tiene:
$$\sum_{i=r+1}^{n}\alpha_i[u_i]$$

Enotnces al ser los $r$ primeros una base de $F$ (su clase es cero). Acabamos de demostrar que si tomamos un vector $u$ del espacio cociente $[u]\in E/F$, entonces cualquier clase de un vector $u$ del cociente se puede escribir como combinación lineal de las clases $[u_i]$ desde el $i=r+1$ hasta el $n$. Por tanto concluimos que desde la clase $[u_{r+1}]$ hasta la clase del $[u_n]$ son ni más ni menos que un sistema generador del espacio cociente.

Queda demostrado que en efecto son un sistema generador, son linealmente independientes, son una base y por tanto la dimensión $dim(E/F)=n-r$ que si los contamos es el número de clases, el número de vectores que nos salen precisamente de este conjunto $[u_{r+1}],...,[u_n]$ que hemos utilizado para hacer la demostración y queda así probada la fórmula de la dimensión para el caso del espacio vectorial cociente.

En particular hay que ir con cuidado porque en dimensiones infinitas, la fórmula anterior ya no sería válida. Un ejemplo de ello es el $\mathbb K$-espacio vectorial de los polinomios $\mathbb K[x]$. Recurda que $\mathbb K[x]$ tenía dimensión infinita. Habíamos demostrado en su momento que una base de $\mathbb K[x]$ no podía tener un número finito de polinomios, habíamos llegado a contradiccion y en particular dentro de $\mathbb K[x]$ podemos considerar un polinomio $p(x)$ que no sea constante.

Si $p(x)\in\mathbb K[x]$ es un polinomio no constante cualquiera, y consideramos el subconjunto de los múltiplos de $p(x)$, Una vez lo tenemos podemos considerar el conjunto de lo que voy a llamar los polinomios que son múltiplos de $p(x)$ ese subconjunto $F$ lo  denotamos por: (Va a simbolizar polinomios de la forma $p(x)q(x)$, siendo $q(x)$ otro polinomio de $\mathbb k[x]$)

$$F=(p(x)) = \{p(x)q(x)| q(x)\in\mathbb K[x]\}$$

Es decir son los polinomios múltiplos de $p(x)$ que es un polinomio que se obtiene a partir de multiplicar $p(x)$ por otro polinomio, eso sería un múltiplo. 

Los múltiplos de $p(x)$ serán todos aquellos polinomios que se pueden obtener como producto de $p(x)$ por otro polinomio cualquiera $q(x)$ multiplicando cualquier cosa, los múltiplos de $p(x)$ van a ser los polinomios de la forma $p(x)q(x)$ siendo $q(x)$ otro polinomio de $\mathbb K[x]$.

se puede demostrar fácilmente que $F=(p(x))$ es un subespacio vectorial de los polinomios o aea de todo $\mathbb K[x]$, ya que la suma de múltiplos de $p(x)$ es múltiplo de $p(x)$ (si escribimos los elementos de esta forma $p(x)q(x)$ y los sumamos podremos sacar el $p(x)$ Factor Común, asi que va a ser un multiplo de $p(x)$) y el producto de un escalar por un elemnto ya sea múltiplo de $p(x)$ también es múltiplo de $p(x)$ porque el escalar se puede llevar la $p$ o se puede llebar la $q$ y si se lleva la $q$ ya se puede escribir $p(x)$ por otro polinomio asi es trivialmente es un subespacio vectorial de $\mathbb k[x]$. Además es de dimensión infinita ya que podemos demostrar que no puede tener un número finito de generadores como ocurría con el caso de $\mathbb K[x]$.

En este caso podemos definir el $\mathbb K$-espacio vectorial cociente $\mathbb K[x]/F$ con $F=(p(x))$ (Siendo $F$ el subespacio de los multiplos de $p(x)$) donde la relación módulo $F$ sería:

$$a(x)\sim_F b (x)\Longleftrightarrow a(x) - b(x) \in F = (p(x))$$

Dos polinomios $a(x)$ y $b(x)$ que pertenecen a $\mathbb K[x]$ se relacionan a través de la relación $\sim_F$ si y solo si la resta de $a(x)-b(x)$ pertenece a $F$ o lo que es lo mismo es un múltiplo de $p(x)$, la resta de $a(x)-b(x)$ se puede escribir como un polinomio multiplicado por $p(x)$, bajo esta definición de conjunto cociente resulta que:

**Proposición.**  Sea $p(x)\in\mathbb{K}[x]$ un polinomio no constante y de grado $\deg(p(x)) = n\ge 1$. Entonces, dentro de cada clase no nula de un polinomio $a(x)\in \mathbb K[x]$, $[a(x)]_F$ siempre hay un representante de grado menor a $n$

De modo que cualquier clase de equivalencia de un polinomio $a(x)$ que pertenezca a $\mathbb K[x]$ siempre tendrá un representante que tendrá grado menor e igual que $n$ para demostrarlo.

Es que, si nos dan un polinomio cualquiera, lo tendremos que expresar como un múltiplo de el polinomio $p(x)$ más otro polinomio, no necesariamente tendrá que ser un múltiplo, pero sí que puede escribirse como múltiplo de $p(x)$ más otro polinomio, si hacemos la división en caja del polinomio $a(x)$ entre $p(x)$ nos va a quedar un cociente y el resto, el resto en general tiene grado menor que $n$ así que la idea es que podramos escribir el polinomio $a(x)$ como múltiplo de $p(x)$ más el resto y ese resto tendrá un grado menor estricto que $n$ y si ponemos las cajas de la clase de equivalencia a la izquierda y a la derecha del igual nos va a quedar precisamente la clase de $a(x)$ que es igual a la clase de $p$ por la clase del cociente más la clase del resto. La clase del propio polinomio va a ser la cero, porque $p$ pertenece al propio generado por $p$ y $p$ pertenece al subespacio $F$.

Así que nos va a quedar que la clase de $a(x)$ es la clase de su resto, pero el resto es grado menor estricto que $n$.


**Ejercicio 23.** Demostrar formalmente esta Proposición.

**Proposición.** Sea $p(x)\in\mathbb{K}[x]$ un polinomio no constante de grado $\deg(p(x))= n\ge 1$. Entonces, las clases $[1],[x],[x^2],\dots,[x^{n-1}]$ se entiende todas ellas con la relacion modulo $F$ (que son multiplo del polinomio $p(x$), forman una base de $\mathbb{K}[x]/(p(x))$ (Entre los multiplos de $p(x)$), y, por lo tanto, como nos acaban de presentar $n$ elemenots que son esas clases hasta $[x^{n-1}]$, lo de abajo se lee como:
La dimensión del espacio vectorial cociente $\mathbb K[x]$ entre los multiplos de $p(x)$ tiene dimensión $n$ que es exactamente el grado de $p(x)$.

$$\dim(\mathbb{K}[x]/(p(x))) = n = \deg(p(x))$$

Así que si nos dan todos los polinomios y hacemos cociente por un polinomio determinado, ese espacio vectorial que resulta tiene la misma dimensión qué el grado que tiene el polinomio.

**Ejercicio 24.** Demostrar formalmente esta Proposición.

Todas estas proposiciones habéis visto que se ah utilizado que el polinomio es no constante es decir el polinomio tiene que ser por lo menos de grado 1. ¿Qué pasa con los polinomios constantes?.

**Observación.** En el caso en que hiciésemos el cociente de $\mathbb{K}[x]/(p(x))$ donde $p(x)$ fuese un polinomio constante (de grado 0), entonces tendríamos que el generado $(p(x)) = \mathbb{K}[x]$ (serain todos los polinomios) ya que las constantes son invertibles y todo polinomio se puede poner como múltiplo de una constante cualquiera (ya que cualquier polinomio $p(x)$ se puede escribir como $p(x) = k(1/k\cdot p(x))\ \forall k\in \mathbb K^*$), asi que el generado por $p(x)$ (los multiplos de cualquier constante serian todos los polinomios), simplemente multiplicando un numero (constante) por cualquier polinomio da cualquier polinimo por eso los teoremas suponian que $p(x)$ fuera un polinimo no constante de grado $1$ o superior

De este modo, $\mathbb{K}[x]/(p(x))$ sería el $\mathbb{K}$-espacio vectorial trivial, el ${0}$.

Simplemente al hacer cociente de $\mathbb{K}[x]$ entre el generado por $p(x)$ que sería $\mathbb{K}[x]$ resultaría que solo tendríamos una clase, que es la del cero o lo que es lo mismo todos los polinomios se relacionaría con todos. Puesto que cualquier polinomio menos sí mismo, daría igual al vector cero. así que todos los polinomios estarían relacionados a través de la relación de equivalencia de ser múltiplo de $p(x)$ en el caso en que $p(x)$ fuera un polinomio constante.

## Ejercicio 23 y 24

Recuerda que el espacio vectorial de polinomios que es un espacio vectorial infinito $\mathbb{K}[x]$ y los elementos que viven dentro son polinomios $p(x)\in\mathbb{K}[x]$ que tienen cierto grado pero con coeficientes en el cuerpo $\mathbb{K}$. Pues resulta que si $p$ es un polinomio no constante, tiene sentido considerar el subespacio vectorial $F$ formado por el conjunto de múltiplos del polinomio $p(x)$ que se define como:
$F=(p(x))=\{p(x)q(x)\|q(x)\in\mathbb{K}$ (el producto de $p(x)$ por cualquier $q(x)$ siendo $q(x)$ un polinomio cualquiera de $\mathbb{K}$).

Esto genera el conjunto de múltiplos de $p(x)$.

Es bastante fácil demostrar que $F$ era un subespacio vectorial ya que la suma de múltiplos de $p$ es un múltiplo de $p$ y el producto de un escalar por un múltiplo de $p$ también será múltiplo de $p$ y en este caso tenía sentido poder estudiar el espacio vectorial cociente $\mathbb{K}[x]\|p(x)$ o mejor dicho por el subespacio vectorial del generado por $p(x)$.

En este caso la relación módulo $F$ sería que dos polinomios $a(x)\sim_F b(x)$ se relacionan a través de $F$, siendo $F$ el generado por $p$ si y sólo si, $a(x)-b(x)\in F$ resulta ser un múltiplo del polinomio $p(x)$. es decir si la resta pertenece a $F$ o lo que es lo mismo, es un múltiplo del polinomio $=(p(x))$, Básicamente lo que nos queda es un múltiplo del polinomio $p$.

Bajo estas hipótesis y si $p$ es un polinomio no constante llamemosle, grado de $(p(x))=n\geq 1$ no puede ser un polinomio constante, entonces resulta que dentro de cada clase no nula y que queramos considerar de este cociente $a[x]$ siempre existe un representante de grado menor que $n$ es decir da igual el polinomio que elegimos de cada $\mathbb{K}$ hacemos su clase que pertenecerá a el conjunto cociente $\mathbb{K}[x]$ entre los múltiplos de $p(x)$: $\mathbb{K}[x]\|(p(x))$ pues ahí dentro siempre podremos encontrar un representante de grado menor estricto que el del polinomio del cual partimos.

La demostración resulta que si dividimos el polinomio $a(x)$ entre el $p(x)$ que es en el que hacemos módulo y nos va a quedar un cociente $q(x)$ y un resto $r(x)$. Además recuerda que en la división polinómica siempre podríamos escribir la relación $a(x)=p(x)\cdot q(x)+r(x)$ (divisor por cociente más resto) y además el resto tiene una particularidad en polinomios y es que o bien el resto es cero (es un polinomio $r(x)=0$) o bien el grado del polinomio $r(x)$ es menor estricto que el grado de divisor (el grado de $p$) o sea que es $n$ este caso.

Esa es la particularidad que tiene el resto o bien el resto es cero o bien el resto es un polinomio de grado menor estricto que el grado de del polinomio del cual partimos que es en este caso $p$.

Esto significaría si tomó esta expresión que $a(x)-r(x)$ sería un múltiplo de $p(x)$ porque se podría escribir como $p(x)\cdot q(x)$. Pero esto es precisamente la definición de que dos polinomios se relacionan, fijesen $a(x)\sim_Fb(x)$ si su resta es un múltiplo de $p$ y $a(x)$ y $r(x)$ el polinomio de partida y su resto, se relacionan porque sus resta es múltiplo de $p$. Así que lo que podría decir es que la clase de un polinomio cualquiera $[a(x)]$ siempre será igual a la clase $[r(x)]$ de su resto de dividir el polinomio original entre $p$ y fijaros que por propia construcción el grado de $r(x)$ o es cero o es directamente menor que el de $p$. Por tanto siempre existe dentro de cada clase no nula al menos un representante de grado menor estricto que el grado menor que $n$

**La otra Demostracion**

sea $p(x)\in \mathbb{K}[x]$ no constante con grado $(p(x))=n$ y $n$ es un número mayor o igual que $1$, la segunda proposición decía que las clases de los polinomios: $[1],[x],[x^2],\cdots,[x^{n-1}]$ son una base del cociente (son una base de $\mathbb{K}[x]$ módulo $F$: $\mathbb{K}[x]/F$) donde $F$ es el subespacio vectorial generado por los múltiplos del polinomio $(p(x))$, es decir que basta con tomar hasta el grado del polinomio y restarle 1 y los polinomios hasta $[x^{n-1}]$ serán una base del conjunto cociente.

De aqui se deriva directamente, que la dimensión del cociente es el mismo grado que el polinomio $p$, $dim(\mathbb{K}[x]/F)=n$ por tanto la dimensión de hacer un cociente por los múltiplos de un polinomio $p$ de grado $n$ tiene dimensión exactamente igual a $n$, igual al grado del polinomio $p$.

Si quieremos demostrar que esto es una base, tendremos que demostrar que son linealmente independientes y que generan todo el espacio:

si generamos una combinación lineal de esos polinomios, ponemos $i=0$ porque tenemos un polinomio constante.
$$\sum_{i=0}^{n-1}\alpha_i[x^i]$$

si escribimos esta combinación lineal, igualada al vector clase $[0]$ del cociente entonces resulta lo siguiente, (ya sabemos que la combinación lineal de clases, es la clase de la combinación lineal). Por tanto podemos escribir la clase de la combinación lineal:

$$\left[\sum_{i=0}^{n-1}\alpha_ix^i\right]=[0]$$

y esto lo que significa es que básicamente la combinación lineal pertenece al subespacio vectorial $F$ o lo que es lo mismo, el polinomio resulta que pertenece a $F$:
$$a(x)=\sum_{i=0}^{n-1}\alpha_i x^i \in F$$
o lo que es lo mismo, es un múltiplo del polinomio $(p(x))$.

El grado de $a(x)$ sería menor o igual que $n-1$ porque llega hasta ahi como mucho donde $n-1$ es menor o estricto que $n$ que es el grado de $p(x)$, entonces tenémos un problema porque estamos diciendo que un polinomio de grado como mucho, $n-1$ es múltiplo de un polinomio de grado $p$. La única forma de que así sea, es que, el único múltiplo del polinomio $p$ de grado menor estricto que $n$, es si se multiplica el polinomio $p(x)$ por cero o lo que es lo mismo si $a(x)=0$, si es el polinomio $0$ lo que nos queda es que:
$$\sum_{i=0}^{n-1}\alpha_i x^i=0$$

y la única forma de conseguirlo es que el término independiente sea cero, el término de grado 1 sea cero y así hasta el término de grado $n-1$ todos ellos sean $0$: $\alpha_0=\alpha_1,...,\alpha_{n-1}=0$ en cuyo caso en efecto son linealmente independientes.

Pero para que formen una base también necesitamos que sean un sistema generador. Veamos que si tomamos cualquier vector que de hecho es una clase de polinomio $[a(x)]$ del conjunto cociente $\mathbb{K}(x)$ módulo, el generado $(p(x))$, los múltiples de $p(x)$ veamos que se puede generar por cualquiera de ese conjunto de vectores, por la proposició anterior.

la clase $[a(x)]$ es la misma que la clase de cierto $[r(x)]$, un polinomio de grado menor o igual que $n$ es aquel que cumple que $a(x)$ es un múltiplo de $p(x)\cdot q(x)+r(x)$ y recuerda que el grado de $r(x)$ es menor estricto que el grado de $p$. Así que $r(x)$ o bien sería cero $r(x)=0$ o el grado de $r(x)$ sería un número menor estricto que $n$, menor estricto que el grado de $p$.

Así que en cualquier caso $r(x)$ será un múltiplo o se podrá escribir como un polinomio constante por ejemplo: $r(x)=a_0+a_{1}x+a_{2}x^2+...+a_{n-1}x^{n-1}$. Pero claro si el polinomio $r(x)$ se puede escribir de esta forma su clase sería: $[r(x)]=a_0[1]+a_{1}[x]+a_{2}[x^2]+...+a_{n-1}[x^{n-1}]$ ya se ah aplicado la linealidad de las clases, se han sacado y demás.

Si el polinomio cualquiera $a(x)$ es igual a cierto polinomio de grado menor que $n$ y cualquier polinomio de grado menor que $n$ es una combinación lineal desde la clase del $[1]$ hasta la clase de $[x^{n-1}]$. Esto significa que los vectores o los elementos: clase de $[1]$, clase de $[x]$, clase de $[x^2]$ hasta la clase $[x^{n-1}]$ son un sistema generador del cociente. 

Por tanto concluimos que $[1],[x],[x^2],...,[x^{n-1}] $ es un sistema generador del cociente de $\mathbb{K}[x]$ modulo $F$ y por tanto si son sistema generador y por el apartado anterior eran linealmente independientes.

En efecto son una base del cociente y además una base muy interesante porque nos permite decir que el cociente por un polinomio de grado $p$ da lugar a un espacio vectorial de dimensión $n$, de la misma dimensión que el polinomio.

Así que ya esta es otra forma de generar espacios vectoriales en este caso a través de hacer cociente por los múltiples de un polinomio.

# Rango de un conjunto de vectores. Coordenadas en una base.

**Rango de un conjunto de vectores.** Sea $E$ un $\mathbb{K}$-espacio vectorial cualquiera. Se llama rango de vectores $\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\in E$ a la dimensión del subespacio vectorial que generan

Es decir nosotros tomarias todos esos vectores, tomamos el subespacio vectorial generado por ellos le calculamos la dimencion

$$\text{rg}\{\vec{u}_1,\dots,\vec{u}_n\} = \dim(\langle\vec{u}_1,\dots,\vec{u}_n\rangle)$$

la dimencion que precisamente coincide con el número máximo de vectores LI que se pueden extraer del conjunto $\{\vec{u}_1,\dots,\vec{u}_n\}$ se le denomina el rango de ese conjunto de vectores

Otra definición de rango:

**Rango de un conjunto de vectores.** Dados una serie de vectores del espacio vectorial $E$ $\vec{u}_1,\dots,\vec{u}_n\in E$, se dice que tiene rango $r$, seindo $r\le n$, $n$ es el numero de vectores, si existe como mínimo un subconjunto de $r$ vectores LI entre ellos y no existe ningun conjunto de $r+1$ que sea LI.

En otras palabras, como bien se dijo anteriormente, es el número máximo de vectores LI que pueden extraerse del conjunto fuera $r$ (el rango).

Esto nos suena que todos los menores de orden $r$ son no nulos y apartir de los $r+1$ son nulos, esto es como se define el rango de una matriz, resulta que una forma de calcular el rango de un conjunto de vectores es escribir una matriz donde todas sus filas sean cada una de ellas sea un vector de estos $\vec{u}_1,\dots,\vec{u}_n$ o sus columnas, resulta que si esa matriz cuyas filas o columnas son los vectores que queremos estudiar tiene rango $r$ (seria buscar el orden del mayor, menor no nulo),el rango de ese conjunto de vectores coincide con el rango de esta matriz fabricada con las filas o columnas por los vectores qe queremos estudiar, una forma de saber cual es el rango de un conjunto de vectores, tomamos ese conjunto:

 - Forma 1: medimos cuantos de ellos son como mucho LI 
 - Forma 2: tomamos esos vectores los escribimos como filas o columnas de una mmatriz  y               sacamos el rango de esa matriz
 - Forma 3: El rango del conjunto de vectores es la dimencion del espacio vectorial generado por esos vectores

## Coordenadas en una base

El rango de $n$ vectores está relacionado con el rango de una matriz tal y como veremos a continuación.

En primer lugar, recordemos que dado un $\mathbb{K}$-espacio vectorial de dimensión $n$ y $B=\{e_1,\dots,e_n\}$ una base cualquiera de $E$ con $n$ vectores, todo vector $x\in E$ se escribe de manera única como combinación lineal de los elemntos de la base $B$ de la forma, que ese vector $x$ de $E$ se puede escribir como:

$$x = \sum_{i = 1}^n\alpha_i\cdot e_i = \alpha_1\cdot e_1+\cdots+\alpha_n\cdot e_n$$

Lo interesante de que sea una base es que la forma de escribir cada uno de los vectores en una base es única

Por tanto, podemos dar la siguiente definición con esos escalares hasta $\alpha_n$ se les llama:

**Coordenadas de un vector en base $B$.** (las coordenadas dependen de la base elegida, si cambiamos de una base las coordenadas cambian, si vemos un mismo vector en una base tendra unas coordenadas a si lo vemos en otra base tendra otrras coordenadas) Son los escalares $\alpha_1,\dots,\alpha_n$ de la combinación lineal anterior

Más formalmente,

**Coordenadas en una base.** Dado un $\mathbb{K}$-espacio vectorial $E$ con una base $B = \{e_1,\dots,e_n\}$ y un vector $u\in E$,si no dan ese vector $u$ se sabe que existen unos únicos escalares $\alpha_1,\dots,\alpha_n\in\mathbb{K}$ (sobre el que montamos el espacio vectorial) tales que: ($u$ se puede escribir como esa forma unica)

$$u = \sum_{i = 1}^n\alpha_i\cdot e_i$$

Estos escalares se denominan coordenadas del vector $u$ en la base $B$. Y se escribe asi: 
($u$ es el vector con esos $\alpha$ en la base $B$), poner esa $B$ es importante porque las coordenadas cambiaran al cambiar de base, Así que un vector $u$ se puede escribir de forma única a partir de las coordenadas en una base $B$ fija.
$$u = (\alpha_1,\dots,\alpha_n)_B$$

**Proposición.** Sea $E$ un $\mathbb{K}$-espacio vectorial de dimensión finita $n$ y $B=\{e_1,\dots,e_n\}$ una base de $E$. Sean $u_1,\dots,u_m\in E$ vectores de forma que cada $u_j$ tiene coordenadas en la base $B$ dadas por $$u_j = \sum_{i = 1}^n a_{ij}\cdot e_i = a_{1j}\cdot e_1+\cdots+a_{nj}\cdot e_n$$

Entonces,

- $u_1,\dots,u_m$ son LI si, y solo si, (el rango de la matriz $A$ formada por las coordenadas de los vectores $u_j$ en la base $E$ es exactamente al numero de vectores que tenemos y menor o igual que la dimencion del espacio vectorial)$\text{rg}(A) = m\le n$
- $\text{rg}(u_1,\dots,u_m) = \text{rg}(A)$, el rango de los vectores coincide con el rango de sus coordenadas, aqui podemos mirar las dependencias lineales

**Ejercicio 25.** Demostrar formalmente esta Proposición.

Por la proposición anterior, un método para calcular el rango de un conjunto de vectores consiste en construir una matriz utilizando los vectores como columnas (o filas) y definir el rango de la matriz como el rango de sus vectores columna (o fila). Que el rango de un conjunto de vectores coincide con el rango de la matriz donde esos vectores estan como filas o columnas.

**Observación.** Si se nos facilitan las coordenadas de un vector sin especificar la base, se sobreentiende que se trata de la base canónica. si no nos dicen lo contrario las coordenadas de un vector siempre vienen en una base canonica 

También reciben el nombre de **coordenadas cartesianas** y son las que en temas anteriores hemos definido como las componentes de un vector. es decir estas son las basicas, las fundamentales 

A partir de este momento en el que ya sabemos que cada espacio vectorial puede tener una base u otra puede tener infinitas bases y que cada vector puede tener componentes o coordenadas diferentes (una base o la otra). Es importante que coloquemos ese subíndice para enfatizar de qué base estáis hablando porque si no colocamos ese subíndice (no indica en qué base) vamos a suponer que siempre estas hablando de la base canónica.

## Ejercicio 25

La propocicion nos decia que si $E\in\mathbb{K}-e.v$ de dimencion finita $n$ y $B$ esra una base formada por los vectores $B={e_1,e_2,...,e_n}$ por lo tanto la dimencion del espacio vectorial es $n$ y tomabamos $u_1,u_2,...,u_m$ otros vectores del espacio vectorial $E$ tal que estos vectores tenian por coordenadas en la base de $B$ anterior unos ciertos ccoeficientes cada $u_j$ se puede escibir como sumatorio, cada uno de los $alpha_ij$ por los vectores de la base $E$los $e_i$: 
$$u_j=\sum_{i=1}^{n}\alpha_ije_i=a_{1j}e_1+a_{2j}e_2\cdots+a_{nj}e_n$$ 
para todo $j=1,...,m$, entonces resultaban dos apartados, el primero era que desde $u_1,...,u_m$ son linealmente independientes si y solo siel rango de la matriz $A$ formado por los coeficientes de los vectores $u$ en la base $B$ el rango de esa matriz $rango(A)=m$, es exactamente $m$ y evidentemente $m\leq n$ tomamos menos vecotres que los de la base no podemos tener mas vectores L.I de los que tenemos en la base y el **segundo apartado** decia que el rango de los vectores $u$, $rg(u_1,...,u_m)=rg(A)$ es igual al rango de la matriz de coeficientes $A$ que es esa  matriz $A=a_{ij}$ y desde $1$ hasta $n$ por tanto tenemos $n$ filas en este caso y la $j$ en este caso hasta $m$ columnas.

Vamos a demostrar esta propocicion:

1) es la dependnecia si y solo si el rango es exactamente igual al numero de vectores

Las dos afirmaciones, tanto que los vectores sean L.I como el $rg(A)=m$ solo pueden ser ciertas en el caso de que $m\leq n$ (no podremos tener mas vectores independientes que la dimencion del espacio vectorial), durante toda la demostracion vamos a suponer esa condicion de $m$, en este caso vamos a considerar la combinacion lineal de los $m$ vectores que son L.I:
$$\sum_{j=1}^{m}\alpha_ju_j=0$$

ya sabemos que si sustituimos cada uno de los $u_j$ que por las coordenadas que tienen esos vectores del espacio vectorial en la base $B$ que serian los $u_j$ igual al sumatorio, entonces nos quedaria:
$$\sum_{j=1}^{m}\alpha_j\left(\sum_{i=1}^{n}\alpha_ije_i\right)=0$$
podemos desrrollar esta exprecion y nos quedaria:
$$\alpha_1(a_{11}e_1+...+a_{n_1}e_n)+...+\alpha_m(a_{1m}e_1+...+a_{n_m}e_n=0$$

de aqui ya podemos ver que si multiplicamos distributivamente y reagrupamos se puede poner como un dobel sumarotio donde se sacan los vectores $e$ factor comun y qedaria como:
$$\sum_{i=1}^{n}\left(\sum_{j=1}^{m}\alpha_ja_{ij}\right)e_i=0$$
Pero asi como los $e_i$ forman una base del espacio vectorial $E$, son L.I por tanto aqui tenemos una combinacion lineal de todos ellos $\sum_{j=1}^{m}\alpha_ja_{ij}$ esto seria el escalar que aocmpaña a uno de los vectores $e_i$, una combinacion L.I de esos vectores igualado a cero, significa que todos los escalares son iguales a cero o lo que es lo mismo:
$$\sum_{j=1}^{m}\alpha_ja_{ij}=0\ \forall\ i=1,...,0$$
si llamamos a $x$ la matriz columna (Vector) formada por los $\alpha_j$
$$x=\begin{pmatrix}
\alpha_1\\
\alpha_2\\
...\\
\alpha_m
\end{pmatrix}$$

Entonces con lo que tenemos, resultaen un sistema con $n$ ecuaciones y $m$ incognicas, se puede escribir de forma matricial como, la matriz $A$ de coeficientes de los vectores $u$
por el vector $x$, $A\cdot x=0$, entonces nos quedaria de la formamatricial y de esta forma sabemos que los diferentes vectores $u_m$ seran L.I si y solo si para cualqueir combinacion lineal de ellos, todos los escalares son $0$ porque la combinacion lineal que habiamos puesto de los $\alpha_i$ cualquieras eran arbitarios en $\sum_{j=1}^{m}\alpha_ju_j=0$ y como conclucion era que la unica forma de que sa combinacion Lineal sea cero es que para todas las Combinaciones lineales que eligieramos este ssitema de Ecuaciones Lineal tendira la solucion cero pero esto se verifica si y solo si el sistema $A\cdot x=0$ tiene solucion unica, ya sabemos que si un sistema es homogeneo la unica forma en la que tenga solucion unica es que la solucion sea la trivial y por tanto si la solucion es trivial el rango de la matriz debe de ser igual al de la ampliada igual al numero de incognitas o lo que es lo mismo el rango de la matriz $A$ es exactamente igual a $m$ que es al numero de incognita que tubieramos (el numero de $\alpha_i$ que tengamos) para que la solucion sea unica

**Explicado de otra forma mas clara: ** Sabemos que los $u_i$ son L.I si y solo si para cualquier combinacion lineal (porque hemos elegido una cualquiera) resulta que todos y cada uno de los escalares son $0$ pero esto se verifica si y solo si el correspondiente sistema $A\cdot x=0$ tiene solucion unica (Solucion Trivial) y esto solo pasa si el $rg(A)$ es igual al numero de incognitas que son las $m$ que tenemos en el vector $x$

a partir de ahora uqe un conjunto de vectores sea L.I se traduce que el rango de su matriz de coordenadas es  de rango maximo, es igual al numero de vectores

2) el rango de un conjunto de Vectores coincide con el rango de la matriz de los coeficientes de los mismos

Este es concecuencia inmediata del 1), y de las definiciones del rango de una matriz y el rango de un vector, simplemente que el rango de un vector sea $m$ significa que son $m$ vectores que son L.I por el apartado 1) se traduce que en el rango de la matriz es $m$ y por tanto  todo cuadra.

Este resulta se podra utilizar para detectar una serie de vectores son L.I o no.

Ejemplo que si son L.I: si nos dan 5 vectores, la matriz de coeficientes tienen que ser de rango 5 por tanto ponemos en filo o columan los coeficientes de los vectores de una base en otra y averiguramos que el determiante 5x5 resultante es diferente de 0

## Como saber si unos vectores son o no L.I

Utilizando la propocicion anteriror, vamos a comprobar que a partir de una serie de vectores linealmente independientes podemos generar otros.

Por tanto vamos a suponer que tenemos $u_1,u_2,u_3$ unos vectores linealmente independientes de un $\mathbb{K}$ espacio vectorial cualquiera y vamos a fabricar tres vectores (los vectores $v$), esto es algo que hacemos mucho en cambio de base:
$$v_1=u_1+u_2$$
$$v_2=u_2-u_3$$
$$v_3=u_1+u_2+u_3$$

hemos generado tres vectores a partir de otros tres. Vamos a demostrar que $v_1,v_2,v_3$ son tres vectores linealmente independientes.

Si consideramos el subespacio vectorial generado $F=\langle u_1,u_2,u_3\rangle$ este resulta que tiene una base, simplemente los vectores $\{u_1,u_2,u_3\}$ al ser linealmente independientes y generar todo $F$, son una base del subespacio vectorial $F$, como son una base entonces tenemos la expresión de los vectores $v_1,v_2,v_3$ en esta base tenemos las coordenadas de estos vectores en una base y por la proposición que hemos demostrado para ver si los vectores $v_1,v_2,v_3$ son linealmente independientes. Lo único que hay que comprobar es que su matriz de coeficientes sea de rango máximo.

La matriz de coeficientes $A$ que tenga las coordenadas de los vectores $v_i$ en términos de la base $u$ en columna, sería la matriz: $1\ 1\ 0$ estos serían las coordenadas de $v_1$ en columna, las coordenadas de $v_2$ en columnas sería la $0\ 1 \ -1$ y las coordenadas de $v_3$ en columnas serían la $1\ 1\ 1$.
$$x=\begin{pmatrix}
1 && 0 && 1\\
1 && 1 && 1\\
0 && -1 && 1
\end{pmatrix}$$

Hemos colocado las coordenadas de los vectores $v$ en términos de la base $u$ en columna y por tanto los vectores $v$ son linealmente independientes si y solo si el rango de esta matriz $A$ es $3$(maximo) pero para ver que esta matriz tiene rango 3 porque $A$ tiene rango 3 si y sólo si su determinante es diferente de cero. Para calcular el determinante utilizando la regla de sarruss nos quedaría $|A|=1+(-1)-(-1)=1$, simplificando nos queda $|A|=\not1+\not{(-1)}-(-1)=1$ y el determinante de la matriz es $1$ por tanto diferente de cero significa que el $rg(A)=3$ y por el teorema anterior significa que los vectores $v_1,v_2,v_3$ de los cuales se han puesto sus coordenadas en la base $u$ en columna, son linealmente independientes

Así es como serían este tipo de ejercicios y sería la forma más sencilla que a partir de una serie de vectores linealmente independientes generar otros.

Así pues el rango de $n$ vectores se puede calcular a través de sus coordenadas en cualquier base y el rango no dependerá nunca de la base que utilizáis.

Ahora bien las coordenadas de cualquier vector en una base $B$ y en otra base $B'$ si que son evidentemente diferentes, lo que pasa es que si son independientes en una base también lo van a ser en otra. Por tanto esto nos deja a las puertas de una operación muy interesante, Cómo puedo calcular las coordenadas de un vector que está en una base y en otra.

Pues eso lo veremos precisamente donde se introducirá el concepto de cambio de base.

# Cambio de base

ya sabemos que en un espacio vectorial puede haber infinitas bases, podemos clcular las coordenadas de un vector en una base o en otra base o cambiar las coordenadas de una base a otra

Sabemos que las coordenadas de un vector son únicas en cada base, pero distintas cuando cambian de base.

Partiendo de este punto, el problema que se nos plantea es el de calcular las coordenadas de un vector en cierta base $B'$ dadas las coordenadas del mismo en otra base $B$.

Se necesitará pues conocer la relación entre ambas bases. como podemos establecer lo que llamaremos una **matriz de cambio de base**

**Ejemplo**

Dadas las bases $B_u = \{\vec{u}_1,\dots,\vec{u}_n\}$ y $B_v = \{\vec{v}_1,\dots,\vec{v}_n\}$ (Las dos son bases y al serlo continen el mismo numero de elemntos con $n$ elementos) de un espacio vectorial $E$ de dimencion $n$, si queremos calcular las coordenadas de los vectores de $B_u$ en la base $B_v$, se han de expresar los vectores $\vec{u}_i$ como combinación lineal de los vectores de $\vec{v}_i$, es decir si queremos calcular las coordenadas de los vectores de la base $u$ lo que nececitamos es que cada uno de esos $u_i$ se exprese como una combinacio lineal de los vectores $v_i$, esa combinacion lineal nos va a dar las coordenadas de los vectores $u$ en terminos de la base de los vectores $v$

**Ejemplo 13**

Dado el vector $\vec{u}\in\mathbb{R}^3$ de coordenadas $(-2,3,5)_B$ en la base $B$ que es la base formada por los vectores:
$$B = \{(2,4,0),(1,0,1),(-1,2,0)\}$$
Esto significa que es $-2$ veces el vector $(2,4,0)$, $3$ veces el vector $(1,0,1)$ y $5$ veces el vector $-1,2,0$, el primer paso es traducir el $(-2,3,5)_B$ a coordenadas en base canonica 

Calculemos sus coordenadas en la base canónica $C$.

En primer lugar, tenemos que expresar los vectores de la base $B = \{\vec{u}_1,\vec{u}_2,\vec{u}_3\}$ en la base canónica $C = \{\vec{e}_1,\vec{e}_2,\vec{e}_3\}$ (serian $1,0,0$,$0,1,0$,$0,0,1$):

Por lo tanto el $2,4,0$ en base canonica, seria $2$ veces el $1,0,0$, $4$ veces el $0,1,0$ y $0$ veces el $0,0,1$ y asi con los demas vectores de la base $B$ expresados en forma canonica serian:
$$(2,4,0) = 2(1,0,0)+4(0,1,0)+0(0,0,1)$$
$$(1,0,1) = 1(1,0,0)+0(0,1,0)+1(0,0,1)$$
$$(-1,2,0) = -1(1,0,0)+2(0,1,0)+0(0,0,1)$$
Cada uno de estos tres vectores de la base $B$ los acabamos de calcular en la base canonica

A continuación, lo que buscamos son las coordenadas del vector $(2,3,-5)_B$ en la base canonica por tanto lo que buscamos son 3 escalares $\alpha,\beta,\gamma\in\mathbb{R}$ tales que este vector $u$ (que nosotros conocemos en la base $B$) tenga por coordenadas esas 3 de arriba en la base canonica o lo que es lo mismo se pueda escribir como esta despues del igual

$$\vec{u} = (\alpha,\beta,\gamma)_C = \alpha\vec{e}_1+\beta\vec{e}_2+\gamma\vec{e}_3$$
nosotros sabemos que sus coordenadas son $(-2,3,5)$ pero en esa base, lo que buscamos por sus coordenadas van a ser $\alpha,\beta,\gamma$ en la base canonica y ahora hay que descubirir quienes son esos valores de $\alpha,\beta,\gamma$

Pero lo que nosotros sabemos es que,  

$$\vec{u} = (-2,3,5)_B = -2\vec{u}_1+3\vec{u}_2+5\vec{u}_3$$
Pero de lo anteriro en lugar de $u_1$ podiamos escribir todo ese vector $2e_1+4e_2+0e_3$, es decir el vector $u_1$ de la base $B$ se puede escribir como dos veces el primer vector de la base canonica mas cuatro veses el segundo vector de la base canonica mas 0 veces el tercer vector de la base canonica, asi que $-2u_1$ es lo mismo que poner, todo esto que esta abajo, $-2$ veces $(2\vec{e}_1+4\vec{e}_2)$, cuando arriba ponemos, $+3$ veses $u_2$ es lo miso que poner mas tres veses el $u_2$ el segundo vector de la base $B$ que es $(1,0,1)$ que es lo mismo una ves $e_1$ y una ves $e_3$(Por eso la representacion de abajo) mas cuando ponemos $+5$ el vector $u_3$ es lo mismo que colocar cinco veses el vector $u_3$ de la base $B$ que es $(-1,2,0)$ y seria menos una ves el $e_1$ mas dos veses el $e_2$: 

$$ = -2(2\vec{e}_1+4\vec{e}_2)+3(\vec{e}_1+\vec{e}_3)+5(-\vec{e}_1+2\vec{e}_2) = (-4+3-5)\vec{e}_1 + (-8+10)\vec{e}_2+3\vec{e}_3 = -6\vec{e}_1+2\vec{e}_2+3\vec{e}_3$$

Arriba ya tenemos expresado el vector $u$ como combinacion lineal de los vectores de la base canonica, lo que hace falta es agrupar esos vectores 
 
Así pues el vector $u$ que sus coordenadas eran $(-2,3,5)$ en la base $B$ ahora son el $\vec{u} = (-6,2,3)_C$ en la base canonica, asi que hemos hecho un cambio de base 

**Ejercicio 26**

Dadas las bases (Aqui nos dan dos bases) $B_u = \{\vec{u}_1,\vec{u}_2,\vec{u}_3\}$ y $B_v=\{\vec{v}_1,\vec{v}_2,\vec{v}_3\}$ (No nos dicen quienes son, si no que esos tres vectores son base de cierto espacio vectorial) de un espacio vectorial de dimensión 3 y sabiéndose que se relacionana como

$$\left\{\begin{matrix}
\vec{v}_1 &=& 2\vec{u}_1&-&\vec{u}_2&+&\vec{u}_3\\
\vec{v}_2 &=& &-&\vec{u}_2&+&2\vec{u}_3\\
\vec{v}_3 &=& -\vec{u}_1&+&\vec{u}_2&-&3\vec{u}_3
\end{matrix}\right.$$

Esta relacion de arriba es similar a la relacion del primer ejemplo cunado se relacionaban los $u_i$ y las $e_i$ base canonica, Aqui no hay base canonica, simplemente hay dos bases $u$ y $v$ que se relacionan de la forma de arriba

Considerad el vector $u$ que tiene como coordenadas $\vec{u} = (2,0,-1)_{B_u}$ (En las bases de las $u$) y calculad sus coordenadas en la base $B_v$ (en la base $v$). Hay que considerar el vector $u$ que tiene por coordenadas $(2,0,-1)$ en la base $u$ asi que seria $(2u_1-1u_3)$, nosotros lo que queremos es este $u_1$ y $u_3$ expresarlo en funcion de $v$ pero la relacion que tenemos arriba es la opuesta tenemos la $v$ en funcion de la $u$ asi que tendremos que hacer la matriz inversa para despejar $u_1,u_2,u_3$ en terminos de las variables $v_1,v_2,v_3$ en otras palabras si lo escribimos de forma matricial $v_1,v_2,v_3$ igual a la matriz de coeficiente (asi como se muestran arriba con el $=$) por el vector $(u_1,u_2,u_3)$ tendremos que calcular la inversa de toda esa matriz de coeficientes y multiplicarla por las $v$ para asi despejar las $u$ y poder pasar de que ese vector $(2,0,-1)$ este en la base de las $u$ a que este en la base de las $v$. Con el siguente tema de abajo este proceso se puede automatizar.

## Cambio de base

Veamos de dónde sale la relación anterior:

Sea $E$ un $\mathbb{K}$-espacio vectorial de dimensión finita $n$ y sean $B_u = \{u_1,\dots,u_n\}$ y $B_v = \{v_1,\dots,v_n\}$ dos bases de $E$.(Estas son dos bases de un mismo espacio vctorial $E$, los que estan entre $\{\}$ serian los vectores)

Considremos un vector $x\in E$ y sean $(\alpha_1,\dots,\alpha_n)_{B_u}$ y $(\beta_1,\dots,\beta_n)_{B_v}$ las coordenadas del vector $x$ en las bases $B_u$ y $B_v$ respectivamente en esas dos bases.

Entonces, $x$ se podra escribir por un lado en la primera sumatoria con $\alpha_i$ ya que estas son las coordenadas del vector $x$ en la base $u$ y en el segundo sumatorio las $\beta$ son las coordenadas del vector $x$ en las bases de las $v$

$$x = \sum_{i = 1}^n\alpha_i\cdot u_i\qquad x=\sum_{j = 1}^n\beta_j\cdot v_j$$

Ahora bien, los elementos de la base $B_v$ tienen también unas coordenadas en la base inicial $B_u$.

Digamos que cada $v_j$ se podra escribir como $v_j = \sum_{i = 1}^n a_{ij}\cdot u_i\quad j=1,\dots,n$, simplemente como siguen siendo vectores por mucho que formen una base son vectores del espacio vectorial asi que cada $v_j$ tendra esas coordenadas $a_{ij}$ en la base de las $u$

y si sustituimos los $v_j$ por sus expresiones anteriores y escribimos el sumatorio de arriba, obtenemos

$$x=\sum_{j = 1}^n\beta_j\cdot v_j = x=\sum_{j = 1}^n\beta_j\cdot \left(\sum_{i = 1}^n a_{ij}\cdot u_i\right)$$

En la parte de arriba tenemos la suma deun escalar por toda una suma de escalares por vectores lo que podemos hacer es mover el parenteesis, seria aplicar la asociatiba, distribuirla las veses que nos haga falta y rescribir la exprecion

lo hemos reescrito con todo en el interior pero ahora la parte de $(\beta_{j}a_{ij})$ podemos permutar los sumatorios y dejar y dejar el sumatorio desde $i=1$ fuera, el sumatorio desde $j=1$ dentro y como la zona interna del parentesisi, solo la $\beta$ y la $a$ dependen de $j$ se puede escribir asi como esta ya en el ultimo igual

$$= \sum_{j = 1}^n\sum_{i = 1}^n (\beta_j a_{ij})\cdot u_i = \sum_{i = 1}^n\left(\sum_{j = 1}^n \beta_ja_{ij}\right)\cdot u_i$$

por asi decir lo que depende de $j$ que es el vecto $u_i$ lo dejamos fuera del parentesisi y hacemos estos porque......

Ahora como que las coordenadas de $x$ en la base $B$ son únicas, se debe verificar que todo lo que teniamos entre parentesis **ARRIBA** son las coordenadas de $x$ en la base $u$ **PERO** anteriormente teniamos tambien que todo esto $x = \sum_{i = 1}^n\alpha_i\cdot u_i$ tambien eran las coordenadas de $x$ en la base $u$ asi que ese sumatorio y ese $u_i$ son el mismo en estos dos sumatorios que tenemos aqui eso significa que cada uno se los $\alpha_i$ tiene que ser absolutamente igual a todo el sumatorio que tenemos entre parentesis de arriba o en otras palabra. Cada $\alpha_i$ debe ser igual a:
 
$$\alpha_i = \sum_{j = 1}^n\beta_ja_{ij}\quad \text{para todo }i=1,\dots,n$$

esa relacion se debe verificar porque las coordenadas de un vector en una base son unicas y y ya las teniamos calculadas y despues de estas tranformaciones hemos obtenidas estas otras, esta es la primera relacion que obtenemos

Esta expresión la podemos escribir de forma matricial como $PX_v = X_u$ (en la base de las $u$) donde $X_u$ es la matriz columna formada por las coordenadas de $x$ en la base $B_u$ (es decir los $\alpha_i$ de la transparencia anterior (las coordenadas del vector $x$ en la base de  las $u$)), $X_v$ es la matriz columna de las coordenadas de $x$ en la base $B_v$ (es decir los $\beta_j$ en columna) y $P$ (seria la matriz de cambio de base) es la matriz de las $a_{ij}$ que estos eran los coeficientes de cada vector $v_j$ expresado en la base de las $u$

**Observación.** La columna $j$-ésima de $P$ está formada por las coordenadas, en la base $B_u$, del correspondiente vector $v_j$ de la base $B_j$ (de las $v$), esto significa que cada una de las columnas de esta matriz $P$ tiene por coeficientes (tiene por entradas dicha matriz) las coordenadas de $v_j$ en la base de las $u$ 

De esta manera tenemos la ecuación en forma matricial ($P$ por las coordenadas de la base de las $v$ igual a las coordenadas de la base $u$, son los coefisintes de los vectores de $v$ en la base $u$ en columna por los coeficientes en la base $v$ igual a los cueficientes en la base de las $u$),$$PB_v = B_u\Leftrightarrow \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}\begin{pmatrix}
\beta_1\\
\beta_2\\
\vdots\\
\beta_n
\end{pmatrix} = \begin{pmatrix}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_n
\end{pmatrix} $$

que nos da las coordenadas de $x$ en la base $B_u$ (ve que nos da las $\alpha$) en función de las coordenadas del propio $x$ en la base $B_v$, esto signifca que nos regalan un vector en la base de las $v$, sustituimos en la columna de las $\beta$ por las coordenadas de ese vector en la base de las $v$ y a base de multiplicar por toda esa matriz $P$ (que esa matriz es unica, simplemente va a ser siempre la misma) a base de tener el producto de esa matriz de cambio de base por las coordenadas en columna del vector en la base de las $v$ vamos a obtener las coordenads respectivas de la base de las $u$

**Matriz de cambio de base.** La matriz $P$ anterior es la matriz del cambio de la base $B_v$ a la base $B_u$ y se obtiene escribiendo los vectores de la base $B_v$ en columna como combinación lineal de la base $B_u$.(Nosotros le metoes un vector en las coordenadas de ls $v$ (es donde estan las $\beta$) y nos la da en la base de las $u$)

Además, las coordenadas de un vector $x$ que este en la base $B_u$ se obtienen multiplicando las coordenadas de $x$ en la base $B_v$ por la matriz $P$ del cambio de base de $v$ a $u$

**Ejemplo 14 (Anterior que ya habiamos hecho)**

Dadas las bases $B_u = \{\vec{u}_1,\vec{u}_2,\vec{u}_3\}$ y $B_v=\{\vec{v}_1,\vec{v}_2,\vec{v}_3\}$ de un espacio vectorial de dimensión 3 y sabiéndose que exite esta relacion lineal (que los $v$ se pueden escribir como combinacion lineal de las $u$ utilizando estos coeficientes)

$$\left\{\begin{matrix}
\vec{v}_1 &=& 2\vec{u}_1&-&\vec{u}_2&+&\vec{u}_3\\
\vec{v}_2 &=& &-&\vec{u}_2&+&2\vec{u}_3\\
\vec{v}_3 &=& -\vec{u}_1&+&\vec{u}_2&-&3\vec{u}_3
\end{matrix}\right.$$

**Empezemos **Considerad el vector $\vec{u} = (2,0,-1)_{B_u}$ y calculad sus coordenadas en la base $B_v$ haciendo uso de matrices

Expresando el anterior sistema en su forma matricial, (el sistema anterior se puede escribir como: ponendo los coeficintesy multiplicar por la base $u$ igual a las coordenadas de la base $v$)

$$\begin{pmatrix}2 & -1 & 1\\
0 & -1 & 2\\
-1 & 1 & -3\end{pmatrix}\begin{pmatrix}\vec{u}_1\\
\vec{u}_2\\
\vec{u}_3\end{pmatrix}= \begin{pmatrix}\vec{v}_1\\
\vec{v}_2\\
\vec{v}_3\end{pmatrix}$$

o bien si queremos escribir por columnas (la anterior era por filas), fijese como cambia ahora en la matriz

$$\begin{pmatrix}\vec{u}_1 & \vec{u}_2 & \vec{u}_3\end{pmatrix}\begin{pmatrix}2 & 0 & -1\\
-1 & -1 & 1\\
1 & 2 & -3\end{pmatrix}= \begin{pmatrix}\vec{v}_1 & 
\vec{v}_2 & 
\vec{v}_3\end{pmatrix}$$

En la primera forma, las filas de la matriz son las coordenadas de los vectores $\vec{v}_1,\vec{v}_2,\vec{v}_3$ en la base de las $u$ mientras que en el segundo caso, las columnas son las coordenadas de dichos vectores en la base $B_u$ o en otras palabras en la Primera parte $v$ tiene por coordenadas $\vec{v}_1=2\vec{u}_1-\vec{u}_2+\vec{u}_3$ en la base $u$ pues ya puesto en la matriz nos queda la primera fila $2\quad -1\quad 1$ mientras que de la otra forma esa seria por columna

Por otro lado, se puede expresar el vector $\vec{u}$ en ambas bases de la siguiente manera:

El vector $u$ que era el $2\vec{u}_1-\vec{u}_3$ matricialmente se puede poner como el $\begin{pmatrix}\vec{u}_1 & \vec{u}_2 & \vec{u}_3\end{pmatrix}$ multiplicando el $\begin{pmatrix}2\\0\\1\end{pmatrix}$ en columna, o tambien seria por fila este ultimo vector que pusimos por las $u$ en columna

$$\vec{u} = 2\vec{u}_1-\vec{u}_3 = \begin{pmatrix}\vec{u}_1 & \vec{u}_2 & \vec{u}_3\end{pmatrix}\begin{pmatrix}2\\
0\\
-1\end{pmatrix} = \begin{pmatrix}2 & 0 & -1\end{pmatrix}\begin{pmatrix}\vec{u}_1\\
\vec{u}_2\\
\vec{u}_3\end{pmatrix}$$

lo que estamos buscando ahora con las coordenadas de $u$ en la base de las $v$, estos ban a ser ciertas coordenadas $\alpha$,$\beta$,$\gamma$ multiplicando a cada una de su respectiva $v$ esto del mismo modo de forma matricial sse puede escribir como esta representado aqui abajo despues del primer $=$:en fila o por columna

$$\vec{u} = \alpha\vec{v}_1+\beta\vec{v}_2+\gamma\vec{v}_3 = \begin{pmatrix}\vec{v}_1 & \vec{v}_2 & \vec{v}_3\end{pmatrix}\begin{pmatrix}\alpha\\
\beta\\
\gamma\end{pmatrix} = \begin{pmatrix}\alpha &\beta & \gamma\end{pmatrix}\begin{pmatrix}\vec{v}_1\\
\vec{v}_2\\
\vec{v}_3\end{pmatrix}$$

Con lo cual, tenemos la siguiente igualdad, si las tomamos por filas a las coordenadas podemos poner que  $\begin{pmatrix}2 & 0 & -1\end{pmatrix}_{B_u}$ multiplicado por la propia base $u$ es igual al vector ese con $\alpha$,$\beta$,$\gamma$ que multiplica al vector $v$ por columnas,asi que una expresion en la base de las $u$ es lo mismo que una exprecion en la base de las $v$ porque cada una esta en su misma base y representan el mismo vector $u$

$$\begin{pmatrix}2 & 0 & -1\end{pmatrix}_{B_u}\begin{pmatrix}\vec{u}_1\\
\vec{u}_2\\
\vec{u}_3\end{pmatrix} = \begin{pmatrix}\alpha &\beta & \gamma\end{pmatrix}_{B_v}\begin{pmatrix}\vec{v}_1\\
\vec{v}_2\\
\vec{v}_3\end{pmatrix}$$

Si en la igualdad anterior ahora sustituimos $\begin{pmatrix}\vec{v}_1\\ \vec{v}_2\\\vec{v}_3\end{pmatrix}$ por laexprecion matricial que nos habian enseñado de la transparenci anterior, la que salia de la relacion matricial entre $v$ y $u$ la que multiplicba por la matriz de cambio de base

$$\begin{pmatrix}\vec{v}_1\\
\vec{v}_2\\
\vec{v}_3\end{pmatrix} = \begin{pmatrix}2 & -1 & 1\\
0 & -1 & 2\\
-1 & 1 & -3\end{pmatrix}\begin{pmatrix}\vec{u}_1\\
\vec{u}_2\\
\vec{u}_3\end{pmatrix}$$

Lo que tenemos son los siguentes producto iguales ya remplasado el vector de las $v$ por lo de arriba

$$\begin{pmatrix}2 & 0 & -1\end{pmatrix}_{B_u}\begin{pmatrix}\vec{u}_1\\
\vec{u}_2\\
\vec{u}_3\end{pmatrix} = \begin{pmatrix}\alpha &\beta & \gamma\end{pmatrix}_{B_v}\begin{pmatrix}2 & -1 & 1\\
0 & -1 & 2\\
-1 & 1 & -3\end{pmatrix}\begin{pmatrix}\vec{u}_1\\
\vec{u}_2\\
\vec{u}_3\end{pmatrix}$$ 

Desarrollado el producto de mutiplicar el vector de los $\alpha,\beta,\gamma$ por la forma matricial nos queda sin dehar de lado el vector $\begin{pmatrix}\vec{u}_1\\ \vec{u}_2\\ \vec{u}_3\end{pmatrix}$

$$= \begin{pmatrix}2\alpha-\gamma &-\alpha-\beta +\gamma& \alpha+2\beta-3\gamma\end{pmatrix}\begin{pmatrix}\vec{u}_1\\
\vec{u}_2\\
\vec{u}_3\end{pmatrix}$$

Ahora ya solo falta resolver el sistema, fiajese que arriba como antes de desarrollarlo tenemos a ambos lados del igual ese mismo vector de las $u$ por tanto podemos coincidir que ya acomodado el vector que lo multiplica $(2,0,-1)$ y tomando a cada uno de los resultados que nos dio como $2\alpha-\gamma$,$-\alpha-\beta +\gamma$,$\alpha+2\beta-3\gamma$ formaremos la siguente ecuacion

$$\left\{\begin{matrix}2\alpha &&&-&\gamma &=& 2\\
-\alpha & -&\beta&+&\gamma &=&0\\
\alpha&+&2\beta&-&3\gamma &=&-1\end{matrix}\right.$$

Asi que ya solo falta resolver el sistema de ecuacion lineal final y obetener que 

Cuya única solución es $(1,-1,0)$ es un sistema compatible determinado ya que si es una base las coordenadas de un vector de una base son unicas asi que es normal que el sistema salga compatible determinado

Así pues, lo que podemos afirmar es que el vector que era $(2,0,-1)$ en la base $u$ ahora nos da el vector $(1,-1,0)$ en la base de las $v$, asi es como utilizamos esa matriz $A$ para establecer un cambio de base que nos ah llebado de $u$ hasta $v$, nos ah cambiado las coordenadas de los vectores $u$ a los vectores $v$

$$\begin{pmatrix}2 & 0 & -1\\
-1 & -1 & 1\\
1 & 2 & -3\end{pmatrix}\begin{pmatrix}
1\\
-1\\
0\end{pmatrix}_{B_v} = \begin{pmatrix}
2\\
0\\
-1
\end{pmatrix}_{B_u}$$

**Proposición.** Las matrices de cambio de base son siempre invertibles (Cuadradar e invertivles, el determinante no va a ser $0$) y, si $P$ es la matriz del cambio de base de $B_u$ a $B_v$, entonces $P^{-1}$ es la matriz del cambio de base de $B_v$ a $B_u$. ya teniendo la forma de llegar de una forma a otra $P^{-1}$ nos lleva de la base $v$ a la base $u$

**Ejercicio 27.** Demostrar formalmente esta Proposición.

**Proposición.** Sea $E$ un $\mathbb{K}$-espacio vectorial de dimensión $n$ y sean $B,B',B''$ son 3 bases diferentes de $E$. Si $P$ es la matriz de cambio de base que nos lleva de $B$ a $B'$ y $Q$ es la matriz de cambio de base que nos lleva de $B'$ a $B''$, entonces la matriz de cambio de base que lleva desde $B$ a $B''$ es $QP$ (es multplicar $Q$ por $P$), es decir el primer cambio de base lo ejecutabamos como $P$ el segundo lo llebamamos a cabo a traves de la matriz de cambio de base $Q$ para hacer el camino directo desde la primera $B$ hasta la tercera $B''$ sin tener que pasar por la segundo solo hay que multiplicar al revez es decir la primera que habremos multiplicado (en este caso $P$) es la ultima que queda en el producto  ($QP$), siempre hay que leerlo hacia detraz ya que aqui habremos hecho $PQ$ originalmete enotnces sera al revez la primera matriz de cambio de base que ejecutemos es la ultima que ponemos

## Ejercicio 27

La proposición decía que las matrices de cambio de base siempre van a ser invertibles y que si tenemos $B$  una base de un espacio vectorial y $B'$ otra base del mismo espacio vectorial, si $P$ es la matriz de cambio de base de $B$ a $B'$ entonces la matriz de cambio de base que me lleva de $B'$ a $B$ **(la matriz inversa)** es precisamente la inversa de la matriz $P$ que es $P^{-1}$, es decir si que si ya se ah fabricado una matriz de cambio de base que me lleva de la base $B$ a la base $B'$, si queremos pasar de la base $B'$ a la base $B$ tendrémos que utilizar como matriz de cambio de base $P^{-1}$

**Demostracion: ** Sea $P$ la matriz de cambio de base de $B$ a $B'$ (Si multiplicamos por un vector de $B$conseguimos sus corrdenadas en la base $B'$) le vamos a llamar $P=(a_{ij})$ y sea $P'$ la matriz de cambio de base $P'=(b_{ij})$ que es la que nos lleva de $B'$ a $B$.

Evidentemente el objetivo del ejercicio es demostrar que $P$ y $P'$ son inversas la una de la otra, recordemos que si $P$ es la matriz $(a_{ij})$ Eso significa que los vectores $v_j$ se pueden escribir como:
$$v_j=\sum_{i=1}^{n}a_{ij}e_i=a_{1j}e_1+a_{2j}e_2+\cdots+a_{nj}n_1\quad \text{para todo }j=1,\dots,n$$

Mientras que al revés los vectores $e_i$ los que fueran de la base $B$ fijese que en este contexto la base $B$ es la formada por los vectores $B=\{e_1,...,e_n\}$ mientras que la base $B'$ es la que está formada por los vectores $B'=\{v_1,...,v_n\}$. como se decía los $e_i$ a su vez como la matriz de cambio de base de $B'$ a $B$ es la $b_{ij}$ (coeficientes de la matriz) cada uno de los vectores $e_j$ se podría escribir como cierta combinación lineal:
$$e_i=\sum_{j=1}^{n}b_{ij}v_j=b_{1j}v_1+b_{2j}v_2+\cdots+b_{nj}v_n\quad\text{para todo }i=1,\cdots,n$$

Simplemente hemos detallado lo que significa las matrices de cambio de bases, recuerda que tienen por columnas las coordenadas de los vectores de la base nueva en la base antigua como $P$ nos lleva de $B$ a $B'$, la base de $B'$ se puede expresar como las coordenadas en columna multiplicados por los vectores de la base $B$ y lo mismo para $P'$.

De este modo si utilizamos la primera ecuación y sustituimos los $e_i$ respectivos que están en la sumatoria de $v_j$ por su expresión completa que tenémos abajo (seria el segundo sumatorio) nos quedaría que los $v_j$ se podrían escribir como:

$$v_j=a_{1j}(b_{11}v_1+b_{21}v_2+...+b_{n1}v_n)+...+a_{nj}(b_{1n}e_1+...+b_{nn}v_n)$$

Entonces esta sería la expresión y si la redistribuimos correctamente nos quedaría como (Estos serían todos los términos que acompañarían al primero vector y asi sucesivamente llegaríamos hasta el último que sería $v_n$):
$$=(b_{11}a_{1j}+b_{12}a_{2j}+...+b_{1n}a_{nj})v_1+...+(b_{n1}a_{1j}+b_{n2}a_{2j}+...+b_{nn}a_{nj})v_n$$
Entonces tendríamos esta expresión de aquí pero ya sabéis que debido a la unicidad de coordenadas esto quiere decir que todos estos coeficientes $(b_{11}a_{1j}+b_{12}a_{2j}+...+b_{1n}a_{nj})$ y $(b_{n1}a_{1j}+b_{n2}a_{2j}+...+b_{nn}a_{nj})$ que nos salen delante de los vectores todos sería un cero a excepción del que estuviera en el $v_j$ que sería un $1$, fijaros que en la exprecion de arriba es como si tuvieramos un $1$.

lo que podría escribir es el sistema en el que todos serian cero a excepcion del $j$-esimo en este caso es decir que el de enmedio seria igual a $1$ y a partir de aqui todos serian igual a $0$ hasta el ultimo: 

$$\left\{\begin{matrix}
b_{11}a_{1j} &+& b_{12}a_{2j} &+&\cdots&+&b_{1n}a_{nj}&=&0\\
\cdots&+&\cdots&+&\cdots&+&\cdots&=&\cdots\\
b_{j1}a_{1j} &+& b_{j2}a_{2j} &+&\cdots&+&b_{jn}a_{nj}&=&1\\
\cdots&+&\cdots&+&\cdots&+&\cdots&=&\cdots\\
b_{n1}a_{1j} &+& b_{n2}a_{2j} &+&\cdots&+&b_{nn}a_{nj}&=&0\\
\end{matrix}\right.$$

Entonces todos serían ceros a excepción de la $j\text{-ésima}$.

Pues resulta que esta igualdad que tenéis aquí la podéis desarrollar en forma matricial, esto es precisamente $P'\cdot P$ igual a la matriz identidad de orden $n$, $P'\cdot P=I_n$ porque tendríais que escribir estas ecuaciones para todo $i$ para todo $j$ entre $1$ y $n$.

Pues resulta que si desarrollamos el producto de $P'$ por $P$ da la identidad $I$ de forma análoga en sentido contrario (en un inicio hemos cogido la expresión de las $v$ y hemos sustituyéndo las $e$) si hicieramos lo contrario de tomar la expresión de las $e$ y donde hay una $v$ pusieras la expresión de las $v$ llegaríamos a lo contrario que $P\cdot P'$ también resulta la matriz identidad de orden $n$, $P\cdot P'=I_n$ por lo tanto esto demuestra que las matrices $P$ y $P'$ son inversas la una de la otra y ya esta completa la demostración de la proposición y es que si tenéis una matriz de cambio de base que os lleva de una a otra para hacer el cambio inverso simplemente tendréis que utilizar la matriz inversa del cambio anterior.

## Ejercicios de Cambio de Base

Como a partir de una base y otra que fabriquemos, podemos obtener las matrices que nos llevan de una base a otra.

Consideremos durante lo que va a ser este ejercicio el espacio vectorial $\mathbb{R}^3$ y vamos a considerar dentro de él dos bases, una la base canónica formada por $B=\{e_1,e_2,e_3\}$  (ya sabéis coordenadas $(1 0 0, 0 1 0, 0 0 1)$ y vamos a considerar el conjunto $B'=\{u_1,u_2,u_3\}$ donde estos vectores tienen por coordenadas en la base canónica pues sería $u_1=\{1,1,0\}_B$, $u_2=\{1,0,1\}_B$ y el $u_3=\{0,1,1\}_B$. Os recuerdo que esto significa que los vectores $u_1,u_2,u_3$ tienen estas coordenadas en la base canónica que seria la base $B$ o lo que es lo mismo que $u_1$ sería por ejemplo el vector $u_1=e_1+e_2$,$u_2=e_1+e_3$ y $u_3=e_2+e_3$

Típicamente estos ejercicios empiezan diciendo algo así como demostrar que en efecto $B'$ es una base también de $\mathbb{R}^3$ ya sabéis que para que sea una base tiene que generar todo el espacio ser linealmente independientes.

Por tanto por el teorema que vimos anteriormente para que fueran una base tendríais que demostrar que el rango de esos tres vectores $rg(u_1,u_2,u_3)$ es de rango $3$ lo cual es muy fácil porque escribis las coordenadas en columna hacemos el determinante vemos que es diferente de $0$ y ya está.

Vale os lo dejo como pequeño ejercicio que ya hay un ejemplo hecho anteriormente.

Ahora necesitaríamos una matriz de cambio de base para pasar de $B$ a $B'$ y otra para pasar de $B'$ a $B$, serian dos matrices de cambio de base. Pero el ejercicio es un poquito más exagerado porque nos dice que queremos hallar las coordenadas de cierto vector $x$ que es el $(1,1,1)$ en la base $B$ y queremos hallar cómo sería ese vector (Cuáles serían sus coordenadas $(\alpha, \beta, \gamma)$) si lo expresáramos en la base de $B'$.

Pues bueno para ello podemos utilizar la proposición anterior y ver la matriz de cambio de base que lleva de $B$ a $B'$ es la que hemos llamado la matriz $P$ será aquella que tiene las coordenadas de los vectores $u_1,u_2,u_3$ en columna, en la base canónica por tanto la matriz de cambio de base que transforma un vector de la base $B$ a la base de $B'$ será la matriz $(1,1,0)$ las coordenadas del primer vector en columna, el $(1,0,1)$ el segundo vector en columna y el $(0,1,1)$ el tercer y último vector en columna:

$$\begin{pmatrix}1 & 1 & 0\\
1 & 0 & 1\\
0 & 1 & 1\end{pmatrix}$$

La matriz $P$ nos transforma un vector de la base $B$ a la base $B'$.

En este caso tenemos las coordenadas de $x=(1,1,1)_B$ en la base $B$ y las quiero transformar a la base $B'$, como tenemos la matriz de cambio de base de $B$ a $B'$ esto significa que si las coordenadas de $x$ en la base canónica son la $(1,1,1)$ y $\alpha,\beta,\gamma$ son las coordenadas en la base $B'$ se cumplirá que:

$$P\cdot\begin{pmatrix}\alpha\\
\beta\\
\gamma\end{pmatrix}=\begin{pmatrix}1\\
1\\
1\end{pmatrix}$$

Será igual al vector $(1,1,1)$ que son las coordenadas precisamente del el vector $x$ en la base canónica. Así que en nuestro caso si quieremos obtener las coordenadas de un vector en la base $B'$,$\begin{pmatrix}\alpha\\\beta\\\gamma\end{pmatrix}_{B'}$,y conocemos las que están en la base $B$,$\begin{pmatrix}1\\1\\1\end{pmatrix}_{B}$,nos va a hacer falta buscar la inversa de $P$ para poder despejar que $\begin{pmatrix}\alpha\\\beta\\\gamma\end{pmatrix}=P^{-1}\cdot\begin{pmatrix}1\\1\\1\end{pmatrix}$, **hacer como ejercicio buscar esa inversa**.

simplemente esa inversa es: 

$$P^{-1}=\frac12\begin{pmatrix}1&1&-1\\
1&-1&1\\
-1&1&1\end{pmatrix}$$

Si nosotros calculamos quien sería $\begin{pmatrix}\alpha\\\beta\\\gamma\end{pmatrix}$ habría que multiplicar esta matriz de arriba por el vector $(1,1,1)$ en columna:

$$\begin{pmatrix}\alpha\\\beta\\\gamma\end{pmatrix}\cdot\frac12\begin{pmatrix}1&1&-1\\
1&-1&1\\
-1&1&1\end{pmatrix}\cdot\begin{pmatrix}1\\
1\\
1\end{pmatrix}$$

lo cual nos daría. $\begin{pmatrix}\frac12\\\frac12\\\frac12\end{pmatrix}$

así que el vector que en la base canónica es el $(1,1,1)_B$ cuando hemos transformado y hemos cambiado de base $B$ a la base $B'$ resulta que sus coordenadas son la $(\frac12,\frac12,\frac12)_{B'}$ y así es como se haría el cambio para pasar el vector que estaba inicialmente en la base $B$ (en la base canónica) para pasarlo a otra base que es $B'$.

Como ejercicio demostrar que en efecto $u_1,u_2,u_3$ forman una base y que en este caso para cambiar a un vector de una base a otra tenéis que utilizar la matriz que corresponda

Esto completaría un poco lo que es el tema de espacios vectoriales y cómo a través de las bases uno puede tener localizados los diferentes vectores dependiendo de qué base utilizais en cada caso.

# Bases ortogonales y ortonormales

De todas la bases que hay estas son especiales:

**Base ortogonal.** Dada una base $B = \{\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\}$ de un espacio vectorial $E$, se dice que se trata de una base ortogonal si sus elementos son ortogonales dos a dos (significa que cada uno de ellos producto escalar con otro tiene que dar $0$, dos vectores son ortogonales si su producto escalar daba $0$) en este caso una base es ortogan si cada uno de los $u_i$ producto escalar con el otro $u_j$ da cero, osea haciendo su producto escalar dos a dos siempre da cero:

$$\langle\vec{u}_i,\vec{u}_j\rangle = 0\quad\forall i\ne j$$
Los vectores de esta base son ortogonales (Perpendiculares), la base canonica lo cumple, si hacemos producto escalar en $\mathbb{R}^3$ de la base canonica (producto escalar de (1,0,0) con (0,0,1) nos da cero, el de (1,0,0) con (0,1,0) nos da cero y el de (0,1,0) con (0,0,1) tambirn da cero) asi que el produccto escalar de cualquier vector de la base canonica con otro da cero, asi que esta base esortogonal

**Base ortonormal.** Dada una base $B = \{\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\}$ de un espacio vectorial $E$, se dice que se trata de una base ortonormal si es ortogonal y todos sus elementos (vectores) son unitarios:

si su producto escalar es $0$ (vectores diferentes multiplicados escalarmente nos dan cero)

$$\langle\vec{u}_i,\vec{u}_j\rangle = 0\quad\forall i\ne j$$
Y el modulo de cada uno de sus vectores hasta el modulo de $u_n$ todos esos vectores tienen norma (Modulo) igual $1$

$$||\vec{u}_i|| = 1\quad\forall i$$

## Método de ortogonalización de Gram-Schmidt

**Método de ortogonalización de Gram-Schmidt.** Permite construir una base ortogonal a partir de una base cualquiera del espacio vectorial. (e incluso ortonormal)

Sea $B= \{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n\}$ una base cualquiera de un espacio vectorial $E$ de $\dim(E) = n$ (supongamos que los vectores son las $v$). queremos ortogonalizar estos vectores.

A partir de los vectores de la base $B$, se construirá una nueva base (la base ortogonal) $B_o = \{\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\}$ que será ortogonal y del mismo espacio. Tiene que ser ambas bases del mismo espacio vectorial con la diferencia que el ortogonal no tiene porque ser una base ortogonal pero la base con las $u$ si sera ortogonal

1. (El primer vector $u_1$ de la nueva base es el mismo primer vector de la anitgua base $v_1$) Se toma $\vec{u}_1 =\vec{v}_1$ como primer vetor de la base nueva.
2. El segundo vector será una combinación lineal de $\vec{v}_1$ y $\vec{v}_2$ de la forma $\vec{u}_2 = \vec{v}_2-\alpha\vec{u}_1$, al cual se le impondrá la condición de que debe ser perpendicular a (al anterior) $\vec{u}_1$. Es decir, $\vec{u}_1\perp\vec{u}_2$. De este modo obtendremos. (El nuevo vector $u_2$ lo que vamos hacer es calcularlo como $v_2$ menos $\alpha\vec{u}_1$ y $u_1$ debe de ser perpendicular al nuevo $u_2$ que estamos fabricando) esta condicion la podremos imponer invocando a que el producto escalar de $u_1$  con $u_2$ tienen que ser cero, desarrollando de la ecuacion $\vec{u}_2 = \vec{v}_2-\alpha\vec{u}_1$ nos quedaria el producto escalar de $u_1$ con $u_2$, cero (porque queremos que sea pernediculares) igual a producto escalar de $u_1$ con $v_2$ menos $\alpha$ producto escalar de $u_1$ con $u_1$, en otras palabras podriamos despejar la $\alpha$ de esa formula porque esta igualada a cero y obtendriamos que $\alpha$ deberia ser esto que esta aqui abajo.

$\alpha$ es igual al producto escalar de $u_1$ con $v_2$ dividido por la norma de $u_1$ al cuadrado, por tanto si esto es $\alpha$ el nuevo $u_2$ (esta aqui abajo) sera $v_2$ menos la $\alpha$ que acabamos de hayar multiplicado por el vector $v_1$. **Fijese que** (en un incio teneiamos que $u_1=v_1$ como se ve aqui abajo se pone como nos interesa) en la fomula inicial para la demostracion nos interesaba el $u_1$, aqui abajo como se muestra se traduce que $u_2$ es $v_2$ menos lo que le sobra a $v_2$ para que sea perpendicular a $v_1$ es decir vamos qutando la aprte que nunca es perpendicular al vector que ya tenemos gracias a la formula de $\frac{\langle\vec{u}_1,\vec{v}_2\rangle}{\langle\vec{u}_1,\vec{u}_1\rangle}$ que es la formula de la proyeccion ortogonal

$$\alpha = \frac{\langle\vec{u}_1,\vec{v}_2\rangle}{\langle\vec{u}_1,\vec{u}_1\rangle}\Rightarrow \vec{u}_2 = \vec{v}_2-\frac{\langle\vec{u}_1,\vec{v}_2\rangle}{\langle\vec{u}_1,\vec{u}_1\rangle}\vec{v_1}$$
3. Para calcular el tercer vector, se procede del mismo modo: el tercer vector será quitarle una combinación lineal de $\vec{v}_1,\vec{v}_2,\vec{v}_3$ de la forma (que $u_3$ seria $v_3$ menos lo que le sobre a $\alpha$ del vector $u_1$ de la nueva base menos lo qu le sobre a $\alpha_2$ del vector $u_2$ de la nueva base)  $\vec{u}_3= \vec{v}_3-\alpha_1\vec{u_1}-\alpha_2\vec{u}_2$ a la cual se impondrán las condiciones (de que este nuevo $u_3$ debe ser perpendicular a $u_1$ pero tambien tiene que ser perpendicular a $u_2$) $\vec{u}_1\perp\vec{u}_3$ y $\vec{u}_2\perp\vec{u}_3$ (esti significa que podriamos ecribir desde $\vec{u}_3= \vec{v}_3-\alpha_1\vec{u_1}-\alpha_2\vec{u}_2$ el producto escalar de $u_1$ con $u_3$ igualado a cero, el producto escalar de $u_2$ con $u_3$ igualado a cero, desarollar toda esa formula para los productos escalares, despejar $\alpha_1$ y $\alpha_2$) y al despejar las $\alpha$ se optiene lo que se muestra abajo (lo que esta entre $\langle \rangle$ son rpductos escalares). Operando se obtniene: ($u_1$ y $u_2$ son perpendiculares porque asi los contruimos en la etapa dos)

$$\alpha_1 = \frac{\langle\vec{u}_1,\vec{v}_3\rangle}{\langle\vec{u}_1,\vec{u}_1\rangle};\quad\alpha_2 = \frac{\langle\vec{u}_2,\vec{v}_3\rangle}{\langle\vec{u}_2,\vec{u}_2\rangle}$$ 
con esos coeficientes que hayamos ayado ($\alpha$) solo nos queda decir que el nuevo $u_3$ es $v_3$ menos el producto escalar de $u_1$ con $v_3$ dividio el producto esclar de $u_1$ con $u_1$ en la direccion de $u_1$ es decir en la formula de arriba ($u_3=$) despejamos $\alpha$ por lo que hemos allado por lo tanto sustitumios las $\alpha$ por lo que hemos ayado en el proceso anterior

$$\vec{u}_3 = \vec{v}_3-\frac{\langle\vec{u}_1,\vec{v}_3\rangle}{\langle\vec{u}_1,\vec{u}_1\rangle}\vec{u}_1-\frac{\langle\vec{u}_2,\vec{v}_3\rangle}{\langle\vec{u}_2,\vec{u}_2\rangle}\vec{u}_2$$

**Y operamos de forma análoga hasta llegar al ** ultimo que seria el $u_n$ menos el $v_n$ menos una combinacion lineal de todos los anteriores y esa combinacion lineal seria ese producto escalar, (aqui y cuando tenemos $\langle u_i,u_i\rangle$ seria el producto de escalar de esos dos o lo que seria lo mismo la norma de $(u_i)^2$) por asi decir a cada uno de los vectroes $v_n$ le hemos estando restando de los anteriores que hemos creado para la base la parte que no era perpendicular a ellos (les restamos la parte que les sobraba para que el nuevo vector $u_i$ fuera perpendicular a todos los anteriores)

$$\vec{u}_n = \vec{v}_n-\sum_{i=1}^{n-1}\frac{\langle\vec{u}_i,\vec{v}_n\rangle}{\langle\vec{u}_i,\vec{u}_i\rangle}\vec{u}_i$$

Finalmente ya se tendira una base ortogonal, si lo que se quiere es una base ortonormal, bastará con dividir cada vector por su norma para así normalizar todos los elementos de la base, que todos los veectores tengan norma igual a uno

Este algoritmo nos da una forma de demostrar que los $u_i$ son ortogonales, esto es la demostracion del teorema, obtenemos todo un conjunto de vectores $u_i$ que ahora son que ahora son ortogonales los unos a los otros por propia construccion porque a cada vector le hemos estando restnado de los anteriores lo que no era ortogonal y portanto esto que resulta es ortogonal y este algoritmo se puede programar con estas formulas para que apartir de una base cualqueira prorcionada se convierta en una base ortogonal

## Proyección ortogonal de un vector sobre un subespacio

buscamos proyectar un vector sobre todo un subespacio vectorial 

**Vector ortogonal a un subespacio.** Un vector $\vec{u}\in E$ es ortogonal a un subespacio vectorial (subespacio vectorial $S$ de $E$)$S\subseteq E$ si, y solo si, tal que producto escalar de $u$ con cualquier otro vector del subespacio  da $0$, por ejemplo En un plano que viva dentro de $\mathbb{R}^3$, un vector  ortogonal sería por ejemplo el vector normal, un vector que es perpendicular y cualquier vector que tomemos dentro del plano (producto escalar con su vector normal sería cero).


$$\langle\vec{u},\vec{x}\rangle = 0\quad\forall\vec{x}\in S$$
Con la definicon dada,Si nos dan un vector y nos preguntamos si este vector es perpendicular a este subespacio. Con esta definición hay que ir y buscar todos los vectores que tuvieran un subespacio (los cuales son infinitos) y comprobar que el producto escalar de ese vector contra todos esos infinitos vectores que viven dentro del subespacio da cero.
(se tendira que ir probando uno por uno y verficar que de cero). Por tanto esta definición aunque está muy bien y es muy formal es totalmente inútil.

---------------------------------------------------------------------------

Sin embargo este teorema nos dice que para comprobar que un vector $u$ es perpendicular a un subespacio $S$ no hace falta probar con todos los vectores. Basta con que probemos con todos los que son vectores de la base $A$ eso significa que si un plano tiene dimensión $2$ la base de un plano que sea un subespacio vectorial, tendrá dos vectores para comprobar que un vector $u$ es ortogonal al plano, solo con comprobar que es ortogonal a los vectores de una base de ese plano.

**Teorema.** Un vector $\vec{u}\in E$ es ortogonal a un subespacio vectorial $S\subseteq E$ si, y solo si, es ortogonal a todos los vectores de una base de $S$ (una base cualquiera del Subespacio).

---------------------------------------------------------------------------

Este concepto se puede extender a un nivel más elevado y definir subespacios ortogonales entre sí. Si nos dan dos subespacios vectoriales $V$ y $W$ los dos subespacios del mismo espacio vectorial $E$ tiene sentido decir que son ortogonales si sólo si el producto de cualquier vector del primer subespacio $V$ producto escalar contra cualquier vector del segundo subespacio $W$ da cero.

Por tanto dos subespacios $V$ y $W$ de $E$ son ortogonales si y solo si para cualquier par de vectores uno de $V$ y otro de $W$, el producto escalar de esos dos vectores da cero. De nuevo se tendria que probar con todos los vectores de $V$ y todos los vectores de $W$, nunca acabariamos

**Teorema.** Dos subespacio $V$ y $W$ de $E$ son ortogonales si:

$$\forall\vec{x}\in V,\ \forall\vec{y}\in W\Rightarrow \langle\vec{x},\vec{y}\rangle = 0$$

-------------------------------------------------------------------------------

**Teorema.**Para que dos subespacios $V$ y $W$ sean ortogonales, es suficiente con que los vectores de una base de $V$ sean ortogonales a los vectores de una base de $W$

Es decir basta con comprobar todos con todos los vectores y que todos los vectores de $V$ son perpendiculares a todos los vectores de $W$.

Con eso es suficiente para demostrar que dos subespacio ortogonales.

Por ejemplo podríais tomar dos rectas y demostrar que esas dos rectas son ortogonales tomando los vectores que definen el subespacio vectorial de cada una de esas rectas y comprobar que los vectores de esa base de cada una de las rectas son perpendiculares los unos contra los otros.

Pero no hay que pensar que esto es valido en $\mathbb{R}^2$ o $\mathbb{R}^3$, si tenemos dos subespacios vectoriales dentro de $\mathbb{R}^infinito$ podemos comprobar que en efecto son ortogonales, comprobando que el producto escalar de todos los vectores de una base contra todos los vectores de otra base son todos ellos cero

Recordemos...

**Proyección ortogonal.** La proyección ortogonal de un vector $\vec{u}$ sobre otro $\vec{v}$, se expresa como la proyección de $u$ sobre el vector $v$ que viene dado como el producto escalar de $u$ con $v$ dividido. el producto escalar $v$ con $v$ multiplicado por el vector $v$ o en forma un poquito más reducida, era el producto escalar de $u$ con $v$ dividido a la norma de $v$ al cuadrado y multiplicada por el vector $v$. (Aquí tenemos la versión girada, aquí se proyecto el vector $u$ sobre $v$ cuando vimos la definición la vimos al revés)

$$P_{\vec{u}}(\vec{v}) = \frac{\langle\vec{u},\vec{v}\rangle}{\langle\vec{v},\vec{v}\rangle}\vec{v} = \frac{\langle\vec{u},\vec{v}\rangle}{||\vec{v}||^2}\vec{v} $$

**Proyección ortogonal de un vector sobre un subespacio.** Dado $S$ un subespacio vectorial de un espacio vectorial $E$, todo vector $\vec{u}\in E$ se descompone de manera única en:
En suma de dos vectores, el vector $u_s$ y el vector $u_0$, $u_s$ es un vector que pertenece al subespacio vectorial $S$ y $u_0$ es un vector que pertenece al subespacio ortogonal de $S$, 

$$\vec{u} = \vec{u}_S+\vec{u}_0$$

Con $\vec{u}_S\in S$ y $\vec{u}_0\in S^{\perp}$ (el subespacio ortogonal de $S$ que se escribe así son esa especie de T invertida.). En particular, el vector $\vec{u}_S\in S$ se denomina **vector proyección ortogonal** de $\vec{u}$ sobre $S$.

Así que cualquier vector de un espacio vectorial siempre descompondrá en suma de dos vectores, uno que pertenece a $S$ y otro que pertenece al ortogonal de $S$. precisamente el que pertenece a $S$ se le llama la proyección ortogonal de $u$ sobre el subespacio $S$, así que ya no sólo tiene sentido proyectar vectores sobre vectores sino también vectores sobre subespacios.

Si queremos calcular la proyección ortogonal de un vector sobre un subespacio

Si se toma en $S$ una base ortogonal $\{\vec{s}_1,\vec{s}_2,\dots,\vec{s}_r\}$ (Una base ortogonal fabricada por ejemplo utilizando el teorema de grand Smit), la proyección de $\vec{u}$ sobre $S$ viene dada por la generalisacion de la formula anterior, es decir la proyección ortogonal del vector $u$ sobre el espacio $S$ será el vector $u_s$ que se podrá calcular como el sumatorio de los productos escalares de $u$ por $s_i$ (por cada uno de los vectores de esa base ortogonal) dividido en la norma de cada uno de los vectores (no tienen porque tener norma 1, sólo que sea ortogonal, no necesariamente ortonormal) y multiplicado por $s_i$.

$$P_{S}(\vec{u}) = \vec{u}_S = \sum_{i = 1}^r\frac{\langle\vec{u},\vec{s}_i\rangle}{||\vec{s}_i||^2}\vec{s}_i$$

Así que la proyección ortogonal del vector $u$ sobre el subespacio $S$ vendrá dada como la combinación lineal (la suma) de los productos escalares de ese vector por cada uno de los elementos de la base ortogonal dividido a la norma al cuadrado de cada uno de esos vectores de la base ortogonal y multiplicados por esos mismos vectores.
